# =============================================================================
# Dataset Configuration
# =============================================================================
DATASET_NAME=Azzindani/ID_REG_DB_2510
HF_TOKEN=

# =============================================================================
# Model Configuration
# =============================================================================
EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B
RERANKER_MODEL=Qwen/Qwen3-Reranker-0.6B
LLM_MODEL=Azzindani/Deepseek_ID_Legal_Preview
MAX_LENGTH=32768
EMBEDDING_DIM=1024

# =============================================================================
# Device & Inference Configuration (Local)
# =============================================================================
# Main device
DEVICE=cuda

# Per-component device allocation (for CPU/GPU split)
EMBEDDING_DEVICE=cpu    # CPU for small embedding model
RERANKER_DEVICE=cpu     # CPU for small reranker model
LLM_DEVICE=cuda         # GPU for large LLM

# Quantization for local LLM (reduces VRAM usage)
LLM_QUANTIZATION=4bit   # none, 4bit, 8bit
LLM_LOAD_IN_4BIT=true
LLM_LOAD_IN_8BIT=false
EMBEDDING_DTYPE=float32 # float32, float16, bfloat16

# =============================================================================
# LLM Provider Configuration
# =============================================================================
# Provider: local, openai, anthropic, google, openrouter
LLM_PROVIDER=local

# API Keys (only needed for cloud providers)
OPENAI_API_KEY=
ANTHROPIC_API_KEY=
GOOGLE_API_KEY=
OPENROUTER_API_KEY=

# Cloud model names (when using API providers)
OPENAI_MODEL=gpt-4o
ANTHROPIC_MODEL=claude-sonnet-4-20250514
GOOGLE_MODEL=gemini-1.5-pro
OPENROUTER_MODEL=anthropic/claude-sonnet-4

# API Configuration
API_TIMEOUT=120
API_MAX_RETRIES=3

# =============================================================================
# Context Cache Configuration
# =============================================================================
ENABLE_CONTEXT_CACHE=true
CONTEXT_CACHE_SIZE=100      # Max cached contexts
CONTEXT_MAX_TOKENS=8192     # Max tokens per context
CONTEXT_COMPRESSION=true    # Enable context compression
CONTEXT_SUMMARY_THRESHOLD=4096

# =============================================================================
# System Configuration
# =============================================================================
LOG_DIR=logs
ENABLE_FILE_LOGGING=true
CACHE_DIR=.cache
BATCH_SIZE=32
MAX_MEMORY_MB=15000

# =============================================================================
# Search Configuration
# =============================================================================
FINAL_TOP_K=3
MAX_ROUNDS=5
INITIAL_QUALITY=0.95
QUALITY_DEGRADATION=0.1
MIN_QUALITY=0.5
PARALLEL_RESEARCH=true
RESEARCH_TEAM_SIZE=4

# =============================================================================
# LLM Generation
# =============================================================================
TEMPERATURE=0.7
MAX_NEW_TOKENS=2048
TOP_P=1.0
TOP_K=20
MIN_P=0.1

# =============================================================================
# Consensus Configuration
# =============================================================================
ENABLE_CROSS_VALIDATION=true
ENABLE_DEVIL_ADVOCATE=true
CONSENSUS_THRESHOLD=0.6
