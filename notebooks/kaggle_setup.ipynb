{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üèõÔ∏è Legal RAG Indonesia - Kaggle Setup\n",
                "\n",
                "This notebook sets up and runs the Legal RAG system in Kaggle.\n",
                "\n",
                "**Run cells in order:**\n",
                "1. Setup & Dependencies\n",
                "2. GPU Check\n",
                "3. Run Diagnostic Test (optional but recommended)\n",
                "4. Launch Application"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 1: Setup Path & Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "# Set project root\n",
                "PROJECT_ROOT = '/kaggle/working/06_ID_Legal'\n",
                "os.chdir(PROJECT_ROOT)\n",
                "sys.path.insert(0, PROJECT_ROOT)\n",
                "\n",
                "print(f\"Working directory: {os.getcwd()}\")\n",
                "print(f\"Python path includes project: {PROJECT_ROOT in sys.path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 2: GPU Memory Check"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gc\n",
                "import torch\n",
                "\n",
                "def check_gpu():\n",
                "    gc.collect()\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.empty_cache()\n",
                "        torch.cuda.synchronize()\n",
                "        \n",
                "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
                "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
                "        name = torch.cuda.get_device_name(0)\n",
                "        \n",
                "        print(f\"‚úÖ GPU: {name}\")\n",
                "        print(f\"‚úÖ Memory: {allocated:.2f}GB / {total:.2f}GB used ({100*allocated/total:.1f}%)\")\n",
                "        return True\n",
                "    else:\n",
                "        print(\"‚ùå No GPU available!\")\n",
                "        return False\n",
                "\n",
                "check_gpu()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 3: (Optional) Run Diagnostic Test\n",
                "\n",
                "Run this to verify the pipeline works for multiple sequential queries.\n",
                "This takes ~15-20 minutes but confirms everything is working."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# OPTIONAL: Run diagnostic test to verify multi-turn works\n",
                "# Uncomment the line below to run\n",
                "# %run tests/minimal_pipeline_test.py"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 4: Launch Application\n",
                "\n",
                "This launches the API server in background and then the Gradio UI.\n",
                "\n",
                "**Important:** This cell will:\n",
                "1. Initialize and load all models (takes ~5-10 minutes)\n",
                "2. Start API server on port 8000\n",
                "3. Launch Gradio UI with public share URL"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%run kaggle_launcher.py"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Alternative: Manual Launch (if above doesn't work)\n",
                "\n",
                "If the launcher above has issues, use these cells to manually control each step."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Alternative Cell 4a: Start API Server in Background Thread\n",
                "import threading\n",
                "import time\n",
                "\n",
                "def run_api_server():\n",
                "    import uvicorn\n",
                "    from api.server import create_app\n",
                "    \n",
                "    app = create_app()\n",
                "    config = uvicorn.Config(app, host=\"127.0.0.1\", port=8000, log_level=\"warning\")\n",
                "    server = uvicorn.Server(config)\n",
                "    server.run()\n",
                "\n",
                "# Start in background thread\n",
                "api_thread = threading.Thread(target=run_api_server, daemon=True)\n",
                "api_thread.start()\n",
                "\n",
                "print(\"API server starting in background...\")\n",
                "print(\"Wait for 'API ready' message before running next cell.\")\n",
                "print(\"This may take 5-10 minutes to load models.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Alternative Cell 4b: Wait for API to be ready\n",
                "import requests\n",
                "import time\n",
                "\n",
                "def wait_for_api(timeout=600):\n",
                "    print(\"Waiting for API to be ready...\")\n",
                "    start = time.time()\n",
                "    \n",
                "    while time.time() - start < timeout:\n",
                "        try:\n",
                "            resp = requests.get(\"http://127.0.0.1:8000/api/v1/ready\", timeout=5)\n",
                "            if resp.status_code == 200:\n",
                "                data = resp.json()\n",
                "                if data.get('ready'):\n",
                "                    print(\"‚úÖ API is ready!\")\n",
                "                    return True\n",
                "                else:\n",
                "                    print(f\"Loading: {data.get('message', '...')}\")\n",
                "        except:\n",
                "            pass\n",
                "        time.sleep(10)\n",
                "    \n",
                "    print(\"‚ùå API failed to start\")\n",
                "    return False\n",
                "\n",
                "wait_for_api()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Alternative Cell 4c: Launch Gradio UI\n",
                "from ui.unified_app_api import launch_app\n",
                "\n",
                "# share=True generates a public URL for Kaggle\n",
                "launch_app(share=True, server_port=7860, server_name=\"0.0.0.0\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Troubleshooting\n",
                "\n",
                "### If the API blocks after a few requests:\n",
                "1. Run `%run tests/minimal_pipeline_test.py` to verify pipeline works\n",
                "2. Check memory: `check_gpu()` should show < 90% used\n",
                "3. Restart kernel and run again\n",
                "\n",
                "### If Gradio doesn't connect:\n",
                "1. Check API is running: `requests.get('http://127.0.0.1:8000/api/v1/health')`\n",
                "2. Try the Alternative cells (4a, 4b, 4c) separately\n",
                "\n",
                "### If out of memory:\n",
                "1. Restart kernel\n",
                "2. Don't run diagnostic test before launching (saves memory)\n",
                "3. Use lower thinking_mode ('low' instead of 'high')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}