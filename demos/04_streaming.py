"""
Demo 04: Streaming Responses

Demonstrates real-time streaming output as the LLM generates the response.
This is useful for providing immediate feedback to users.

Run: python demos/04_streaming.py
"""
import sys
import os
import time

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


def main():
    print("=" * 80)
    print(" " * 25 + "STREAMING DEMO")
    print(" " * 20 + "Indonesian Legal Assistant")
    print("=" * 80)
    print()

    from pipeline import RAGPipeline

    # Initialize
    print("[1/4] Initializing pipeline...")
    pipeline = RAGPipeline()

    if not pipeline.initialize():
        print("  ❌ Failed to initialize pipeline")
        return 1

    print("  ✓ Pipeline ready")
    print()

    # Test query
    query = "Jelaskan tentang hak-hak pekerja dalam UU Ketenagakerjaan?"

    print(f'[2/4] Query: "{query}"')
    print()
    print("[3/4] Streaming response (real-time):")
    print("-" * 80)

    # Stream response
    collected_text = ""
    token_count = 0
    start_time = time.time()

    try:
        for chunk in pipeline.query(query, stream=True):
            if chunk.get('type') == 'token':
                token = chunk.get('token', '')
                print(token, end='', flush=True)
                collected_text += token
                token_count += 1
            elif chunk.get('type') == 'complete':
                total_time = time.time() - start_time
                metadata = chunk.get('metadata', {})
                break

        print()  # New line after streaming
        print("-" * 80)
        print()

    except Exception as e:
        print(f"\n\n  ⚠️  Streaming not supported by current provider: {e}")
        print("  ℹ️  Some providers (like local models) may not support streaming.")
        print()

        # Fall back to non-streaming
        print("  ⏳ Falling back to non-streaming mode...")
        result = pipeline.query(query, stream=False)

        if result and 'answer' in result:
            print("-" * 80)
            print(result['answer'])
            print("-" * 80)
            print()
            collected_text = result['answer']
            token_count = len(collected_text.split())
            total_time = result.get('metadata', {}).get('total_time', 0)
        else:
            print("  ❌ Query failed")
            pipeline.shutdown()
            return 1

    # Statistics
    print("[4/4] Streaming Statistics:")
    print(f"  Total tokens: {token_count}")
    print(f"  Total characters: {len(collected_text)}")
    print(f"  Total time: {total_time:.2f}s")
    if total_time > 0 and token_count > 0:
        print(f"  Tokens/second: {token_count/total_time:.1f}")
    print()

    # Cleanup
    print("[Cleanup] Shutting down...")
    pipeline.shutdown()
    print()

    print("=" * 80)
    print("✅ STREAMING DEMO COMPLETE")
    print("=" * 80)
    print()
    print("Key Takeaways:")
    print("  - Streaming provides real-time feedback to users")
    print("  - Tokens appear as they are generated by the LLM")
    print("  - Improves perceived responsiveness")
    print("  - Not all providers support streaming")
    print()

    return 0


if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\n⚠️  Demo interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n\n❌ Demo failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
