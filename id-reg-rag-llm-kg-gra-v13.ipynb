{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T23:13:24.134510Z",
     "iopub.status.busy": "2025-10-23T23:13:24.133938Z",
     "iopub.status.idle": "2025-10-23T23:25:12.159868Z",
     "shell.execute_reply": "2025-10-23T23:25:12.159308Z",
     "shell.execute_reply.started": "2025-10-23T23:13:24.134477Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üèõÔ∏è Enhanced KG Indonesian Legal RAG System\n",
      "================================================================================\n",
      "\n",
      "üì± Creating Gradio interface...\n",
      "‚úÖ Interface created\n",
      "\n",
      "üîß Initializing system...\n",
      "--------------------------------------------------------------------------------\n",
      "   Initializing enhanced system...\n",
      "   Loading embedding model...\n",
      "   Loading reranker model...\n",
      "   Loading LLM model on GPU with 4-bit quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'attn_factor'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1770e27238214c9c83c50081fcf3e199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìä Loading ENHANCED KG dataset...\n",
      "   üì• Loading enhanced KG dataset with streaming...\n",
      "   Limit: 100000 records\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e10a62ce5324ca293ff70e7d6ad33b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìä Processing 100,000 records in chunks of 1,000...\n",
      "   üìä Converting embeddings to numpy array...\n",
      "   üîç Processing TF-IDF vectors...\n",
      "   üóÉÔ∏è Building enhanced KG indexes...\n",
      "   ‚úÖ Ready: 100,000 records with 0 KG-enhanced\n",
      "   üóÉÔ∏è Building enhanced search indexes...\n",
      "Building search indexes...\n",
      "‚úÖ Indexes built: 10 authority tiers\n",
      "   üîó Linking context manager with knowledge graph...\n",
      "   ‚úÖ Enhanced system initialization complete!\n",
      "   üìå Embedding model: CPU\n",
      "   üìå Reranker model: CPU\n",
      "   üìå LLM model: GPU (4-bit)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ System initialized successfully!\n",
      "================================================================================\n",
      "\n",
      "üìä Dataset Statistics:\n",
      "   - Total Records: 100,000\n",
      "   - KG Enhanced: 0\n",
      "   - Enhancement Rate: 0.0%\n",
      "\n",
      "üè• Running health check...\n",
      "   Status: HEALTHY\n",
      "\n",
      "üöÄ Launching interface...\n",
      "--------------------------------------------------------------------------------\n",
      "   Server: 127.0.0.1:7860\n",
      "   Share: False\n",
      "   Auto-open browser: True\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: No context exists, first query\n",
      "DEBUG: Default case - treating as new query\n",
      "   üîç Executing normal semantic search...\n",
      "   üîç Executing normal semantic search...\n",
      "   üîç Executing normal semantic search...\n",
      "   üîç Executing normal semantic search...\n",
      "   üîç Executing normal semantic search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "KG-Enhanced Indonesian Legal RAG System - Complete Integrated Version v2\n",
    "Enhanced with advanced Knowledge Graph features from new dataset\n",
    "Maintains all original functionality + team simulation + new KG capabilities\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import gradio as gr\n",
    "from collections import defaultdict, Counter\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, TextIteratorStreamer, BitsAndBytesConfig\n",
    "import torch.nn.functional as F\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "from datasets import load_dataset\n",
    "from typing import Dict, List, Set, Tuple, Optional, Any\n",
    "from collections import deque\n",
    "import warnings\n",
    "import threading\n",
    "from threading import Thread\n",
    "import time\n",
    "import json\n",
    "import markdown\n",
    "import igraph as ig\n",
    "from community import community_louvain\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# ENHANCED CONFIGURATION WITH NEW KG DATASET\n",
    "# =============================================================================\n",
    "\n",
    "dataset_source = 'D:/AI_Workspace/10_Dataset_HF'\n",
    "embed_source = 'D:/AI_Workspace/10_Encoder_Model_HF'\n",
    "model_source = 'D:/AI_Workspace/10_Model_HF'\n",
    "\n",
    "# Dataset configuration - UPDATED TO NEW DATASET\n",
    "DATASET_NAME = dataset_source + \"/ID_REG_KG_2511\"\n",
    "HF_TOKEN = None  # Set if needed\n",
    "\n",
    "# Model configurations\n",
    "EMBEDDING_MODEL = embed_source + '/Qwen3-Embedding-0.6B'\n",
    "RERANKER_MODEL = embed_source + '/Qwen3-Reranker-0.6B'\n",
    "LLM_MODEL = model_source + \"/Deepseek_ID_Legal_Preview_FP4\"\n",
    "LLM_MODEL = model_source + \"/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\"\n",
    "MAX_LENGTH = 32768\n",
    "\n",
    "# Enhanced default configurations (unchanged)\n",
    "DEFAULT_CONFIG = {\n",
    "    'final_top_k': 3,\n",
    "    'max_rounds': 5,\n",
    "    'initial_quality': 0.95,\n",
    "    'quality_degradation': 0.1,\n",
    "    'min_quality': 0.5,\n",
    "    'parallel_research': True,\n",
    "    'research_team_size': 4,\n",
    "    'temperature': 0.7,\n",
    "    'max_new_tokens': 2048,\n",
    "    'top_p': 1.0,\n",
    "    'top_k': 20,\n",
    "    'min_p': 0.1,\n",
    "    'enable_cross_validation': True,\n",
    "    'enable_devil_advocate': True,\n",
    "    'consensus_threshold': 0.6\n",
    "}\n",
    "\n",
    "# Enhanced search phases (unchanged)\n",
    "DEFAULT_SEARCH_PHASES = {\n",
    "    'initial_scan': {\n",
    "        'candidates': 400,\n",
    "        'semantic_threshold': 0.20,\n",
    "        'keyword_threshold': 0.06,\n",
    "        'description': 'Quick broad scan like human initial reading',\n",
    "        'time_limit': 30,\n",
    "        'focus_areas': ['regulation_type', 'enacting_body'],\n",
    "        'enabled': True\n",
    "    },\n",
    "    'focused_review': {\n",
    "        'candidates': 150,\n",
    "        'semantic_threshold': 0.35,\n",
    "        'keyword_threshold': 0.12,\n",
    "        'description': 'Focused review of promising candidates',\n",
    "        'time_limit': 45,\n",
    "        'focus_areas': ['content', 'chapter', 'article'],\n",
    "        'enabled': True\n",
    "    },\n",
    "    'deep_analysis': {\n",
    "        'candidates': 60,\n",
    "        'semantic_threshold': 0.45,\n",
    "        'keyword_threshold': 0.18,\n",
    "        'description': 'Deep contextual analysis like careful reading',\n",
    "        'time_limit': 60,\n",
    "        'focus_areas': ['kg_entities', 'cross_references'],\n",
    "        'enabled': True\n",
    "    },\n",
    "    'verification': {\n",
    "        'candidates': 30,\n",
    "        'semantic_threshold': 0.55,\n",
    "        'keyword_threshold': 0.22,\n",
    "        'description': 'Final verification and cross-checking',\n",
    "        'time_limit': 30,\n",
    "        'focus_areas': ['authority_score', 'temporal_score'],\n",
    "        'enabled': True\n",
    "    },\n",
    "    'expert_review': {\n",
    "        'candidates': 45,\n",
    "        'semantic_threshold': 0.50,\n",
    "        'keyword_threshold': 0.20,\n",
    "        'description': 'Expert specialist review for complex cases',\n",
    "        'time_limit': 40,\n",
    "        'focus_areas': ['legal_richness', 'completeness_score'],\n",
    "        'enabled': False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Research team personas (unchanged)\n",
    "RESEARCH_TEAM_PERSONAS = {\n",
    "    'senior_legal_researcher': {\n",
    "        'name': 'üë®‚Äç‚öñÔ∏è Senior Legal Researcher',\n",
    "        'experience_years': 15,\n",
    "        'specialties': ['constitutional_law', 'procedural_law', 'precedent_analysis'],\n",
    "        'approach': 'systematic_thorough',\n",
    "        'strengths': ['authority_analysis', 'hierarchy_understanding', 'precedent_matching'],\n",
    "        'weaknesses': ['modern_technology', 'informal_language'],\n",
    "        'bias_towards': 'established_precedents',\n",
    "        'search_style': {\n",
    "            'semantic_weight': 0.25,\n",
    "            'authority_weight': 0.35,\n",
    "            'kg_weight': 0.25,\n",
    "            'temporal_weight': 0.15\n",
    "        },\n",
    "        'phases_preference': ['verification', 'deep_analysis'],\n",
    "        'speed_multiplier': 0.8,\n",
    "        'accuracy_bonus': 0.15\n",
    "    },\n",
    "    'junior_legal_researcher': {\n",
    "        'name': 'üë©‚Äç‚öñÔ∏è Junior Legal Researcher',\n",
    "        'experience_years': 3,\n",
    "        'specialties': ['research_methodology', 'digital_search', 'comprehensive_coverage'],\n",
    "        'approach': 'broad_comprehensive',\n",
    "        'strengths': ['semantic_search', 'keyword_matching', 'broad_coverage'],\n",
    "        'weaknesses': ['authority_evaluation', 'precedent_weighting'],\n",
    "        'bias_towards': 'comprehensive_results',\n",
    "        'search_style': {\n",
    "            'semantic_weight': 0.45,\n",
    "            'authority_weight': 0.15,\n",
    "            'kg_weight': 0.25,\n",
    "            'temporal_weight': 0.15\n",
    "        },\n",
    "        'phases_preference': ['initial_scan', 'focused_review'],\n",
    "        'speed_multiplier': 1.2,\n",
    "        'accuracy_bonus': 0.0\n",
    "    },\n",
    "    'specialist_researcher': {\n",
    "        'name': 'üìö Knowledge Graph Specialist',\n",
    "        'experience_years': 8,\n",
    "        'specialties': ['knowledge_graphs', 'semantic_analysis', 'entity_relationships'],\n",
    "        'approach': 'relationship_focused',\n",
    "        'strengths': ['kg_analysis', 'entity_extraction', 'relationship_mapping'],\n",
    "        'weaknesses': ['traditional_legal_hierarchy', 'formal_procedures'],\n",
    "        'bias_towards': 'interconnected_concepts',\n",
    "        'search_style': {\n",
    "            'semantic_weight': 0.20,\n",
    "            'authority_weight': 0.15,\n",
    "            'kg_weight': 0.50,\n",
    "            'temporal_weight': 0.15\n",
    "        },\n",
    "        'phases_preference': ['deep_analysis', 'expert_review'],\n",
    "        'speed_multiplier': 0.9,\n",
    "        'accuracy_bonus': 0.1\n",
    "    },\n",
    "    'procedural_expert': {\n",
    "        'name': '‚öñÔ∏è Procedural Law Expert',\n",
    "        'experience_years': 12,\n",
    "        'specialties': ['procedural_law', 'administrative_law', 'process_analysis'],\n",
    "        'approach': 'step_by_step_methodical',\n",
    "        'strengths': ['procedure_analysis', 'step_identification', 'requirement_mapping'],\n",
    "        'weaknesses': ['abstract_concepts', 'philosophical_law'],\n",
    "        'bias_towards': 'clear_procedures',\n",
    "        'search_style': {\n",
    "            'semantic_weight': 0.30,\n",
    "            'authority_weight': 0.25,\n",
    "            'kg_weight': 0.30,\n",
    "            'temporal_weight': 0.15\n",
    "        },\n",
    "        'phases_preference': ['focused_review', 'verification'],\n",
    "        'speed_multiplier': 1.0,\n",
    "        'accuracy_bonus': 0.08\n",
    "    },\n",
    "    'devils_advocate': {\n",
    "        'name': 'üîç Devil\\'s Advocate Reviewer',\n",
    "        'experience_years': 10,\n",
    "        'specialties': ['critical_analysis', 'alternative_interpretations', 'edge_cases'],\n",
    "        'approach': 'critical_challenging',\n",
    "        'strengths': ['weakness_identification', 'alternative_perspectives', 'critical_thinking'],\n",
    "        'weaknesses': ['positive_reinforcement', 'consensus_building'],\n",
    "        'bias_towards': 'challenging_assumptions',\n",
    "        'search_style': {\n",
    "            'semantic_weight': 0.35,\n",
    "            'authority_weight': 0.20,\n",
    "            'kg_weight': 0.30,\n",
    "            'temporal_weight': 0.15\n",
    "        },\n",
    "        'phases_preference': ['verification', 'expert_review'],\n",
    "        'speed_multiplier': 0.7,\n",
    "        'accuracy_bonus': 0.12\n",
    "    }\n",
    "}\n",
    "\n",
    "# Query-specific team compositions (unchanged)\n",
    "QUERY_TEAM_COMPOSITIONS = {\n",
    "    'specific_article': ['senior_legal_researcher', 'specialist_researcher', 'devils_advocate'],\n",
    "    'procedural': ['procedural_expert', 'junior_legal_researcher', 'senior_legal_researcher'],\n",
    "    'definitional': ['senior_legal_researcher', 'specialist_researcher', 'junior_legal_researcher'],\n",
    "    'sanctions': ['senior_legal_researcher', 'procedural_expert', 'devils_advocate'],\n",
    "    'general': ['senior_legal_researcher', 'junior_legal_researcher', 'specialist_researcher', 'procedural_expert']\n",
    "}\n",
    "\n",
    "# Human priorities (unchanged)\n",
    "DEFAULT_HUMAN_PRIORITIES = {\n",
    "    'authority_hierarchy': 0.20,\n",
    "    'temporal_relevance': 0.18,\n",
    "    'semantic_match': 0.18,\n",
    "    'knowledge_graph': 0.15,\n",
    "    'keyword_precision': 0.12,\n",
    "    'legal_completeness': 0.09,\n",
    "    'cross_validation': 0.08\n",
    "}\n",
    "\n",
    "# Query patterns (unchanged)\n",
    "QUERY_PATTERNS = {\n",
    "    'specific_article': {\n",
    "        'indicators': ['pasal', 'ayat', 'huruf', 'angka', 'butir'],\n",
    "        'priority_weights': {'authority_hierarchy': 0.30, 'semantic_match': 0.25, 'knowledge_graph': 0.20, 'keyword_precision': 0.15, 'temporal_relevance': 0.10}\n",
    "    },\n",
    "    'procedural': {\n",
    "        'indicators': ['prosedur', 'tata cara', 'persyaratan', 'cara', 'langkah'],\n",
    "        'priority_weights': {'semantic_match': 0.25, 'knowledge_graph': 0.20, 'legal_completeness': 0.20, 'temporal_relevance': 0.20, 'authority_hierarchy': 0.15}\n",
    "    },\n",
    "    'definitional': {\n",
    "        'indicators': ['definisi', 'pengertian', 'dimaksud dengan', 'adalah'],\n",
    "        'priority_weights': {'authority_hierarchy': 0.35, 'semantic_match': 0.25, 'knowledge_graph': 0.15, 'keyword_precision': 0.15, 'temporal_relevance': 0.10}\n",
    "    },\n",
    "    'sanctions': {\n",
    "        'indicators': ['sanksi', 'pidana', 'denda', 'hukuman', 'larangan'],\n",
    "        'priority_weights': {'authority_hierarchy': 0.30, 'knowledge_graph': 0.25, 'keyword_precision': 0.20, 'temporal_relevance': 0.15, 'semantic_match': 0.10}\n",
    "    },\n",
    "    'general': {\n",
    "        'indicators': [],\n",
    "        'priority_weights': DEFAULT_HUMAN_PRIORITIES\n",
    "    }\n",
    "}\n",
    "\n",
    "# NEW: Enhanced KG weights for advanced features\n",
    "KG_WEIGHTS = {\n",
    "    'direct_match': 1.0,\n",
    "    'one_hop': 0.8,\n",
    "    'two_hop': 0.6,\n",
    "    'concept_cluster': 0.7,\n",
    "    'hierarchy_boost': 0.5,\n",
    "    'temporal_relevance': 0.4,\n",
    "    'cross_reference': 0.6,\n",
    "    'domain_match': 0.5,\n",
    "    'legal_action_match': 0.7,\n",
    "    'sanction_relevance': 0.8,\n",
    "    'citation_impact': 0.4,\n",
    "    'connectivity_boost': 0.3\n",
    "}\n",
    "\n",
    "# Indonesian stopwords (unchanged)\n",
    "INDONESIAN_STOPWORDS = {\n",
    "    'yang', 'dan', 'di', 'ke', 'dari', 'dalam', 'untuk', 'pada', 'dengan', 'adalah',\n",
    "    'ini', 'itu', 'atau', 'jika', 'maka', 'akan', 'telah', 'dapat', 'harus', 'tidak',\n",
    "    'ada', 'oleh', 'sebagai', 'karena', 'sehingga', 'bahwa', 'tentang', 'antara',\n",
    "    'seperti', 'setelah', 'sebelum', 'sampai', 'hingga', 'namun', 'tetapi', 'juga'\n",
    "}\n",
    "\n",
    "# System prompt (unchanged)\n",
    "SYSTEM_PROMPT = '''Anda adalah asisten AI yang ahli di bidang hukum Indonesia. Anda dapat membantu konsultasi hukum, menjawab pertanyaan, dan memberikan analisis berdasarkan peraturan perundang-undangan yang relevan. Untuk setiap respons, Anda harus berfikir dan menjawab dengan Bahasa Indonesia, serta gunakan format: <think> ... </think> Tuliskan jawaban akhir secara jelas, ringkas, profesional, dan berempati jika diperlukan. Gunakan bahasa hukum yang mudah dipahami. Sertakan referensi hukum Indonesia yang relevan. Selalu rekomendasikan konsultasi dengan ahli hukum untuk keputusan final. Manfaatkan hubungan semantik antar konsep hukum untuk memberikan konteks yang lebih kaya.'''\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION: Regulation Patterns (Easy to adjust)\n",
    "# ============================================================================\n",
    "\n",
    "# Indonesian regulation type patterns\n",
    "REGULATION_TYPE_PATTERNS = {\n",
    "    'undang-undang': ['undang-undang', 'uu', 'undang undang'],\n",
    "    'peraturan_pemerintah': ['peraturan pemerintah', 'pp', 'perpem'],\n",
    "    'peraturan_presiden': ['peraturan presiden', 'perpres', 'pres'],\n",
    "    'peraturan_menteri': ['peraturan menteri', 'permen', 'permenkeu', 'permendikbud'],\n",
    "    'peraturan_daerah': ['peraturan daerah', 'perda', 'peraturan daerah provinsi', 'peraturan daerah kabupaten'],\n",
    "    'keputusan_presiden': ['keputusan presiden', 'keppres', 'kepres'],\n",
    "    'peraturan_gubernur': ['peraturan gubernur', 'pergub'],\n",
    "    'peraturan_bupati': ['peraturan bupati', 'perbup'],\n",
    "    'peraturan_walikota': ['peraturan walikota', 'perwali']\n",
    "}\n",
    "\n",
    "# Year separator patterns\n",
    "YEAR_SEPARATORS = ['tahun', 'th', 'th.', '/', '-']\n",
    "\n",
    "# Pronoun patterns that reference previous regulations\n",
    "REGULATION_PRONOUNS = [\n",
    "    'peraturan tersebut', 'peraturan ini', 'pp tersebut', 'pp ini',\n",
    "    'uu tersebut', 'uu ini', 'regulasi tersebut', 'regulasi ini',\n",
    "    'ketentuan tersebut', 'ketentuan ini', 'undang-undang tersebut',\n",
    "    'undang-undang ini', 'perda tersebut', 'perda ini'\n",
    "]\n",
    "\n",
    "# Keywords indicating follow-up questions\n",
    "FOLLOWUP_INDICATORS = [\n",
    "    'apa yang diatur', 'mengatur apa', 'isi dari', 'membahas apa',\n",
    "    'tentang apa', 'mengenai apa', 'berisi apa', 'materi apa',\n",
    "    'ketentuan apa', 'pasal apa', 'bagaimana dengan', 'lalu bagaimana',\n",
    "    'terus', 'kemudian', 'selanjutnya', 'dan', 'serta'\n",
    "]\n",
    "\n",
    "# Keywords indicating clarification/disagreement\n",
    "CLARIFICATION_INDICATORS = [\n",
    "    'tidak melihat', 'tidak ada', 'tidak menemukan', 'bukan tentang',\n",
    "    'seharusnya', 'maksud saya', 'yang saya maksud', 'saya kira',\n",
    "    'tetapi', 'namun', 'tapi', 'kok', 'kenapa', 'mengapa'\n",
    "]\n",
    "\n",
    "# Keywords for regulation content queries\n",
    "CONTENT_QUERY_KEYWORDS = [\n",
    "    'mengatur', 'diatur', 'pengaturan', 'ketentuan', 'isi', 'materi',\n",
    "    'membahas', 'berisi', 'tentang', 'mengenai', 'menyangkut', 'terkait'\n",
    "]\n",
    "\n",
    "# =============================================================================\n",
    "# GLOBAL VARIABLES\n",
    "# =============================================================================\n",
    "\n",
    "# Global variables for models and data\n",
    "embedding_model = None\n",
    "embedding_tokenizer = None\n",
    "reranker_model = None\n",
    "reranker_tokenizer = None\n",
    "llm_model = None\n",
    "llm_tokenizer = None\n",
    "dataset_loader = None\n",
    "search_engine = None\n",
    "knowledge_graph = None\n",
    "reranker = None\n",
    "llm_generator = None\n",
    "conversation_manager = None\n",
    "\n",
    "# Model setup variables\n",
    "device = None\n",
    "EMBEDDING_DIM = None\n",
    "token_false_id = None\n",
    "token_true_id = None\n",
    "prefix_tokens = None\n",
    "suffix_tokens = None\n",
    "\n",
    "# Progress tracking\n",
    "current_progress = {\"status\": \"Not started\", \"details\": \"\"}\n",
    "initialization_complete = False\n",
    "initialization_lock = threading.Lock()\n",
    "\n",
    "# =============================================================================\n",
    "# MEMORY MANAGEMENT\n",
    "# =============================================================================\n",
    "\n",
    "def clear_cache():\n",
    "    \"\"\"Clear GPU cache and collect garbage\"\"\"\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Cache clear failed: {e}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXPORT CONVERSATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def format_complete_search_metadata(rag_result: Dict, include_scores: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Format complete search metadata including ALL documents retrieved\n",
    "    \n",
    "    Args:\n",
    "        rag_result: Complete RAG result with all_retrieved_metadata\n",
    "        include_scores: Include detailed scoring information\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string with all search results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not rag_result or not rag_result.get('all_retrieved_metadata'):\n",
    "            return \"No search metadata available.\"\n",
    "        \n",
    "        output = []\n",
    "        output.append(\"# üìä COMPLETE SEARCH RESULTS METADATA\")\n",
    "        output.append(\"=\" * 80)\n",
    "        output.append(\"\")\n",
    "        \n",
    "        all_metadata = rag_result['all_retrieved_metadata']\n",
    "        \n",
    "        # Group by phase\n",
    "        phase_order = ['initial_scan', 'focused_review', 'deep_analysis', 'verification', 'expert_review']\n",
    "        phase_groups = {}\n",
    "        \n",
    "        for phase_key, phase_data in all_metadata.items():\n",
    "            phase_name = phase_data.get('phase', 'unknown')\n",
    "            if phase_name not in phase_groups:\n",
    "                phase_groups[phase_name] = []\n",
    "            phase_groups[phase_name].append((phase_key, phase_data))\n",
    "        \n",
    "        total_docs = 0\n",
    "        \n",
    "        # Process each phase\n",
    "        for phase_name in phase_order:\n",
    "            if phase_name not in phase_groups:\n",
    "                continue\n",
    "            \n",
    "            output.append(f\"\\n## üîç PHASE: {phase_name.upper()}\")\n",
    "            output.append(\"-\" * 80)\n",
    "            \n",
    "            phase_entries = phase_groups[phase_name]\n",
    "            \n",
    "            for phase_key, phase_data in phase_entries:\n",
    "                researcher_name = phase_data.get('researcher_name', 'Unknown Researcher')\n",
    "                researcher_id = phase_data.get('researcher', 'unknown')\n",
    "                candidates = phase_data.get('candidates', [])\n",
    "                confidence = phase_data.get('confidence', 0)\n",
    "                \n",
    "                output.append(f\"\\n### Researcher: {researcher_name}\")\n",
    "                output.append(f\"- **ID:** `{researcher_id}`\")\n",
    "                output.append(f\"- **Documents Found:** {len(candidates)}\")\n",
    "                output.append(f\"- **Confidence:** {confidence:.2%}\")\n",
    "                output.append(\"\")\n",
    "                \n",
    "                if candidates:\n",
    "                    output.append(f\"#### Retrieved Documents ({len(candidates)} total):\")\n",
    "                    output.append(\"\")\n",
    "                    \n",
    "                    # Show all documents\n",
    "                    for idx, candidate in enumerate(candidates, 1):\n",
    "                        try:\n",
    "                            record = candidate.get('record', {})\n",
    "                            \n",
    "                            # Basic info\n",
    "                            reg_type = record.get('regulation_type', 'N/A')\n",
    "                            reg_num = record.get('regulation_number', 'N/A')\n",
    "                            year = record.get('year', 'N/A')\n",
    "                            about = record.get('about', 'N/A')\n",
    "                            \n",
    "                            output.append(f\"**{idx}. {reg_type} No. {reg_num}/{year}**\")\n",
    "                            output.append(f\"   - About: {about[:100]}{'...' if len(about) > 100 else ''}\")\n",
    "                            \n",
    "                            if include_scores:\n",
    "                                # Scores\n",
    "                                composite_score = candidate.get('composite_score', 0)\n",
    "                                semantic_score = candidate.get('semantic_score', 0)\n",
    "                                keyword_score = candidate.get('keyword_score', 0)\n",
    "                                kg_score = candidate.get('kg_score', 0)\n",
    "                                \n",
    "                                output.append(f\"   - **Scores:** Composite: {composite_score:.4f} | Semantic: {semantic_score:.4f} | Keyword: {keyword_score:.4f} | KG: {kg_score:.4f}\")\n",
    "                                \n",
    "                                # KG metadata\n",
    "                                if kg_score > 0:\n",
    "                                    kg_domain = record.get('kg_primary_domain', '')\n",
    "                                    kg_hierarchy = record.get('kg_hierarchy_level', 0)\n",
    "                                    kg_authority = record.get('kg_authority_score', 0)\n",
    "                                    \n",
    "                                    output.append(f\"   - **KG Metadata:** Domain: {kg_domain} | Hierarchy: {kg_hierarchy} | Authority: {kg_authority:.3f}\")\n",
    "                                \n",
    "                                # Team consensus\n",
    "                                if candidate.get('team_consensus'):\n",
    "                                    agreement = candidate.get('researcher_agreement', 0)\n",
    "                                    output.append(f\"   - **Team Consensus:** ‚úÖ Yes ({agreement} researchers)\")\n",
    "                                \n",
    "                                # Researcher bias\n",
    "                                if candidate.get('researcher_bias_applied'):\n",
    "                                    output.append(f\"   - **Researcher:** {candidate['researcher_bias_applied']}\")\n",
    "                            \n",
    "                            # Content snippet\n",
    "                            content = record.get('content', '')\n",
    "                            if content:\n",
    "                                snippet = content[:200] + \"...\" if len(content) > 200 else content\n",
    "                                output.append(f\"   - **Content:** {snippet}\")\n",
    "                            \n",
    "                            output.append(\"\")\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            output.append(f\"   Error formatting document {idx}: {e}\")\n",
    "                            output.append(\"\")\n",
    "                    \n",
    "                    total_docs += len(candidates)\n",
    "                \n",
    "                output.append(\"\")\n",
    "        \n",
    "        # Summary\n",
    "        output.append(\"\\n## üìà SEARCH SUMMARY\")\n",
    "        output.append(\"=\" * 80)\n",
    "        output.append(f\"- **Total Documents Retrieved:** {total_docs}\")\n",
    "        output.append(f\"- **Phases Executed:** {len(phase_groups)}\")\n",
    "        output.append(f\"- **Unique Researchers:** {len(set(pd.get('researcher', 'unknown') for phases in phase_groups.values() for _, pd in phases))}\")\n",
    "        \n",
    "        return \"\\n\".join(output)\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        return f\"Error formatting search metadata: {str(e)}\\n\\n{traceback.format_exc()}\"\n",
    "\n",
    "\n",
    "def export_conversation_to_markdown(conversation_history: List[Dict], include_metadata: bool = True, include_research_process: bool = True) -> str:\n",
    "    \"\"\"FIXED: Export with thinking process properly included\"\"\"\n",
    "    try:\n",
    "        md_parts = []\n",
    "        \n",
    "        md_parts.append(\"# Legal Consultation Export\")\n",
    "        md_parts.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        md_parts.append(\"=\" * 80)\n",
    "        md_parts.append(\"\")\n",
    "        \n",
    "        for idx, entry in enumerate(conversation_history, 1):\n",
    "            md_parts.append(f\"\\n## Exchange {idx}\")\n",
    "            md_parts.append(\"-\" * 80)\n",
    "            \n",
    "            # Query\n",
    "            query = entry.get('query', '')\n",
    "            md_parts.append(f\"\\n**Question:** {query}\\n\")\n",
    "            \n",
    "            # Query metadata\n",
    "            query_type = entry.get('query_type', 'general')\n",
    "            query_entities = entry.get('query_entities', [])\n",
    "            if query_type:\n",
    "                md_parts.append(f\"**Query Type:** {query_type}\")\n",
    "                if query_entities:\n",
    "                    md_parts.append(f\"**Key Entities:** {', '.join(query_entities[:5])}\")\n",
    "                md_parts.append(\"\")\n",
    "            \n",
    "            # *** FIXED: Thinking process properly extracted and displayed ***\n",
    "            thinking = entry.get('thinking', '')\n",
    "            if thinking and include_research_process:\n",
    "                md_parts.append(\"### üß† Thinking Process\")\n",
    "                md_parts.append(\"-\" * 40)\n",
    "                # Clean up thinking content\n",
    "                thinking_clean = thinking.strip()\n",
    "                if thinking_clean:\n",
    "                    md_parts.append(thinking_clean)\n",
    "                md_parts.append(\"\")\n",
    "            \n",
    "            # Response\n",
    "            response = entry.get('response', '')\n",
    "            if response:\n",
    "                md_parts.append(\"### ‚úÖ Answer\")\n",
    "                md_parts.append(\"-\" * 40)\n",
    "                md_parts.append(response)\n",
    "                md_parts.append(\"\")\n",
    "            \n",
    "            # Legal References\n",
    "            sources_used = entry.get('sources_used', [])\n",
    "            if sources_used:\n",
    "                md_parts.append(f\"### üìö Legal References ({len(sources_used)} documents)\")\n",
    "                md_parts.append(\"-\" * 80)\n",
    "                \n",
    "                for i, source in enumerate(sources_used, 1):\n",
    "                    try:\n",
    "                        regulation = f\"{source.get('regulation_type', 'N/A')} No. {source.get('regulation_number', 'N/A')}/{source.get('year', 'N/A')}\"\n",
    "                        md_parts.append(f\"\\n**{i}. {regulation}**\")\n",
    "                        md_parts.append(f\"   - About: {source.get('about', 'N/A')}\")\n",
    "                        md_parts.append(f\"   - Enacting Body: {source.get('enacting_body', 'N/A')}\")\n",
    "                        \n",
    "                        if source.get('final_score'):\n",
    "                            md_parts.append(f\"   - Score: {source.get('final_score', 0):.4f}\")\n",
    "                        if source.get('kg_score'):\n",
    "                            md_parts.append(f\"   - KG Score: {source.get('kg_score', 0):.4f}\")\n",
    "                        if source.get('kg_primary_domain'):\n",
    "                            md_parts.append(f\"   - Domain: {source.get('kg_primary_domain')}\")\n",
    "                        if source.get('team_consensus'):\n",
    "                            md_parts.append(f\"   - Team Consensus: ‚úì Yes\")\n",
    "                        \n",
    "                        content = source.get('content', '')\n",
    "                        if content:\n",
    "                            snippet = content[:200] + \"...\" if len(content) > 200 else content\n",
    "                            md_parts.append(f\"   - Content: {snippet}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing source {i}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                md_parts.append(\"\")\n",
    "            \n",
    "            # Research process details with complete metadata\n",
    "            if include_research_process and include_metadata and entry.get('research_log'):\n",
    "                research_log = entry['research_log']\n",
    "                md_parts.append(\"### üîç Research Process Details\")\n",
    "                md_parts.append(\"-\" * 80)\n",
    "                md_parts.append(f\"- **Team Members:** {len(research_log.get('team_members', []))}\")\n",
    "                md_parts.append(f\"- **Total Documents Retrieved:** {research_log.get('total_documents_retrieved', 0)}\")\n",
    "                \n",
    "                phase_results = research_log.get('phase_results', {})\n",
    "                if phase_results:\n",
    "                    md_parts.append(f\"- **Phases Executed:** {len(phase_results)}\")\n",
    "                    \n",
    "                    # Detailed phase breakdown\n",
    "                    phase_order = ['initial_scan', 'focused_review', 'deep_analysis', 'verification', 'expert_review']\n",
    "                    phase_groups = {}\n",
    "                    \n",
    "                    for phase_key, phase_data in phase_results.items():\n",
    "                        phase_name = phase_key.split('_', 1)[-1] if '_' in phase_key else phase_key\n",
    "                        for base_phase in phase_order:\n",
    "                            if base_phase in phase_key:\n",
    "                                phase_name = base_phase\n",
    "                                break\n",
    "                        \n",
    "                        if phase_name not in phase_groups:\n",
    "                            phase_groups[phase_name] = []\n",
    "                        phase_groups[phase_name].append((phase_key, phase_data))\n",
    "                    \n",
    "                    total_retrieved = 0\n",
    "                    \n",
    "                    for phase_name in phase_order:\n",
    "                        if phase_name not in phase_groups:\n",
    "                            continue\n",
    "                        \n",
    "                        md_parts.append(f\"\\n#### üìÇ PHASE: {phase_name.upper()}\")\n",
    "                        \n",
    "                        phase_entries = phase_groups[phase_name]\n",
    "                        phase_total = sum(len(pd.get('candidates', [])) for _, pd in phase_entries)\n",
    "                        total_retrieved += phase_total\n",
    "                        \n",
    "                        for phase_key, phase_data in phase_entries:\n",
    "                            researcher_name = phase_data.get('researcher_name', 'Unknown')\n",
    "                            candidates = phase_data.get('candidates', [])\n",
    "                            confidence = phase_data.get('confidence', 0)\n",
    "                            \n",
    "                            md_parts.append(f\"\\n**{researcher_name}:** {len(candidates)} documents (Confidence: {confidence:.2%})\")\n",
    "                            \n",
    "                            # Show top 5 documents per researcher\n",
    "                            for idx_doc, candidate in enumerate(candidates[:5], 1):\n",
    "                                try:\n",
    "                                    record = candidate.get('record', {})\n",
    "                                    reg_info = f\"{record.get('regulation_type', 'N/A')} No. {record.get('regulation_number', 'N/A')}/{record.get('year', 'N/A')}\"\n",
    "                                    composite = candidate.get('composite_score', 0)\n",
    "                                    kg_score = candidate.get('kg_score', 0)\n",
    "                                    \n",
    "                                    md_parts.append(f\"   {idx_doc}. {reg_info} (Score: {composite:.3f}, KG: {kg_score:.3f})\")\n",
    "                                except Exception:\n",
    "                                    continue\n",
    "                            \n",
    "                            if len(candidates) > 5:\n",
    "                                md_parts.append(f\"   ... and {len(candidates) - 5} more documents\")\n",
    "                    \n",
    "                    md_parts.append(f\"\\n**Total Documents Retrieved:** {total_retrieved}\")\n",
    "                md_parts.append(\"\")\n",
    "        \n",
    "        md_parts.append(\"\\n\" + \"=\" * 80)\n",
    "        md_parts.append(\"\\n*Export completed successfully*\")\n",
    "        \n",
    "        return \"\\n\".join(md_parts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        error_details = traceback.format_exc()\n",
    "        return f\"# Error Generating Markdown Export\\n\\n```\\n{error_details}\\n```\"\n",
    "\n",
    "\n",
    "def export_conversation_to_json(conversation_history: List[Dict], include_full_content: bool = True) -> str:\n",
    "    \"\"\"FIXED: JSON export with proper error handling and serialization\"\"\"\n",
    "    try:\n",
    "        export_data = {\n",
    "            \"export_info\": {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"total_exchanges\": len(conversation_history),\n",
    "                \"version\": \"2.1-fixed\"\n",
    "            },\n",
    "            \"conversation\": []\n",
    "        }\n",
    "        \n",
    "        for idx, entry in enumerate(conversation_history, 1):\n",
    "            try:\n",
    "                exchange = {\n",
    "                    \"exchange_number\": idx,\n",
    "                    \"query\": str(entry.get('query', '')),\n",
    "                    \"query_type\": str(entry.get('query_type', 'general')),\n",
    "                    \"query_entities\": [str(e) for e in entry.get('query_entities', [])],\n",
    "                    \"response\": {\n",
    "                        \"thinking\": str(entry.get('thinking', '')),\n",
    "                        \"answer\": str(entry.get('response', ''))\n",
    "                    },\n",
    "                    \"legal_references\": {\n",
    "                        \"count\": len(entry.get('sources_used', [])),\n",
    "                        \"sources\": []\n",
    "                    },\n",
    "                    \"research_metadata\": {\n",
    "                        \"available\": bool(entry.get('research_log')),\n",
    "                        \"total_documents_retrieved\": 0,\n",
    "                        \"phases\": {}\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # Add legal references\n",
    "                if entry.get('sources_used'):\n",
    "                    for source in entry['sources_used']:\n",
    "                        try:\n",
    "                            source_info = {\n",
    "                                \"regulation\": {\n",
    "                                    \"type\": str(source.get('regulation_type', '')),\n",
    "                                    \"number\": str(source.get('regulation_number', '')),\n",
    "                                    \"year\": str(source.get('year', '')),\n",
    "                                    \"citation\": f\"{source.get('regulation_type', '')} No. {source.get('regulation_number', '')}/{source.get('year', '')}\"\n",
    "                                },\n",
    "                                \"about\": str(source.get('about', ''))[:500],  # Limit length\n",
    "                                \"enacting_body\": str(source.get('enacting_body', '')),\n",
    "                                \"scores\": {\n",
    "                                    \"final\": float(source.get('final_score', 0)),\n",
    "                                    \"kg\": float(source.get('kg_score', 0))\n",
    "                                },\n",
    "                                \"kg_metadata\": {\n",
    "                                    \"primary_domain\": str(source.get('kg_primary_domain', '')),\n",
    "                                    \"hierarchy_level\": int(source.get('kg_hierarchy_level', 0)),\n",
    "                                    \"team_consensus\": bool(source.get('team_consensus', False))\n",
    "                                }\n",
    "                            }\n",
    "                            \n",
    "                            if include_full_content:\n",
    "                                content = str(source.get('content', ''))\n",
    "                                source_info[\"content\"] = content[:1000] if len(content) > 1000 else content\n",
    "                            \n",
    "                            exchange[\"legal_references\"][\"sources\"].append(source_info)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing source: {e}\")\n",
    "                            continue\n",
    "                \n",
    "                # Add research metadata\n",
    "                if entry.get('research_log') and entry['research_log'].get('phase_results'):\n",
    "                    try:\n",
    "                        phase_results = entry['research_log']['phase_results']\n",
    "                        \n",
    "                        for phase_key, phase_data in phase_results.items():\n",
    "                            phase_name = str(phase_key.split('_', 1)[-1] if '_' in phase_key else phase_key)\n",
    "                            \n",
    "                            if phase_name not in exchange[\"research_metadata\"][\"phases\"]:\n",
    "                                exchange[\"research_metadata\"][\"phases\"][phase_name] = {\n",
    "                                    \"researchers\": [],\n",
    "                                    \"total_documents\": 0\n",
    "                                }\n",
    "                            \n",
    "                            researcher_data = {\n",
    "                                \"researcher_id\": str(phase_data.get('researcher', 'unknown')),\n",
    "                                \"researcher_name\": str(phase_data.get('researcher_name', 'Unknown')),\n",
    "                                \"documents_found\": int(len(phase_data.get('candidates', []))),\n",
    "                                \"confidence\": float(phase_data.get('confidence', 0)),\n",
    "                                \"documents\": []\n",
    "                            }\n",
    "                            \n",
    "                            # Add top documents\n",
    "                            for candidate in phase_data.get('candidates', [])[:10]:\n",
    "                                try:\n",
    "                                    record = candidate.get('record', {})\n",
    "                                    \n",
    "                                    doc_data = {\n",
    "                                        \"regulation\": {\n",
    "                                            \"type\": str(record.get('regulation_type', '')),\n",
    "                                            \"number\": str(record.get('regulation_number', '')),\n",
    "                                            \"year\": str(record.get('year', '')),\n",
    "                                        },\n",
    "                                        \"scores\": {\n",
    "                                            \"composite\": float(candidate.get('composite_score', 0)),\n",
    "                                            \"semantic\": float(candidate.get('semantic_score', 0)),\n",
    "                                            \"keyword\": float(candidate.get('keyword_score', 0)),\n",
    "                                            \"kg\": float(candidate.get('kg_score', 0))\n",
    "                                        },\n",
    "                                        \"team_consensus\": bool(candidate.get('team_consensus', False))\n",
    "                                    }\n",
    "                                    \n",
    "                                    if include_full_content:\n",
    "                                        content = str(record.get('content', ''))\n",
    "                                        doc_data[\"content\"] = content[:500] if len(content) > 500 else content\n",
    "                                    \n",
    "                                    researcher_data[\"documents\"].append(doc_data)\n",
    "                                    \n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error processing document: {e}\")\n",
    "                                    continue\n",
    "                            \n",
    "                            exchange[\"research_metadata\"][\"phases\"][phase_name][\"researchers\"].append(researcher_data)\n",
    "                            exchange[\"research_metadata\"][\"phases\"][phase_name][\"total_documents\"] += len(phase_data.get('candidates', []))\n",
    "                        \n",
    "                        exchange[\"research_metadata\"][\"total_documents_retrieved\"] = sum(\n",
    "                            phase[\"total_documents\"] \n",
    "                            for phase in exchange[\"research_metadata\"][\"phases\"].values()\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing research metadata: {e}\")\n",
    "                \n",
    "                export_data[\"conversation\"].append(exchange)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing exchange {idx}: {e}\")\n",
    "                # Add placeholder for failed exchange\n",
    "                export_data[\"conversation\"].append({\n",
    "                    \"exchange_number\": idx,\n",
    "                    \"error\": str(e),\n",
    "                    \"query\": str(entry.get('query', 'Error processing query'))\n",
    "                })\n",
    "                continue\n",
    "        \n",
    "        # *** FIXED: Ensure proper JSON serialization ***\n",
    "        return json.dumps(export_data, indent=2, ensure_ascii=False, default=str)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # *** FIXED: Return valid JSON even on error ***\n",
    "        import traceback\n",
    "        error_details = traceback.format_exc()\n",
    "        return json.dumps({\n",
    "            \"error\": \"Export failed\",\n",
    "            \"message\": str(e),\n",
    "            \"traceback\": error_details\n",
    "        }, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def export_conversation_to_html(conversation_history: List[Dict], include_metadata: bool = True) -> str:\n",
    "    \"\"\"FIXED: HTML export with thinking process and table rendering support\"\"\"\n",
    "    try:\n",
    "        html_content = []\n",
    "        \n",
    "        # Enhanced CSS with table support\n",
    "        html_content.append(\"\"\"<!DOCTYPE html>\n",
    "<html lang=\"id\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Legal Consultation Export</title>\n",
    "    <style>\n",
    "        * { box-sizing: border-box; }\n",
    "        \n",
    "        body {\n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "            max-width: 1200px;\n",
    "            margin: 0 auto;\n",
    "            padding: 30px 20px;\n",
    "            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);\n",
    "            line-height: 1.8;\n",
    "            color: #2c3e50;\n",
    "        }\n",
    "        \n",
    "        .container {\n",
    "            background: white;\n",
    "            padding: 30px;\n",
    "            border-radius: 12px;\n",
    "            box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n",
    "        }\n",
    "        \n",
    "        h1 { color: #1a237e; border-bottom: 4px solid #3f51b5; padding-bottom: 15px; margin-bottom: 30px; }\n",
    "        h2 { color: #283593; margin-top: 40px; padding: 15px; background: linear-gradient(to right, #e8eaf6, transparent); border-left: 5px solid #3f51b5; }\n",
    "        h3 { color: #3949ab; margin-top: 25px; padding-left: 10px; border-left: 3px solid #5c6bc0; }\n",
    "        \n",
    "        .exchange {\n",
    "            background: #f8f9fa;\n",
    "            padding: 25px;\n",
    "            margin: 30px 0;\n",
    "            border-radius: 12px;\n",
    "            border-left: 4px solid #3f51b5;\n",
    "        }\n",
    "        \n",
    "        .question {\n",
    "            background: #fff3e0;\n",
    "            padding: 15px;\n",
    "            border-radius: 8px;\n",
    "            margin: 15px 0;\n",
    "            border-left: 4px solid #ff9800;\n",
    "        }\n",
    "        \n",
    "        /* *** FIXED: Thinking process styling *** */\n",
    "        .thinking {\n",
    "            background: #e3f2fd;\n",
    "            padding: 20px;\n",
    "            border-radius: 8px;\n",
    "            margin: 20px 0;\n",
    "            border-left: 4px solid #2196f3;\n",
    "            font-family: 'Courier New', monospace;\n",
    "            white-space: pre-wrap;\n",
    "        }\n",
    "        \n",
    "        .thinking h4 {\n",
    "            margin-top: 0;\n",
    "            color: #1976d2;\n",
    "        }\n",
    "        \n",
    "        .answer {\n",
    "            background: #f0f7ff;\n",
    "            padding: 20px;\n",
    "            border-radius: 8px;\n",
    "            margin: 20px 0;\n",
    "            border-left: 4px solid #4caf50;\n",
    "        }\n",
    "        \n",
    "        /* *** FIXED: Table styling *** */\n",
    "        table {\n",
    "            width: 100%;\n",
    "            border-collapse: collapse;\n",
    "            margin: 20px 0;\n",
    "            background: white;\n",
    "            box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "        }\n",
    "        \n",
    "        table thead {\n",
    "            background: #3f51b5;\n",
    "            color: white;\n",
    "        }\n",
    "        \n",
    "        table th, table td {\n",
    "            padding: 12px 15px;\n",
    "            text-align: left;\n",
    "            border-bottom: 1px solid #e0e0e0;\n",
    "        }\n",
    "        \n",
    "        table tbody tr:hover {\n",
    "            background: #f5f5f5;\n",
    "        }\n",
    "        \n",
    "        table th {\n",
    "            font-weight: 600;\n",
    "            text-transform: uppercase;\n",
    "            font-size: 0.9em;\n",
    "            letter-spacing: 0.5px;\n",
    "        }\n",
    "        \n",
    "        /* Collapsible details */\n",
    "        details {\n",
    "            background: #f5f5f5;\n",
    "            padding: 15px;\n",
    "            border-radius: 8px;\n",
    "            margin: 15px 0;\n",
    "            border: 1px solid #e0e0e0;\n",
    "        }\n",
    "        \n",
    "        details summary {\n",
    "            font-weight: 600;\n",
    "            color: #1565c0;\n",
    "            cursor: pointer;\n",
    "            user-select: none;\n",
    "        }\n",
    "        \n",
    "        details[open] summary {\n",
    "            margin-bottom: 15px;\n",
    "            border-bottom: 2px solid #e0e0e0;\n",
    "            padding-bottom: 10px;\n",
    "        }\n",
    "        \n",
    "        .doc-item {\n",
    "            background: #fafafa;\n",
    "            padding: 12px;\n",
    "            margin: 10px 0;\n",
    "            border-radius: 6px;\n",
    "            border-left: 3px solid #1976d2;\n",
    "        }\n",
    "        \n",
    "        .score-badge {\n",
    "            display: inline-block;\n",
    "            background: #4caf50;\n",
    "            color: white;\n",
    "            padding: 3px 10px;\n",
    "            border-radius: 12px;\n",
    "            font-size: 0.85em;\n",
    "            margin-right: 5px;\n",
    "            font-weight: 600;\n",
    "        }\n",
    "        \n",
    "        code {\n",
    "            background-color: #f5f5f5;\n",
    "            padding: 3px 8px;\n",
    "            border-radius: 4px;\n",
    "            font-family: 'Courier New', monospace;\n",
    "            color: #d32f2f;\n",
    "        }\n",
    "        \n",
    "        @media print {\n",
    "            body { background: white; }\n",
    "            .exchange { page-break-inside: avoid; }\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>üìã Legal Consultation Export</h1>\n",
    "        <p><strong>Generated:</strong> \"\"\" + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + \"\"\"</p>\n",
    "        <hr>\n",
    "\"\"\")\n",
    "        \n",
    "        # Process each exchange\n",
    "        for idx, entry in enumerate(conversation_history, 1):\n",
    "            html_content.append(f'<div class=\"exchange\">')\n",
    "            html_content.append(f'<h2>Exchange {idx}</h2>')\n",
    "            \n",
    "            # Query\n",
    "            query = entry.get('query', '')\n",
    "            html_content.append(f'<div class=\"question\"><strong>Question:</strong> {query}</div>')\n",
    "            \n",
    "            # *** FIXED: Thinking process display ***\n",
    "            thinking = entry.get('thinking', '')\n",
    "            if thinking:\n",
    "                thinking_html = thinking.replace('<', '&lt;').replace('>', '&gt;')\n",
    "                html_content.append(f'<details open>')\n",
    "                html_content.append(f'<summary>üß† Thinking Process</summary>')\n",
    "                html_content.append(f'<div class=\"thinking\">{thinking_html}</div>')\n",
    "                html_content.append(f'</details>')\n",
    "            \n",
    "            # Response - convert markdown to HTML including tables\n",
    "            response = entry.get('response', '')\n",
    "            if response:\n",
    "                # *** FIXED: Markdown conversion with table extension ***\n",
    "                response_html = markdown.markdown(\n",
    "                    response, \n",
    "                    extensions=['tables', 'fenced_code', 'nl2br']\n",
    "                )\n",
    "                html_content.append(f'<div class=\"answer\"><strong>Answer:</strong><br>{response_html}</div>')\n",
    "            \n",
    "            # Legal References\n",
    "            sources_used = entry.get('sources_used', [])\n",
    "            if sources_used:\n",
    "                html_content.append(f'<details>')\n",
    "                html_content.append(f'<summary>üìö Legal References ({len(sources_used)} documents)</summary>')\n",
    "                \n",
    "                for i, source in enumerate(sources_used, 1):\n",
    "                    try:\n",
    "                        regulation = f\"{source.get('regulation_type', 'N/A')} No. {source.get('regulation_number', 'N/A')}/{source.get('year', 'N/A')}\"\n",
    "                        html_content.append(f'<div class=\"doc-item\">')\n",
    "                        html_content.append(f'<strong>{i}. {regulation}</strong>')\n",
    "                        \n",
    "                        if source.get('about'):\n",
    "                            html_content.append(f'<br><em>{source.get(\"about\")}</em>')\n",
    "                        \n",
    "                        scores_html = ''\n",
    "                        if source.get('final_score'):\n",
    "                            scores_html += f'<span class=\"score-badge\">Score: {source.get(\"final_score\", 0):.4f}</span>'\n",
    "                        if source.get('kg_score'):\n",
    "                            scores_html += f'<span class=\"score-badge\">KG: {source.get(\"kg_score\", 0):.4f}</span>'\n",
    "                        if scores_html:\n",
    "                            html_content.append(f'<br>{scores_html}')\n",
    "                        \n",
    "                        html_content.append(f'</div>')\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                \n",
    "                html_content.append(f'</details>')\n",
    "\n",
    "            # All search results metadata - DETAILED with COLLAPSIBLE PHASES\n",
    "            if include_metadata and entry.get('research_log') and entry['research_log'].get('phase_results'):\n",
    "                html_content.append('<details>')\n",
    "                html_content.append('<summary>üîç Complete Search Results (All Retrieved Documents)</summary>')\n",
    "                \n",
    "                phase_results = entry['research_log']['phase_results']\n",
    "                phase_order = ['initial_scan', 'focused_review', 'deep_analysis', 'verification', 'expert_review']\n",
    "                phase_groups = {}\n",
    "                \n",
    "                for phase_key, phase_data in phase_results.items():\n",
    "                    phase_name = phase_key.split('_', 1)[-1] if '_' in phase_key else phase_key\n",
    "                    for base_phase in phase_order:\n",
    "                        if base_phase in phase_key:\n",
    "                            phase_name = base_phase\n",
    "                            break\n",
    "                    \n",
    "                    if phase_name not in phase_groups:\n",
    "                        phase_groups[phase_name] = []\n",
    "                    phase_groups[phase_name].append((phase_key, phase_data))\n",
    "                \n",
    "                total_retrieved = 0\n",
    "                \n",
    "                for phase_name in phase_order:\n",
    "                    if phase_name not in phase_groups:\n",
    "                        continue\n",
    "                    \n",
    "                    html_content.append(f'<div class=\"phase-section\">')\n",
    "                    html_content.append(f'<div class=\"phase-title\">üîç PHASE: {phase_name.upper()}</div>')\n",
    "                    \n",
    "                    phase_entries = phase_groups[phase_name]\n",
    "                    phase_total = sum(len(pd.get('candidates', [])) for _, pd in phase_entries)\n",
    "                    total_retrieved += phase_total\n",
    "                    \n",
    "                    for phase_key, phase_data in phase_entries:\n",
    "                        researcher_name = phase_data.get('researcher_name', 'Unknown')\n",
    "                        candidates = phase_data.get('candidates', [])\n",
    "                        confidence = phase_data.get('confidence', 0)\n",
    "                        \n",
    "                        html_content.append(f'<details>')\n",
    "                        html_content.append(f'<summary>{researcher_name}: {len(candidates)} documents (Confidence: {confidence:.2%})</summary>')\n",
    "                        \n",
    "                        html_content.append(f'<div class=\"researcher-section\">')\n",
    "                        \n",
    "                        # Display ALL documents with complete details\n",
    "                        for idx_doc, candidate in enumerate(candidates, 1):\n",
    "                            try:\n",
    "                                record = candidate.get('record', {})\n",
    "                                reg_info = f\"{record.get('regulation_type', 'N/A')} No. {record.get('regulation_number', 'N/A')}/{record.get('year', 'N/A')}\"\n",
    "                                about = record.get('about', 'N/A')\n",
    "                                composite = candidate.get('composite_score', 0)\n",
    "                                kg_score = candidate.get('kg_score', 0)\n",
    "                                \n",
    "                                html_content.append(f'<div class=\"document-item\">')\n",
    "                                \n",
    "                                # Main document line with scores\n",
    "                                score_badges = f'<span class=\"score-badge\">{composite:.3f}</span>'\n",
    "                                score_badges += f'<span class=\"score-badge\">KG: {kg_score:.3f}</span>'\n",
    "                                \n",
    "                                html_content.append(f'<div class=\"doc-title\"><span class=\"doc-number\">{idx_doc}.</span> {reg_info}</div>')\n",
    "                                html_content.append(f'<div class=\"doc-detail\">{score_badges}</div>')\n",
    "                                \n",
    "                                # Additional details\n",
    "                                if about:\n",
    "                                    snippet = about[:100] + \"...\" if len(about) > 100 else about\n",
    "                                    html_content.append(f'<div class=\"doc-detail\"><strong>About:</strong> {snippet}</div>')\n",
    "                                \n",
    "                                if record.get('enacting_body'):\n",
    "                                    html_content.append(f'<div class=\"doc-detail\"><strong>Body:</strong> {record.get(\"enacting_body\")}</div>')\n",
    "                                \n",
    "                                # KG metadata\n",
    "                                kg_details = []\n",
    "                                if record.get('kg_primary_domain'):\n",
    "                                    kg_details.append(f\"Domain: {record.get('kg_primary_domain')}\")\n",
    "                                if record.get('kg_hierarchy_level'):\n",
    "                                    kg_details.append(f\"Hierarchy: Level {record.get('kg_hierarchy_level')}\")\n",
    "                                if kg_details:\n",
    "                                    html_content.append(f'<div class=\"doc-detail\"><strong>KG:</strong> {\", \".join(kg_details)}</div>')\n",
    "                                \n",
    "                                # Team consensus\n",
    "                                if candidate.get('team_consensus'):\n",
    "                                    html_content.append(f'<div class=\"doc-detail\"><span class=\"consensus-badge\">‚úì Team Consensus ({candidate.get(\"researcher_agreement\", 0)} researchers)</span></div>')\n",
    "                                \n",
    "                                html_content.append(f'</div>')\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing document {idx_doc}: {e}\")\n",
    "                                continue\n",
    "                        \n",
    "                        html_content.append(f'</div>')\n",
    "                        html_content.append(f'</details>')\n",
    "                    \n",
    "                    html_content.append(f'</div>')\n",
    "                \n",
    "                html_content.append(f'<p><strong>Total Documents Retrieved:</strong> {total_retrieved}</p>')\n",
    "                html_content.append(f'</details>')\n",
    "            \n",
    "            html_content.append('</div>')\n",
    "        \n",
    "        html_content.append(\"\"\"\n",
    "        <div style=\"text-align: center; margin-top: 50px; padding-top: 20px; border-top: 2px solid #e0e0e0; color: #757575;\">\n",
    "            <p><strong>Generated by Enhanced KG Indonesian Legal RAG System</strong></p>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\")\n",
    "        \n",
    "        return \"\\n\".join(html_content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        error_details = traceback.format_exc()\n",
    "        return f\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<head><meta charset=\"UTF-8\"><title>Export Error</title></head>\n",
    "<body>\n",
    "    <h1>Error Generating HTML Export</h1>\n",
    "    <pre>{error_details}</pre>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "\n",
    "def validate_config(config):\n",
    "    \"\"\"Validate configuration before use\"\"\"\n",
    "    issues = []\n",
    "    warnings = []\n",
    "    \n",
    "    try:\n",
    "        # Basic settings validation\n",
    "        if config.get('final_top_k', 0) < 1:\n",
    "            issues.append(\"final_top_k must be >= 1\")\n",
    "        \n",
    "        if config.get('temperature', 0) < 0 or config.get('temperature', 2) > 2:\n",
    "            issues.append(\"temperature must be between 0 and 2\")\n",
    "        \n",
    "        if config.get('max_new_tokens', 0) < 128:\n",
    "            issues.append(\"max_new_tokens must be >= 128\")\n",
    "        \n",
    "        # Team settings validation\n",
    "        if config.get('research_team_size', 0) < 1 or config.get('research_team_size', 0) > 5:\n",
    "            issues.append(\"research_team_size must be between 1 and 5\")\n",
    "        \n",
    "        if config.get('consensus_threshold', 0) < 0.3 or config.get('consensus_threshold', 0) > 0.9:\n",
    "            warnings.append(\"consensus_threshold outside recommended range (0.3-0.9)\")\n",
    "        \n",
    "        # Search phases validation\n",
    "        search_phases = config.get('search_phases', {})\n",
    "        if not search_phases:\n",
    "            issues.append(\"search_phases configuration missing\")\n",
    "        else:\n",
    "            enabled_phases = 0\n",
    "            for phase_name, phase_config in search_phases.items():\n",
    "                if phase_config.get('enabled', False):\n",
    "                    enabled_phases += 1\n",
    "                    \n",
    "                    # Validate candidates count\n",
    "                    candidates = phase_config.get('candidates', 0)\n",
    "                    if candidates < 10:\n",
    "                        issues.append(f\"{phase_name}: candidates must be >= 10\")\n",
    "                    elif candidates > 1000:\n",
    "                        warnings.append(f\"{phase_name}: high candidate count ({candidates}) may impact performance\")\n",
    "                    \n",
    "                    # Validate thresholds\n",
    "                    sem_threshold = phase_config.get('semantic_threshold', 0)\n",
    "                    if sem_threshold < 0.1 or sem_threshold > 0.9:\n",
    "                        warnings.append(f\"{phase_name}: semantic_threshold outside normal range (0.1-0.9)\")\n",
    "                    \n",
    "                    key_threshold = phase_config.get('keyword_threshold', 0)\n",
    "                    if key_threshold < 0.02 or key_threshold > 0.5:\n",
    "                        warnings.append(f\"{phase_name}: keyword_threshold outside normal range (0.02-0.5)\")\n",
    "            \n",
    "            if enabled_phases == 0:\n",
    "                issues.append(\"At least one search phase must be enabled\")\n",
    "        \n",
    "        # LLM generation parameters validation\n",
    "        if config.get('top_p', 1.0) < 0.1 or config.get('top_p', 1.0) > 1.0:\n",
    "            issues.append(\"top_p must be between 0.1 and 1.0\")\n",
    "        \n",
    "        if config.get('top_k', 20) < 1 or config.get('top_k', 20) > 100:\n",
    "            warnings.append(\"top_k outside recommended range (1-100)\")\n",
    "        \n",
    "        if config.get('min_p', 0.1) < 0.01 or config.get('min_p', 0.1) > 0.5:\n",
    "            warnings.append(\"min_p outside recommended range (0.01-0.5)\")\n",
    "        \n",
    "        # Quality degradation parameters\n",
    "        if config.get('initial_quality', 0.8) < 0.5 or config.get('initial_quality', 0.8) > 1.0:\n",
    "            warnings.append(\"initial_quality outside recommended range (0.5-1.0)\")\n",
    "        \n",
    "        if config.get('quality_degradation', 0.15) < 0.05 or config.get('quality_degradation', 0.15) > 0.3:\n",
    "            warnings.append(\"quality_degradation outside recommended range (0.05-0.3)\")\n",
    "        \n",
    "        if config.get('min_quality', 0.3) < 0.2 or config.get('min_quality', 0.3) > 0.5:\n",
    "            warnings.append(\"min_quality outside recommended range (0.2-0.5)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        issues.append(f\"Configuration validation error: {str(e)}\")\n",
    "    \n",
    "    return {\n",
    "        'valid': len(issues) == 0,\n",
    "        'issues': issues,\n",
    "        'warnings': warnings\n",
    "    }\n",
    "\n",
    "\n",
    "def apply_validated_config(config):\n",
    "    \"\"\"Apply configuration after validation\"\"\"\n",
    "    validation_result = validate_config(config)\n",
    "    \n",
    "    if not validation_result['valid']:\n",
    "        error_msg = \"Configuration validation failed:\\n\"\n",
    "        error_msg += \"\\n\".join([f\"‚ùå {issue}\" for issue in validation_result['issues']])\n",
    "        if validation_result['warnings']:\n",
    "            error_msg += \"\\n\\nWarnings:\\n\"\n",
    "            error_msg += \"\\n\".join([f\"‚ö†Ô∏è {warning}\" for warning in validation_result['warnings']])\n",
    "        raise ValueError(error_msg)\n",
    "    \n",
    "    if validation_result['warnings']:\n",
    "        print(\"Configuration warnings:\")\n",
    "        for warning in validation_result['warnings']:\n",
    "            print(f\"‚ö†Ô∏è {warning}\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "class ProgressTracker:\n",
    "    \"\"\"Thread-safe progress tracking for research process\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "        self.lock = threading.Lock()\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def add(self, message):\n",
    "        \"\"\"Add a progress message\"\"\"\n",
    "        with self.lock:\n",
    "            elapsed = time.time() - self.start_time\n",
    "            timestamp = f\"[{elapsed:.1f}s]\"\n",
    "            self.messages.append(f\"{timestamp} {message}\")\n",
    "            return self.format()\n",
    "    \n",
    "    def format(self):\n",
    "        \"\"\"Format all messages for display\"\"\"\n",
    "        with self.lock:\n",
    "            return \"\\n\".join([f\"üîÑ {m}\" for m in self.messages])\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear all messages\"\"\"\n",
    "        with self.lock:\n",
    "            self.messages = []\n",
    "            self.start_time = time.time()\n",
    "    \n",
    "    def get_duration(self):\n",
    "        \"\"\"Get elapsed time\"\"\"\n",
    "        return time.time() - self.start_time\n",
    "\n",
    "\n",
    "def check_initialization_status():\n",
    "    \"\"\"Check and report initialization status\"\"\"\n",
    "    status = {\n",
    "        'initialized': initialization_complete,\n",
    "        'components': {\n",
    "            'embedding_model': embedding_model is not None,\n",
    "            'reranker_model': reranker_model is not None,\n",
    "            'llm_model': llm_model is not None,\n",
    "            'dataset_loader': dataset_loader is not None,\n",
    "            'search_engine': search_engine is not None,\n",
    "            'knowledge_graph': knowledge_graph is not None,\n",
    "            'reranker': reranker is not None,\n",
    "            'llm_generator': llm_generator is not None,\n",
    "            'conversation_manager': conversation_manager is not None\n",
    "        },\n",
    "        'missing_components': []\n",
    "    }\n",
    "    \n",
    "    for component, loaded in status['components'].items():\n",
    "        if not loaded:\n",
    "            status['missing_components'].append(component)\n",
    "    \n",
    "    return status\n",
    "\n",
    "\n",
    "def wait_for_initialization(timeout=300, check_interval=2):\n",
    "    \"\"\"Wait for system initialization with timeout\"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while not initialization_complete:\n",
    "        if time.time() - start_time > timeout:\n",
    "            raise TimeoutError(f\"System initialization timeout after {timeout} seconds\")\n",
    "        \n",
    "        status = check_initialization_status()\n",
    "        if status['missing_components']:\n",
    "            print(f\"Waiting for: {', '.join(status['missing_components'])}\")\n",
    "        \n",
    "        time.sleep(check_interval)\n",
    "    \n",
    "    print(\"‚úÖ System initialization complete!\")\n",
    "    return True\n",
    "\n",
    "def safe_chat_wrapper(message, history, config_dict, show_thinking=True, show_sources=True, show_metadata=True):\n",
    "    \"\"\"Safe wrapper for chat function with error recovery\"\"\"\n",
    "    max_retries = 2\n",
    "    retry_count = 0\n",
    "    \n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            # Check initialization\n",
    "            if not initialization_complete:\n",
    "                yield history + [[message, \"‚è≥ System is still initializing. Please wait...\"]], \"\"\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            \n",
    "            # Validate configuration\n",
    "            try:\n",
    "                apply_validated_config(config_dict)\n",
    "            except ValueError as e:\n",
    "                yield history + [[message, f\"‚ùå Configuration error:\\n\\n{str(e)}\"]], \"\"\n",
    "                return\n",
    "            \n",
    "            # Call main chat function\n",
    "            for result in chat_with_legal_rag(message, history, config_dict, show_thinking, show_sources, show_metadata):\n",
    "                yield result\n",
    "            \n",
    "            # Success - break retry loop\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            retry_count += 1\n",
    "            error_msg = f\"‚ùå Error (attempt {retry_count}/{max_retries}): {str(e)}\"\n",
    "            \n",
    "            if retry_count < max_retries:\n",
    "                error_msg += \"\\n\\nüîÑ Retrying...\"\n",
    "                yield history + [[message, error_msg]], \"\"\n",
    "                time.sleep(1)\n",
    "                \n",
    "                # Try to recover\n",
    "                try:\n",
    "                    clear_cache()\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                except Exception:\n",
    "                    pass\n",
    "            else:\n",
    "                error_msg += \"\\n\\n‚ùå Maximum retries reached. Please try again or contact support.\"\n",
    "                yield history + [[message, error_msg]], \"\"\n",
    "                \n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "\n",
    "def system_health_check():\n",
    "    \"\"\"Comprehensive system health check\"\"\"\n",
    "    health_report = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'status': 'healthy',\n",
    "        'checks': {},\n",
    "        'warnings': [],\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check initialization\n",
    "        health_report['checks']['initialization'] = initialization_complete\n",
    "        if not initialization_complete:\n",
    "            health_report['errors'].append(\"System not initialized\")\n",
    "            health_report['status'] = 'unhealthy'\n",
    "        \n",
    "        # Check models\n",
    "        if initialization_complete:\n",
    "            health_report['checks']['embedding_model'] = embedding_model is not None\n",
    "            health_report['checks']['reranker_model'] = reranker_model is not None\n",
    "            health_report['checks']['llm_model'] = llm_model is not None\n",
    "            \n",
    "            # Check dataset\n",
    "            if dataset_loader:\n",
    "                stats = dataset_loader.get_statistics()\n",
    "                health_report['checks']['dataset_records'] = stats.get('total_records', 0)\n",
    "                health_report['checks']['kg_enhanced'] = stats.get('kg_enhanced', 0)\n",
    "                \n",
    "                if stats.get('total_records', 0) == 0:\n",
    "                    health_report['errors'].append(\"No records in dataset\")\n",
    "                    health_report['status'] = 'unhealthy'\n",
    "            \n",
    "            # Check KG cache if available\n",
    "            if knowledge_graph:\n",
    "                cache_stats = knowledge_graph.get_cache_stats()\n",
    "                health_report['checks']['kg_cache_hit_rate'] = cache_stats.get('hit_rate', 0)\n",
    "                \n",
    "                if cache_stats.get('hit_rate', 0) < 20 and cache_stats.get('total_requests', 0) > 100:\n",
    "                    health_report['warnings'].append(\"Low KG cache hit rate\")\n",
    "        \n",
    "        # Check memory\n",
    "        try:\n",
    "            import psutil\n",
    "            memory = psutil.virtual_memory()\n",
    "            health_report['checks']['memory_percent'] = memory.percent\n",
    "            \n",
    "            if memory.percent > 90:\n",
    "                health_report['warnings'].append(f\"High memory usage: {memory.percent:.1f}%\")\n",
    "                if memory.percent > 95:\n",
    "                    health_report['status'] = 'degraded'\n",
    "        except ImportError:\n",
    "            health_report['warnings'].append(\"Cannot check memory (psutil not installed)\")\n",
    "        \n",
    "        # Check GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                gpu_memory_allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "                gpu_memory_reserved = torch.cuda.memory_reserved() / 1024**3  # GB\n",
    "                \n",
    "                health_report['checks']['gpu_memory_allocated_gb'] = gpu_memory_allocated\n",
    "                health_report['checks']['gpu_memory_reserved_gb'] = gpu_memory_reserved\n",
    "                \n",
    "                if gpu_memory_allocated > 10:  # Warning at 10GB\n",
    "                    health_report['warnings'].append(f\"High GPU memory usage: {gpu_memory_allocated:.1f}GB\")\n",
    "            except Exception as e:\n",
    "                health_report['warnings'].append(f\"Cannot check GPU memory: {str(e)}\")\n",
    "        \n",
    "        # Check conversation manager\n",
    "        if conversation_manager:\n",
    "            history_count = len(conversation_manager.conversation_history)\n",
    "            health_report['checks']['conversation_history_count'] = history_count\n",
    "            \n",
    "            if history_count > 100:\n",
    "                health_report['warnings'].append(f\"Large conversation history: {history_count} exchanges\")\n",
    "        \n",
    "        # Overall status\n",
    "        if health_report['errors']:\n",
    "            health_report['status'] = 'unhealthy'\n",
    "        elif health_report['warnings']:\n",
    "            if health_report['status'] != 'degraded':\n",
    "                health_report['status'] = 'warning'\n",
    "    \n",
    "    except Exception as e:\n",
    "        health_report['status'] = 'error'\n",
    "        health_report['errors'].append(f\"Health check failed: {str(e)}\")\n",
    "    \n",
    "    return health_report\n",
    "\n",
    "\n",
    "def format_health_report(health_report):\n",
    "    \"\"\"Format health report for display\"\"\"\n",
    "    output = [\"## üè• System Health Report\", \"\"]\n",
    "    output.append(f\"**Status:** {health_report['status'].upper()}\")\n",
    "    output.append(f\"**Timestamp:** {health_report['timestamp']}\")\n",
    "    output.append(\"\")\n",
    "    \n",
    "    # Checks\n",
    "    if health_report['checks']:\n",
    "        output.append(\"### ‚úÖ System Checks\")\n",
    "        for check, value in health_report['checks'].items():\n",
    "            if isinstance(value, bool):\n",
    "                status = \"‚úÖ\" if value else \"‚ùå\"\n",
    "                output.append(f\"- {status} **{check}**: {value}\")\n",
    "            elif isinstance(value, (int, float)):\n",
    "                output.append(f\"- üìä **{check}**: {value}\")\n",
    "            else:\n",
    "                output.append(f\"- **{check}**: {value}\")\n",
    "        output.append(\"\")\n",
    "    \n",
    "    # Warnings\n",
    "    if health_report['warnings']:\n",
    "        output.append(\"### ‚ö†Ô∏è Warnings\")\n",
    "        for warning in health_report['warnings']:\n",
    "            output.append(f\"- {warning}\")\n",
    "        output.append(\"\")\n",
    "    \n",
    "    # Errors\n",
    "    if health_report['errors']:\n",
    "        output.append(\"### ‚ùå Errors\")\n",
    "        for error in health_report['errors']:\n",
    "            output.append(f\"- {error}\")\n",
    "        output.append(\"\")\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "    \n",
    "# =============================================================================\n",
    "# FIXED: EFFICIENT DATASET LOADER WITH MEMORY OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "class EnhancedKGDatasetLoader:\n",
    "    \"\"\"FIXED: Memory-efficient dataset loader with streaming and chunking\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_name, embedding_dim):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.all_records = []\n",
    "        self.embeddings = None\n",
    "        self.tfidf_matrix = None\n",
    "        self.tfidf_vectorizer = None\n",
    "        \n",
    "        # KG indexes\n",
    "        self.kg_entities_lookup = {}\n",
    "        self.kg_cross_references_lookup = {}\n",
    "        self.kg_domains_lookup = {}\n",
    "        self.kg_concept_clusters_lookup = {}\n",
    "        self.kg_legal_actions_lookup = {}\n",
    "        self.kg_sanctions_lookup = {}\n",
    "        self.kg_concept_vectors_lookup = {}\n",
    "        \n",
    "        # Numeric indexes\n",
    "        self.authority_index = {}\n",
    "        self.temporal_index = {}\n",
    "        self.kg_connectivity_index = {}\n",
    "        self.hierarchy_index = {}\n",
    "        self.domain_index = {}\n",
    "        \n",
    "    def load_from_huggingface(self, progress_callback=None, limit=100000):\n",
    "        \"\"\"FIXED: Stream dataset with aggressive memory management\"\"\"\n",
    "        try:\n",
    "            if progress_callback:\n",
    "                progress_callback(\"üì• Loading enhanced KG dataset with streaming...\")\n",
    "            \n",
    "            from datasets import load_dataset\n",
    "            import gc\n",
    "            \n",
    "            print(f\"   Limit: {limit} records\")\n",
    "\n",
    "            # Load dataset\n",
    "            dataset = load_dataset(\n",
    "                self.dataset_name, \n",
    "                split=f'train[:{limit}]',\n",
    "                streaming=False\n",
    "            )\n",
    "            \n",
    "            total_rows = len(dataset)\n",
    "            chunk_size = 1000  # Process 5K records at a time\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(f\"üìä Processing {total_rows:,} records in chunks of {chunk_size:,}...\")\n",
    "            \n",
    "            all_records_temp = []\n",
    "            embeddings_temp = []\n",
    "            tfidf_temp = []\n",
    "            \n",
    "            for start_idx in range(0, total_rows, chunk_size):\n",
    "                end_idx = min(start_idx + chunk_size, total_rows)\n",
    "                \n",
    "                #if progress_callback:\n",
    "                #    progress_callback(f\"   Processing chunk {start_idx:,} to {end_idx:,}...\")\n",
    "                \n",
    "                # Get chunk\n",
    "                chunk = dataset.select(range(start_idx, end_idx))\n",
    "                df_chunk = chunk.to_pandas()\n",
    "                \n",
    "                # Process records\n",
    "                for idx, row in df_chunk.iterrows():\n",
    "                    try:\n",
    "                        record = self._create_record(row, start_idx + idx)\n",
    "                        all_records_temp.append(record)\n",
    "                        \n",
    "                        # Extract embedding\n",
    "                        if 'embedding' in row and row['embedding'] is not None:\n",
    "                            embeddings_temp.append(row['embedding'])\n",
    "                        else:\n",
    "                            embeddings_temp.append(np.zeros(self.embedding_dim, dtype=np.float32))\n",
    "                        \n",
    "                        # Extract TF-IDF\n",
    "                        if 'tfidf_vector' in row and row['tfidf_vector'] is not None:\n",
    "                            tfidf_temp.append(row['tfidf_vector'])\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        if progress_callback:\n",
    "                            progress_callback(f\"   ‚ö†Ô∏è Skipping record {start_idx + idx}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # *** FIXED: Aggressive memory cleanup after each chunk ***\n",
    "                del df_chunk, chunk\n",
    "                gc.collect()\n",
    "                \n",
    "                # Clear CUDA cache if available\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                #if progress_callback:\n",
    "                #    progress_callback(f\"   ‚úì Processed {len(all_records_temp):,} records so far...\")\n",
    "            \n",
    "            self.all_records = all_records_temp\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(\"üìä Converting embeddings to numpy array...\")\n",
    "            \n",
    "            # *** FIXED: Convert embeddings efficiently ***\n",
    "            self.embeddings = torch.tensor(np.array(embeddings_temp, dtype=np.float32), device='cpu')\n",
    "            del embeddings_temp\n",
    "            gc.collect()\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(\"üîç Processing TF-IDF vectors...\")\n",
    "            \n",
    "            # *** FIXED: Handle TF-IDF efficiently ***\n",
    "            if tfidf_temp:\n",
    "                from scipy.sparse import csr_matrix\n",
    "                tfidf_array = np.array(tfidf_temp, dtype=np.float32)\n",
    "                self.tfidf_matrix = csr_matrix(tfidf_array)\n",
    "                del tfidf_temp, tfidf_array\n",
    "                \n",
    "                # Create vectorizer\n",
    "                tfidf_dim = self.tfidf_matrix.shape[1]\n",
    "                self._create_working_vectorizer(tfidf_dim)\n",
    "            else:\n",
    "                self._create_dummy_vectorizer()\n",
    "            \n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(\"üóÉÔ∏è Building enhanced KG indexes...\")\n",
    "            \n",
    "            self._build_enhanced_kg_indexes()\n",
    "            \n",
    "            del dataset\n",
    "            gc.collect()\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            if progress_callback:\n",
    "                kg_count = len(self.kg_entities_lookup)\n",
    "                progress_callback(f\"‚úÖ Ready: {len(self.all_records):,} records with {kg_count:,} KG-enhanced\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            if progress_callback:\n",
    "                progress_callback(f\"‚ùå Loading failed: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "    \n",
    "    def _create_record(self, row, idx):\n",
    "        \"\"\"Create record with all fields\"\"\"\n",
    "        return {\n",
    "            # Original fields\n",
    "            'global_id': row.get('global_id', idx),\n",
    "            'local_id': row.get('local_id', 1),\n",
    "            'regulation_type': str(row.get('regulation_type', 'Unknown')),\n",
    "            'enacting_body': str(row.get('enacting_body', 'Unknown')),\n",
    "            'regulation_number': str(row.get('regulation_number', 'N/A')),\n",
    "            'year': str(row.get('year', '2023')),\n",
    "            'about': str(row.get('about', ''))[:200],\n",
    "            'effective_date': str(row.get('effective_date', '2023-01-01')),\n",
    "            'chapter': str(row.get('chapter', 'N/A')),\n",
    "            'article': str(row.get('article', 'N/A')),\n",
    "            'content': str(row.get('content', ''))[:500],\n",
    "            'chunk_id': row.get('chunk_id', 1),\n",
    "            \n",
    "            # KG numeric features\n",
    "            'kg_entity_count': int(row.get('kg_entity_count', 0)),\n",
    "            'kg_cross_ref_count': int(row.get('kg_cross_ref_count', 0)),\n",
    "            'kg_primary_domain': str(row.get('kg_primary_domain', 'Unknown')),\n",
    "            'kg_domain_confidence': float(row.get('kg_domain_confidence', 0.0)),\n",
    "            'kg_cluster_count': int(row.get('kg_cluster_count', 0)),\n",
    "            'kg_cluster_diversity': float(row.get('kg_cluster_diversity', 0.0)),\n",
    "            'kg_authority_score': float(row.get('kg_authority_score', 0.5)),\n",
    "            'kg_hierarchy_level': int(row.get('kg_hierarchy_level', 5)),\n",
    "            'kg_temporal_score': float(row.get('kg_temporal_score', 0.6)),\n",
    "            'kg_years_old': int(row.get('kg_years_old', 1)),\n",
    "            'kg_legal_richness': float(row.get('kg_legal_richness', 0.0)),\n",
    "            'kg_legal_complexity': float(row.get('kg_legal_complexity', 0.0)),\n",
    "            'kg_completeness_score': float(row.get('kg_completeness_score', 0.0)),\n",
    "            'kg_connectivity_score': float(row.get('kg_connectivity_score', 0.0)),\n",
    "            'kg_has_obligations': bool(row.get('kg_has_obligations', False)),\n",
    "            'kg_has_prohibitions': bool(row.get('kg_has_prohibitions', False)),\n",
    "            'kg_has_permissions': bool(row.get('kg_has_permissions', False)),\n",
    "            'kg_pagerank': float(row.get('kg_pagerank', 0.0)),\n",
    "            'kg_degree_centrality': float(row.get('kg_degree_centrality', 0.0)),\n",
    "            \n",
    "            # Store JSON strings for lazy parsing\n",
    "            'kg_entities_json': str(row.get('kg_entities_json', '[]')),\n",
    "            'kg_cross_references_json': str(row.get('kg_cross_references_json', '[]')),\n",
    "            'kg_legal_domains_json': str(row.get('kg_legal_domains_json', '[]')),\n",
    "            'kg_concept_clusters_json': str(row.get('kg_concept_clusters_json', '{}')),\n",
    "            'kg_legal_actions_json': str(row.get('kg_legal_actions_json', '{}')),\n",
    "            'kg_sanctions_json': str(row.get('kg_sanctions_json', '{}')),\n",
    "            'kg_concept_vector_json': str(row.get('kg_concept_vector_json', '[]')),\n",
    "            'kg_citation_impact_json': str(row.get('kg_citation_impact_json', '{}'))\n",
    "        }\n",
    "    \n",
    "    def _create_working_vectorizer(self, n_features):\n",
    "        \"\"\"Create working vectorizer for TF-IDF\"\"\"\n",
    "        class WorkingVectorizer:\n",
    "            def __init__(self, features):\n",
    "                self.vocabulary_ = {}\n",
    "                self._tfidf = True\n",
    "                self.idf_ = None\n",
    "                self.stop_words_ = set()\n",
    "                self.n_features = features\n",
    "            \n",
    "            def transform(self, texts):\n",
    "                from scipy.sparse import csr_matrix\n",
    "                if isinstance(texts, str):\n",
    "                    texts = [texts]\n",
    "                return csr_matrix((len(texts), self.n_features))\n",
    "        \n",
    "        self.tfidf_vectorizer = WorkingVectorizer(n_features)\n",
    "    \n",
    "    def _create_dummy_vectorizer(self):\n",
    "        \"\"\"Create dummy vectorizer when TF-IDF not available\"\"\"\n",
    "        class DummyVectorizer:\n",
    "            def __init__(self):\n",
    "                self.vocabulary_ = {}\n",
    "                self._tfidf = True\n",
    "                self.idf_ = None\n",
    "                self.stop_words_ = set()\n",
    "                self.n_features = 20000\n",
    "            \n",
    "            def transform(self, texts):\n",
    "                from scipy.sparse import csr_matrix\n",
    "                if isinstance(texts, str):\n",
    "                    texts = [texts]\n",
    "                return csr_matrix((len(texts), self.n_features))\n",
    "        \n",
    "        self.tfidf_vectorizer = DummyVectorizer()\n",
    "    \n",
    "    def _build_enhanced_kg_indexes(self):\n",
    "        \"\"\"Build enhanced KG indexes efficiently\"\"\"\n",
    "        \n",
    "        # Build lookup dictionaries for KG features (lazy parsing)\n",
    "        for record in self.all_records:\n",
    "            try:\n",
    "                doc_id = record['global_id']\n",
    "                \n",
    "                # Store JSON strings - will parse on demand\n",
    "                if record.get('kg_entities_json', '[]') != '[]':\n",
    "                    self.kg_entities_lookup[doc_id] = record['kg_entities_json']\n",
    "                \n",
    "                if record.get('kg_cross_references_json', '[]') != '[]':\n",
    "                    self.kg_cross_references_lookup[doc_id] = record['kg_cross_references_json']\n",
    "                \n",
    "                if record.get('kg_legal_domains_json', '[]') != '[]':\n",
    "                    self.kg_domains_lookup[doc_id] = record['kg_legal_domains_json']\n",
    "                \n",
    "                if record.get('kg_concept_clusters_json', '{}') != '{}':\n",
    "                    self.kg_concept_clusters_lookup[doc_id] = record['kg_concept_clusters_json']\n",
    "                \n",
    "                if record.get('kg_legal_actions_json', '{}') != '{}':\n",
    "                    self.kg_legal_actions_lookup[doc_id] = record['kg_legal_actions_json']\n",
    "                \n",
    "                if record.get('kg_sanctions_json', '{}') != '{}':\n",
    "                    self.kg_sanctions_lookup[doc_id] = record['kg_sanctions_json']\n",
    "                \n",
    "                if record.get('kg_concept_vector_json', '[]') != '[]':\n",
    "                    self.kg_concept_vectors_lookup[doc_id] = record['kg_concept_vector_json']\n",
    "                \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        # Build numeric indexes for fast filtering\n",
    "        self.authority_index = defaultdict(list)\n",
    "        self.temporal_index = defaultdict(list)\n",
    "        self.kg_connectivity_index = defaultdict(list)\n",
    "        self.hierarchy_index = defaultdict(list)\n",
    "        self.domain_index = defaultdict(list)\n",
    "        \n",
    "        for i, record in enumerate(self.all_records):\n",
    "            try:\n",
    "                # Authority tier\n",
    "                authority_tier = max(0, min(10, int(record['kg_authority_score'] * 10)))\n",
    "                self.authority_index[authority_tier].append(i)\n",
    "                \n",
    "                # Temporal tier\n",
    "                temporal_tier = max(0, min(10, int(record['kg_temporal_score'] * 10)))\n",
    "                self.temporal_index[temporal_tier].append(i)\n",
    "                \n",
    "                # KG connectivity tier\n",
    "                kg_tier = max(0, min(10, int(record['kg_connectivity_score'] * 10)))\n",
    "                self.kg_connectivity_index[kg_tier].append(i)\n",
    "                \n",
    "                # Hierarchy tier\n",
    "                hierarchy_tier = max(1, min(10, record['kg_hierarchy_level']))\n",
    "                self.hierarchy_index[hierarchy_tier].append(i)\n",
    "                \n",
    "                # Domain index\n",
    "                domain = record['kg_primary_domain']\n",
    "                self.domain_index[domain].append(i)\n",
    "                \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        # Convert to dict\n",
    "        self.authority_index = dict(self.authority_index)\n",
    "        self.temporal_index = dict(self.temporal_index)\n",
    "        self.kg_connectivity_index = dict(self.kg_connectivity_index)\n",
    "        self.hierarchy_index = dict(self.hierarchy_index)\n",
    "        self.domain_index = dict(self.domain_index)\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Get enhanced dataset statistics\"\"\"\n",
    "        if not self.all_records:\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            total_records = len(self.all_records)\n",
    "            kg_enhanced = len(self.kg_entities_lookup)\n",
    "            \n",
    "            authority_scores = [r['kg_authority_score'] for r in self.all_records]\n",
    "            temporal_scores = [r['kg_temporal_score'] for r in self.all_records]\n",
    "            connectivity_scores = [r['kg_connectivity_score'] for r in self.all_records]\n",
    "            entity_counts = [r['kg_entity_count'] for r in self.all_records]\n",
    "            cross_ref_counts = [r['kg_cross_ref_count'] for r in self.all_records]\n",
    "            \n",
    "            return {\n",
    "                'total_records': total_records,\n",
    "                'kg_enhanced': kg_enhanced,\n",
    "                'kg_enhancement_rate': kg_enhanced / total_records if total_records > 0 else 0,\n",
    "                'embeddings_shape': self.embeddings.shape if self.embeddings is not None else None,\n",
    "                'tfidf_shape': self.tfidf_matrix.shape if self.tfidf_matrix is not None else None,\n",
    "                'tfidf_enabled': self.tfidf_matrix is not None,\n",
    "                'memory_optimized': True,\n",
    "                'authority_tiers': len(self.authority_index),\n",
    "                'temporal_tiers': len(self.temporal_index),\n",
    "                'kg_connectivity_tiers': len(self.kg_connectivity_index),\n",
    "                'hierarchy_tiers': len(self.hierarchy_index),\n",
    "                'unique_domains': len(self.domain_index),\n",
    "                'avg_authority_score': np.mean(authority_scores) if authority_scores else 0,\n",
    "                'avg_temporal_score': np.mean(temporal_scores) if temporal_scores else 0,\n",
    "                'avg_connectivity_score': np.mean(connectivity_scores) if connectivity_scores else 0,\n",
    "                'avg_entities_per_doc': np.mean(entity_counts) if entity_counts else 0,\n",
    "                'avg_cross_refs_per_doc': np.mean(cross_ref_counts) if cross_ref_counts else 0,\n",
    "                'has_obligations': sum(1 for r in self.all_records if r['kg_has_obligations']),\n",
    "                'has_prohibitions': sum(1 for r in self.all_records if r['kg_has_prohibitions']),\n",
    "                'has_permissions': sum(1 for r in self.all_records if r['kg_has_permissions'])\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "\n",
    "# =============================================================================\n",
    "# ENHANCED KNOWLEDGE GRAPH WITH NEW FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "class EnhancedKnowledgeGraph:\n",
    "    \"\"\"Enhanced KG class with caching\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_loader):\n",
    "        self.dataset_loader = dataset_loader\n",
    "        self.entities_lookup = dataset_loader.kg_entities_lookup\n",
    "        self.cross_refs_lookup = dataset_loader.kg_cross_references_lookup\n",
    "        self.domains_lookup = dataset_loader.kg_domains_lookup\n",
    "        self.clusters_lookup = dataset_loader.kg_concept_clusters_lookup\n",
    "        self.legal_actions_lookup = dataset_loader.kg_legal_actions_lookup\n",
    "        self.sanctions_lookup = dataset_loader.kg_sanctions_lookup\n",
    "        self.concept_vectors_lookup = dataset_loader.kg_concept_vectors_lookup\n",
    "        \n",
    "        # Caching\n",
    "        self._parse_cache = {}\n",
    "        self._cache_hits = 0\n",
    "        self._cache_misses = 0\n",
    "    \n",
    "    def extract_entities_from_text(self, text):\n",
    "        \"\"\"Enhanced entity extraction for ANY Indonesian regulation\"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            text_lower = str(text).lower()\n",
    "            entities = []\n",
    "            \n",
    "            # Pattern 1: Standard format - \"Type No. X Tahun YYYY\"\n",
    "            for reg_type, patterns in REGULATION_TYPE_PATTERNS.items():\n",
    "                for pattern in patterns:\n",
    "                    # Build flexible regex pattern\n",
    "                    regex_pattern = (\n",
    "                        rf'{re.escape(pattern)}\\s*'\n",
    "                        r'(?:nomor|no\\.?|num\\.?)?\\s*'\n",
    "                        r'(\\d+)\\s*'\n",
    "                        r'(?:' + '|'.join([re.escape(sep) for sep in YEAR_SEPARATORS]) + r')?\\s*'\n",
    "                        r'(\\d{4})?'\n",
    "                    )\n",
    "                    \n",
    "                    matches = re.finditer(regex_pattern, text_lower, re.IGNORECASE)\n",
    "                    for match in matches:\n",
    "                        number = match.group(1)\n",
    "                        year = match.group(2) if match.group(2) else ''\n",
    "                        \n",
    "                        entity_text = f\"{pattern} {number}\"\n",
    "                        if year:\n",
    "                            entity_text += f\" tahun {year}\"\n",
    "                        \n",
    "                        entities.append((entity_text, 'regulation_reference'))\n",
    "            \n",
    "            # Pattern 2: Pasal references\n",
    "            pasal_pattern = r'pasal\\s*(\\d+)(?:\\s*ayat\\s*\\((\\d+)\\))?(?:\\s*huruf\\s*([a-z]))?'\n",
    "            matches = re.finditer(pasal_pattern, text_lower)\n",
    "            for match in matches:\n",
    "                entities.append((match.group(0), 'article_reference'))\n",
    "            \n",
    "            # Pattern 3: Bab references\n",
    "            bab_pattern = r'bab\\s+([IVX]+|\\d+)'\n",
    "            matches = re.finditer(bab_pattern, text_lower)\n",
    "            for match in matches:\n",
    "                entities.append((match.group(0), 'chapter_reference'))\n",
    "            \n",
    "            return entities\n",
    "        except Exception as e:\n",
    "            print(f\"Error in entity extraction: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_parsed_kg_data(self, doc_id, data_type='entities'):\n",
    "        \"\"\"Parse KG JSON data on demand WITH CACHING\"\"\"\n",
    "        # Create cache key\n",
    "        cache_key = f\"{doc_id}_{data_type}\"\n",
    "        \n",
    "        # Check cache first\n",
    "        if cache_key in self._parse_cache:\n",
    "            self._cache_hits += 1\n",
    "            return self._parse_cache[cache_key]\n",
    "        \n",
    "        self._cache_misses += 1\n",
    "        \n",
    "        try:\n",
    "            result = None\n",
    "            \n",
    "            if data_type == 'entities' and doc_id in self.entities_lookup:\n",
    "                result = json.loads(self.entities_lookup[doc_id])\n",
    "            elif data_type == 'cross_refs' and doc_id in self.cross_refs_lookup:\n",
    "                result = json.loads(self.cross_refs_lookup[doc_id])\n",
    "            elif data_type == 'domains' and doc_id in self.domains_lookup:\n",
    "                result = json.loads(self.domains_lookup[doc_id])\n",
    "            elif data_type == 'clusters' and doc_id in self.clusters_lookup:\n",
    "                result = json.loads(self.clusters_lookup[doc_id])\n",
    "            elif data_type == 'legal_actions' and doc_id in self.legal_actions_lookup:\n",
    "                result = json.loads(self.legal_actions_lookup[doc_id])\n",
    "            elif data_type == 'sanctions' and doc_id in self.sanctions_lookup:\n",
    "                result = json.loads(self.sanctions_lookup[doc_id])\n",
    "            elif data_type == 'concept_vector' and doc_id in self.concept_vectors_lookup:\n",
    "                result = json.loads(self.concept_vectors_lookup[doc_id])\n",
    "            \n",
    "            # Store in cache (limit cache size)\n",
    "            if result is not None:\n",
    "                if len(self._parse_cache) > 1000:  # Max 1000 cached entries\n",
    "                    # Remove oldest 100 entries\n",
    "                    keys_to_remove = list(self._parse_cache.keys())[:100]\n",
    "                    for key in keys_to_remove:\n",
    "                        del self._parse_cache[key]\n",
    "                \n",
    "                self._parse_cache[cache_key] = result\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing KG data for {doc_id}/{data_type}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_cache_stats(self):\n",
    "        \"\"\"Get cache performance statistics\"\"\"\n",
    "        total_requests = self._cache_hits + self._cache_misses\n",
    "        hit_rate = (self._cache_hits / total_requests * 100) if total_requests > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'cache_size': len(self._parse_cache),\n",
    "            'cache_hits': self._cache_hits,\n",
    "            'cache_misses': self._cache_misses,\n",
    "            'hit_rate': hit_rate,\n",
    "            'total_requests': total_requests\n",
    "        }\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the parse cache\"\"\"\n",
    "        self._parse_cache = {}\n",
    "        self._cache_hits = 0\n",
    "        self._cache_misses = 0\n",
    "        \n",
    "    def extract_entities_from_text(self, text):\n",
    "        \"\"\"Enhanced entity extraction for ANY Indonesian regulation\"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            text_lower = str(text).lower()\n",
    "            entities = []\n",
    "            \n",
    "            # Pattern 1: Standard format - \"Type No. X Tahun YYYY\"\n",
    "            for reg_type, patterns in REGULATION_TYPE_PATTERNS.items():\n",
    "                for pattern in patterns:\n",
    "                    # Build flexible regex pattern\n",
    "                    # Matches: \"PP No. 41 Tahun 2009\", \"pp no 41 tahun 2009\", \"PP 41/2009\", etc.\n",
    "                    regex_pattern = (\n",
    "                        rf'{re.escape(pattern)}\\s*'  # Regulation type\n",
    "                        r'(?:nomor|no\\.?|num\\.?)?\\s*'  # Optional \"nomor\"/\"no\"\n",
    "                        r'(\\d+)\\s*'  # Number\n",
    "                        r'(?:' + '|'.join([re.escape(sep) for sep in YEAR_SEPARATORS]) + r')?\\s*'  # Separator\n",
    "                        r'(\\d{4})?'  # Optional year\n",
    "                    )\n",
    "                    \n",
    "                    matches = re.finditer(regex_pattern, text_lower, re.IGNORECASE)\n",
    "                    for match in matches:\n",
    "                        number = match.group(1)\n",
    "                        year = match.group(2) if match.group(2) else ''\n",
    "                        \n",
    "                        # Create normalized entity\n",
    "                        entity_text = f\"{pattern} {number}\"\n",
    "                        if year:\n",
    "                            entity_text += f\" tahun {year}\"\n",
    "                        \n",
    "                        entities.append((entity_text, 'regulation_reference'))\n",
    "            \n",
    "            # Pattern 2: Pasal (Article) references\n",
    "            pasal_pattern = r'pasal\\s*(\\d+)(?:\\s*ayat\\s*\\((\\d+)\\))?(?:\\s*huruf\\s*([a-z]))?'\n",
    "            matches = re.finditer(pasal_pattern, text_lower)\n",
    "            for match in matches:\n",
    "                entities.append((match.group(0), 'article_reference'))\n",
    "            \n",
    "            # Pattern 3: Bab (Chapter) references\n",
    "            bab_pattern = r'bab\\s+([IVX]+|\\d+)'\n",
    "            matches = re.finditer(bab_pattern, text_lower)\n",
    "            for match in matches:\n",
    "                entities.append((match.group(0), 'chapter_reference'))\n",
    "            \n",
    "            return entities\n",
    "        except Exception as e:\n",
    "            print(f\"Error in entity extraction: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def calculate_enhanced_kg_score(self, query_entities, record, query_type='general'):\n",
    "        \"\"\"Enhanced KG scoring with new dataset features\"\"\"\n",
    "        try:\n",
    "            total_score = 0.0\n",
    "            doc_id = record['global_id']\n",
    "            \n",
    "            # 1. Entity matching (enhanced)\n",
    "            doc_entities = self.get_parsed_kg_data(doc_id, 'entities')\n",
    "            if doc_entities and query_entities:\n",
    "                entity_score = self._calculate_entity_match(query_entities, doc_entities)\n",
    "                total_score += entity_score * KG_WEIGHTS['direct_match']\n",
    "            \n",
    "            # 2. Cross-reference boost\n",
    "            cross_refs = self.get_parsed_kg_data(doc_id, 'cross_refs')\n",
    "            if cross_refs:\n",
    "                cross_ref_score = self._calculate_cross_ref_relevance(query_entities, cross_refs)\n",
    "                total_score += cross_ref_score * KG_WEIGHTS['cross_reference']\n",
    "            \n",
    "            # 3. Domain matching\n",
    "            domains = self.get_parsed_kg_data(doc_id, 'domains')\n",
    "            if domains:\n",
    "                domain_score = self._calculate_domain_relevance(query_type, domains, record)\n",
    "                total_score += domain_score * KG_WEIGHTS['domain_match']\n",
    "            \n",
    "            # 4. Legal actions matching (for procedural/sanctions queries)\n",
    "            if query_type in ['procedural', 'sanctions']:\n",
    "                legal_actions = self.get_parsed_kg_data(doc_id, 'legal_actions')\n",
    "                if legal_actions:\n",
    "                    action_score = self._calculate_legal_action_relevance(query_type, legal_actions, record)\n",
    "                    total_score += action_score * KG_WEIGHTS['legal_action_match']\n",
    "            \n",
    "            # 5. Sanctions matching (for sanctions queries)\n",
    "            if query_type == 'sanctions':\n",
    "                sanctions = self.get_parsed_kg_data(doc_id, 'sanctions')\n",
    "                if sanctions:\n",
    "                    sanction_score = self._calculate_sanction_relevance(sanctions)\n",
    "                    total_score += sanction_score * KG_WEIGHTS['sanction_relevance']\n",
    "            \n",
    "            # 6. Concept clusters matching\n",
    "            clusters = self.get_parsed_kg_data(doc_id, 'clusters')\n",
    "            if clusters and query_entities:\n",
    "                cluster_score = self._calculate_cluster_relevance(query_entities, clusters)\n",
    "                total_score += cluster_score * KG_WEIGHTS['concept_cluster']\n",
    "            \n",
    "            # 7. Hierarchy boost\n",
    "            hierarchy_score = self._calculate_hierarchy_boost(record)\n",
    "            total_score += hierarchy_score * KG_WEIGHTS['hierarchy_boost']\n",
    "            \n",
    "            # 8. Connectivity boost\n",
    "            connectivity_score = record.get('kg_connectivity_score', 0.0)\n",
    "            total_score += connectivity_score * KG_WEIGHTS['connectivity_boost']\n",
    "            \n",
    "            # 9. Citation impact (if available)\n",
    "            if record.get('kg_pagerank', 0.0) > 0:\n",
    "                citation_score = record['kg_pagerank']\n",
    "                total_score += citation_score * KG_WEIGHTS['citation_impact']\n",
    "            \n",
    "            return min(1.0, total_score)\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "    \n",
    "    def _calculate_entity_match(self, query_entities, doc_entities):\n",
    "        \"\"\"Calculate entity matching score\"\"\"\n",
    "        try:\n",
    "            if not query_entities or not doc_entities:\n",
    "                return 0.0\n",
    "            \n",
    "            query_entity_set = {str(entity).lower() for entity in query_entities}\n",
    "            \n",
    "            # Handle both list of strings and list of dicts\n",
    "            doc_entity_set = set()\n",
    "            for entity in doc_entities:\n",
    "                if isinstance(entity, dict):\n",
    "                    doc_entity_set.add(str(entity.get('text', '')).lower())\n",
    "                else:\n",
    "                    doc_entity_set.add(str(entity).lower())\n",
    "            \n",
    "            overlap = query_entity_set & doc_entity_set\n",
    "            if overlap:\n",
    "                return min(1.0, len(overlap) / len(query_entity_set))\n",
    "            \n",
    "            # Partial matching\n",
    "            partial_score = 0.0\n",
    "            for q_entity in query_entities:\n",
    "                for d_entity in doc_entity_set:\n",
    "                    if str(q_entity).lower() in str(d_entity).lower():\n",
    "                        partial_score += 0.5\n",
    "                        break\n",
    "            \n",
    "            return min(1.0, partial_score / len(query_entities)) if query_entities else 0.0\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "    \n",
    "    def _calculate_cross_ref_relevance(self, query_entities, cross_refs):\n",
    "        \"\"\"Calculate cross-reference relevance\"\"\"\n",
    "        try:\n",
    "            if not cross_refs:\n",
    "                return 0.0\n",
    "            \n",
    "            # More cross-references = better connected document\n",
    "            ref_count = len(cross_refs) if isinstance(cross_refs, list) else 0\n",
    "            return min(1.0, ref_count / 10.0)  # Normalize to 0-1\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "    \n",
    "    def _calculate_domain_relevance(self, query_type, domains, record):\n",
    "        \"\"\"Calculate domain relevance\"\"\"\n",
    "        try:\n",
    "            # Map query types to relevant domains\n",
    "            query_domain_map = {\n",
    "                'procedural': ['administrative', 'procedural', 'governance'],\n",
    "                'sanctions': ['criminal', 'administrative', 'sanctions'],\n",
    "                'definitional': ['general', 'definitions', 'terminology'],\n",
    "                'specific_article': ['all'],\n",
    "                'general': ['all']\n",
    "            }\n",
    "            \n",
    "            relevant_domains = query_domain_map.get(query_type, ['all'])\n",
    "            \n",
    "            if 'all' in relevant_domains:\n",
    "                return record.get('kg_domain_confidence', 0.5)\n",
    "            \n",
    "            # Check if document domains match relevant domains\n",
    "            if isinstance(domains, list):\n",
    "                for domain_info in domains:\n",
    "                    if isinstance(domain_info, dict):\n",
    "                        domain_name = domain_info.get('domain', '').lower()\n",
    "                        if any(rel_dom in domain_name for rel_dom in relevant_domains):\n",
    "                            return domain_info.get('confidence', 0.5)\n",
    "            \n",
    "            return 0.0\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "    \n",
    "    def _calculate_legal_action_relevance(self, query_type, legal_actions, record):\n",
    "        \"\"\"Calculate legal action relevance\"\"\"\n",
    "        try:\n",
    "            if query_type == 'procedural':\n",
    "                # Check for procedural actions\n",
    "                if record.get('kg_has_obligations', False):\n",
    "                    return 0.8\n",
    "                if record.get('kg_has_permissions', False):\n",
    "                    return 0.6\n",
    "            elif query_type == 'sanctions':\n",
    "                # Check for prohibitions/sanctions\n",
    "                if record.get('kg_has_prohibitions', False):\n",
    "                    return 0.9\n",
    "            \n",
    "            return 0.0\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "    \n",
    "    def _calculate_sanction_relevance(self, sanctions):\n",
    "        \"\"\"Calculate sanction relevance\"\"\"\n",
    "        try:\n",
    "            if not sanctions:\n",
    "                return 0.0\n",
    "            \n",
    "            # Check if sanctions data is available\n",
    "            if isinstance(sanctions, dict):\n",
    "                if sanctions.get('has_sanctions', False):\n",
    "                    return 0.9\n",
    "                sanction_count = len(sanctions.get('sanctions', []))\n",
    "                return min(1.0, sanction_count / 3.0)\n",
    "            \n",
    "            return 0.0\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "    \n",
    "    def _calculate_cluster_relevance(self, query_entities, clusters):\n",
    "        \"\"\"Calculate concept cluster relevance\"\"\"\n",
    "        try:\n",
    "            if not clusters or not query_entities:\n",
    "                return 0.0\n",
    "            \n",
    "            # Extract concepts from clusters\n",
    "            cluster_concepts = set()\n",
    "            if isinstance(clusters, dict):\n",
    "                for cluster_name, concepts in clusters.items():\n",
    "                    if isinstance(concepts, list):\n",
    "                        cluster_concepts.update([str(c).lower() for c in concepts])\n",
    "            \n",
    "            # Check overlap with query entities\n",
    "            query_set = {str(e).lower() for e in query_entities}\n",
    "            overlap = query_set & cluster_concepts\n",
    "            \n",
    "            if overlap:\n",
    "                return min(1.0, len(overlap) / len(query_set))\n",
    "            \n",
    "            return 0.0\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "    \n",
    "    def _calculate_hierarchy_boost(self, record):\n",
    "        \"\"\"Calculate hierarchy-based boost\"\"\"\n",
    "        try:\n",
    "            # Lower hierarchy level = higher authority\n",
    "            hierarchy_level = record.get('kg_hierarchy_level', 5)\n",
    "            # Normalize: level 1 = 1.0, level 10 = 0.1\n",
    "            return max(0.1, (11 - hierarchy_level) / 10.0)\n",
    "        except Exception:\n",
    "            return 0.5\n",
    "\n",
    "\n",
    "    def follow_citation_chain(self, seed_document_ids, max_depth=2):\n",
    "        \"\"\"Follow citation chains from seed documents\"\"\"\n",
    "        try:\n",
    "            citation_network = {}\n",
    "            visited = set()\n",
    "            \n",
    "            def traverse_citations(doc_id, depth):\n",
    "                if depth > max_depth or doc_id in visited:\n",
    "                    return []\n",
    "                \n",
    "                visited.add(doc_id)\n",
    "                related_docs = []\n",
    "                \n",
    "                # Get cross-references\n",
    "                cross_refs = self.get_parsed_kg_data(doc_id, 'cross_refs')\n",
    "                \n",
    "                if cross_refs:\n",
    "                    for ref in cross_refs[:5]:  # Limit to top 5 per document\n",
    "                        try:\n",
    "                            if isinstance(ref, dict):\n",
    "                                ref_id = ref.get('target_id')\n",
    "                            else:\n",
    "                                ref_id = str(ref)\n",
    "                            \n",
    "                            if ref_id and ref_id not in visited:\n",
    "                                related_docs.append({\n",
    "                                    'doc_id': ref_id,\n",
    "                                    'citation_depth': depth,\n",
    "                                    'cited_by': doc_id\n",
    "                                })\n",
    "                                \n",
    "                                # Recursive traversal\n",
    "                                if depth < max_depth:\n",
    "                                    deeper_docs = traverse_citations(ref_id, depth + 1)\n",
    "                                    related_docs.extend(deeper_docs)\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                \n",
    "                return related_docs\n",
    "            \n",
    "            # Start traversal from each seed\n",
    "            for seed_id in seed_document_ids:\n",
    "                if isinstance(seed_id, dict):\n",
    "                    seed_id = seed_id.get('global_id', seed_id)\n",
    "                \n",
    "                citation_network[seed_id] = traverse_citations(seed_id, 1)\n",
    "            \n",
    "            return citation_network\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error following citations: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def boost_cited_documents(self, candidates, citation_network):\n",
    "        \"\"\"Boost documents that appear in citation chains\"\"\"\n",
    "        try:\n",
    "            # Collect all documents in citation network\n",
    "            cited_docs = set()\n",
    "            citation_depths = {}\n",
    "            \n",
    "            for seed_id, citations in citation_network.items():\n",
    "                for citation in citations:\n",
    "                    doc_id = citation['doc_id']\n",
    "                    depth = citation['citation_depth']\n",
    "                    cited_docs.add(doc_id)\n",
    "                    \n",
    "                    # Track minimum depth (closer = better)\n",
    "                    if doc_id not in citation_depths or depth < citation_depths[doc_id]:\n",
    "                        citation_depths[doc_id] = depth\n",
    "            \n",
    "            # Apply citation bonuses\n",
    "            for candidate in candidates:\n",
    "                doc_id = candidate['record'].get('global_id')\n",
    "                \n",
    "                if doc_id in cited_docs:\n",
    "                    depth = citation_depths[doc_id]\n",
    "                    # Closer citations get higher boost\n",
    "                    citation_bonus = 0.15 / depth if depth > 0 else 0.15\n",
    "                    \n",
    "                    # Update scores\n",
    "                    if 'final_consensus_score' in candidate:\n",
    "                        candidate['final_consensus_score'] = min(1.0, \n",
    "                            candidate['final_consensus_score'] + citation_bonus)\n",
    "                    if 'composite_score' in candidate:\n",
    "                        candidate['composite_score'] = min(1.0,\n",
    "                            candidate['composite_score'] + citation_bonus)\n",
    "                    \n",
    "                    candidate['in_citation_chain'] = True\n",
    "                    candidate['citation_depth'] = depth\n",
    "            \n",
    "            return candidates\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error boosting cited documents: {e}\")\n",
    "            return candidates\n",
    "\n",
    "\n",
    "    def extract_regulation_references_with_confidence(self, text):\n",
    "            \"\"\"\n",
    "            Extract regulation references with confidence scores.\n",
    "            Returns: List of (regulation_dict, confidence_score)\n",
    "            \"\"\"\n",
    "            if not text or pd.isna(text):\n",
    "                return []\n",
    "            \n",
    "            try:\n",
    "                text_lower = str(text).lower()\n",
    "                references = []\n",
    "                \n",
    "                # Pattern 1: Complete reference with year (HIGHEST CONFIDENCE)\n",
    "                # e.g., \"UU No. 13 Tahun 2003\", \"PP 41/2009\"\n",
    "                for reg_type, patterns in REGULATION_TYPE_PATTERNS.items():\n",
    "                    for pattern in patterns:\n",
    "                        # Complete format: Type + Number + Year\n",
    "                        complete_pattern = (\n",
    "                            rf'{re.escape(pattern)}\\s*'\n",
    "                            r'(?:nomor|no\\.?|num\\.?|number)?\\s*'\n",
    "                            r'(\\d+)\\s*'\n",
    "                            r'(?:tahun|th\\.?|\\/)\\s*'\n",
    "                            r'(\\d{4})'\n",
    "                        )\n",
    "                        \n",
    "                        matches = re.finditer(complete_pattern, text_lower, re.IGNORECASE)\n",
    "                        for match in matches:\n",
    "                            number = match.group(1)\n",
    "                            year = match.group(2)\n",
    "                            \n",
    "                            references.append({\n",
    "                                'regulation': {\n",
    "                                    'type': pattern,\n",
    "                                    'number': number,\n",
    "                                    'year': year,\n",
    "                                    'full_text': match.group(0)\n",
    "                                },\n",
    "                                'confidence': 1.0,  # HIGHEST - has type, number, and year\n",
    "                                'specificity': 'complete'\n",
    "                            })\n",
    "                \n",
    "                # Pattern 2: Reference with number but no year (MEDIUM CONFIDENCE)\n",
    "                # e.g., \"UU No. 13\", \"PP 41\"\n",
    "                for reg_type, patterns in REGULATION_TYPE_PATTERNS.items():\n",
    "                    for pattern in patterns:\n",
    "                        partial_pattern = (\n",
    "                            rf'{re.escape(pattern)}\\s*'\n",
    "                            r'(?:nomor|no\\.?|num\\.?|number)?\\s*'\n",
    "                            r'(\\d+)(?!\\s*(?:tahun|th\\.?|\\/)\\s*\\d{{4}})'  # Negative lookahead for year\n",
    "                        )\n",
    "                        \n",
    "                        matches = re.finditer(partial_pattern, text_lower, re.IGNORECASE)\n",
    "                        for match in matches:\n",
    "                            number = match.group(1)\n",
    "                            \n",
    "                            # Check if not already captured with year\n",
    "                            already_exists = any(\n",
    "                                ref['regulation']['type'] == pattern and \n",
    "                                ref['regulation']['number'] == number and\n",
    "                                ref['confidence'] == 1.0\n",
    "                                for ref in references\n",
    "                            )\n",
    "                            \n",
    "                            if not already_exists:\n",
    "                                references.append({\n",
    "                                    'regulation': {\n",
    "                                        'type': pattern,\n",
    "                                        'number': number,\n",
    "                                        'year': '',\n",
    "                                        'full_text': match.group(0)\n",
    "                                    },\n",
    "                                    'confidence': 0.7,  # MEDIUM - has type and number\n",
    "                                    'specificity': 'partial'\n",
    "                                })\n",
    "                \n",
    "                # Pattern 3: Just regulation type mentioned (LOW CONFIDENCE)\n",
    "                # e.g., \"undang-undang tersebut\", \"peraturan pemerintah ini\"\n",
    "                if not references:  # Only if no specific references found\n",
    "                    for reg_type, patterns in REGULATION_TYPE_PATTERNS.items():\n",
    "                        for pattern in patterns:\n",
    "                            if pattern in text_lower:\n",
    "                                references.append({\n",
    "                                    'regulation': {\n",
    "                                        'type': pattern,\n",
    "                                        'number': '',\n",
    "                                        'year': '',\n",
    "                                        'full_text': pattern\n",
    "                                    },\n",
    "                                    'confidence': 0.3,  # LOW - only type mentioned\n",
    "                                    'specificity': 'vague'\n",
    "                                })\n",
    "                                break  # Only add once per type\n",
    "                \n",
    "                # Sort by confidence (highest first)\n",
    "                references.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "                \n",
    "                return references\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting regulation references: {e}\")\n",
    "                return []\n",
    "\n",
    "# =============================================================================\n",
    "# ADVANCED QUERY ANALYZER\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# ADVANCED QUERY ANALYZER\n",
    "# =============================================================================\n",
    "\n",
    "class AdvancedQueryAnalyzer:\n",
    "    \"\"\"\n",
    "    Intelligent query analyzer that differentiates between:\n",
    "    1. Specific legal phrases/entities (keyword-first search)\n",
    "    2. Broad conceptual queries (semantic-first search)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, knowledge_graph):\n",
    "        self.kg = knowledge_graph\n",
    "        \n",
    "        # Common law names mapping (expandable)\n",
    "        self.common_law_names = {\n",
    "            'kepabeanan': ['bea cukai', 'customs', 'pabean'],\n",
    "            'ketenagakerjaan': ['tenaga kerja', 'pekerja', 'buruh'],\n",
    "            'perpajakan': ['pajak', 'tax', 'pph', 'ppn'],\n",
    "            'cipta kerja': ['job creation', 'omnibus law cipta kerja'],\n",
    "            'perkawinan': ['nikah', 'marriage', 'kawin'],\n",
    "            'agraria': ['tanah', 'land', 'pertanahan'],\n",
    "            'keimigrasian': ['imigrasi', 'immigration', 'visa', 'paspor'],\n",
    "            'kewarganegaraan': ['citizenship', 'warga negara'],\n",
    "            'kehutanan': ['hutan', 'forest', 'forestry'],\n",
    "            'pertambangan': ['tambang', 'mining', 'minerba'],\n",
    "            'perikanan': ['ikan', 'fishery', 'nelayan'],\n",
    "            'kesehatan': ['health', 'rumah sakit', 'obat'],\n",
    "            'pendidikan': ['education', 'sekolah', 'universitas'],\n",
    "            'perbankan': ['bank', 'banking', 'kredit'],\n",
    "            'asuransi': ['insurance', 'pertanggungan'],\n",
    "            'lingkungan hidup': ['environment', 'pencemaran', 'limbah'],\n",
    "            'perlindungan konsumen': ['consumer protection', 'konsumen'],\n",
    "            'hak cipta': ['copyright', 'intellectual property', 'paten'],\n",
    "            'merek': ['trademark', 'brand'],\n",
    "            'kepailitan': ['bankruptcy', 'pailit', 'insolvensi'],\n",
    "            'arbitrase': ['arbitration', 'alternatif penyelesaian sengketa'],\n",
    "            'pidana': ['criminal', 'tindak pidana', 'kejahatan'],\n",
    "            'perdata': ['civil', 'gugatan', 'perkara perdata']\n",
    "        }\n",
    "        \n",
    "        # Multi-word legal phrases that should be treated as single entities\n",
    "        self.legal_phrases = {\n",
    "            'cipta kerja': {\n",
    "                'type': 'specific_concept',\n",
    "                'priority': 0.95,\n",
    "                'avoid_splitting': True,\n",
    "                'context': 'job creation law, not copyright'\n",
    "            },\n",
    "            'hak cipta': {\n",
    "                'type': 'specific_concept',\n",
    "                'priority': 0.95,\n",
    "                'avoid_splitting': True,\n",
    "                'context': 'copyright, intellectual property'\n",
    "            },\n",
    "            'tenaga kerja': {\n",
    "                'type': 'specific_concept',\n",
    "                'priority': 0.90,\n",
    "                'avoid_splitting': True,\n",
    "                'context': 'labor, workforce'\n",
    "            },\n",
    "            'bea cukai': {\n",
    "                'type': 'specific_concept',\n",
    "                'priority': 0.90,\n",
    "                'avoid_splitting': True,\n",
    "                'context': 'customs duties'\n",
    "            },\n",
    "            'tindak pidana': {\n",
    "                'type': 'specific_concept',\n",
    "                'priority': 0.90,\n",
    "                'avoid_splitting': True,\n",
    "                'context': 'criminal offense'\n",
    "            },\n",
    "            'penanaman modal': {\n",
    "                'type': 'specific_concept',\n",
    "                'priority': 0.90,\n",
    "                'avoid_splitting': True,\n",
    "                'context': 'investment'\n",
    "            },\n",
    "            'tanggung jawab': {\n",
    "                'type': 'specific_concept',\n",
    "                'priority': 0.85,\n",
    "                'avoid_splitting': True,\n",
    "                'context': 'liability, responsibility'\n",
    "            },\n",
    "            'lingkungan hidup': {\n",
    "                'type': 'specific_concept',\n",
    "                'priority': 0.90,\n",
    "                'avoid_splitting': True,\n",
    "                'context': 'environment'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Conceptual query indicators (triggers semantic-first search)\n",
    "        self.conceptual_indicators = [\n",
    "            'bagaimana', 'apa itu', 'mengapa', 'kenapa', 'jelaskan',\n",
    "            'apa saja', 'berapa', 'kapan', 'dimana', 'siapa',\n",
    "            'prosedur', 'cara', 'langkah', 'tata cara',\n",
    "            'definisi', 'pengertian', 'maksud', 'arti',\n",
    "            'sanksi apa', 'hukuman apa', 'denda berapa',\n",
    "            'syarat apa', 'ketentuan apa',\n",
    "            'perbedaan', 'perbandingan', 'hubungan antara'\n",
    "        ]\n",
    "        \n",
    "        # Specific query indicators (triggers keyword-first search)\n",
    "        self.specific_indicators = [\n",
    "            'tentang', 'mengenai', 'mengatur', 'diatur dalam',\n",
    "            'berdasarkan', 'sesuai', 'menurut',\n",
    "            'undang-undang', 'peraturan', 'pp', 'perpres', 'permen'\n",
    "        ]\n",
    "    \n",
    "    def analyze_query(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Main analysis function that determines search strategy.\n",
    "        \n",
    "        Returns:\n",
    "            {\n",
    "                'search_strategy': 'keyword_first' | 'semantic_first' | 'hybrid_balanced',\n",
    "                'key_phrases': [...],\n",
    "                'law_name_detected': bool,\n",
    "                'specific_entities': [...],\n",
    "                'confidence': float,\n",
    "                'reasoning': str,\n",
    "                'metadata_hints': {...}\n",
    "            }\n",
    "        \"\"\"\n",
    "        query_lower = query.lower().strip()\n",
    "        \n",
    "        analysis = {\n",
    "            'search_strategy': 'semantic_first',  # default\n",
    "            'key_phrases': [],\n",
    "            'law_name_detected': False,\n",
    "            'specific_entities': [],\n",
    "            'confidence': 0.5,\n",
    "            'reasoning': '',\n",
    "            'metadata_hints': {},\n",
    "            'keyword_boost': 0.0,\n",
    "            'semantic_boost': 0.0\n",
    "        }\n",
    "        \n",
    "        # Step 1: Check for exact regulation reference (already handled by hybrid search)\n",
    "        if self._has_exact_regulation_ref(query_lower):\n",
    "            analysis['search_strategy'] = 'metadata_first'\n",
    "            analysis['confidence'] = 0.95\n",
    "            analysis['reasoning'] = 'Exact regulation reference detected'\n",
    "            return analysis\n",
    "        \n",
    "        # Step 2: Check for specific legal phrases (HIGHEST PRIORITY)\n",
    "        detected_phrases = self._detect_legal_phrases(query_lower)\n",
    "        if detected_phrases:\n",
    "            analysis['key_phrases'] = detected_phrases\n",
    "            analysis['search_strategy'] = 'keyword_first'\n",
    "            analysis['confidence'] = max([p['priority'] for p in detected_phrases])\n",
    "            analysis['keyword_boost'] = 0.40  # Strong keyword emphasis\n",
    "            analysis['semantic_boost'] = 0.10\n",
    "            analysis['reasoning'] = f\"Specific legal phrase detected: {', '.join([p['phrase'] for p in detected_phrases])}\"\n",
    "            \n",
    "            # Add context to prevent semantic drift\n",
    "            for phrase_info in detected_phrases:\n",
    "                analysis['metadata_hints'][phrase_info['phrase']] = phrase_info['context']\n",
    "            \n",
    "            return analysis\n",
    "        \n",
    "        # Step 3: Check for common law names\n",
    "        detected_law_name = self._detect_common_law_name(query_lower)\n",
    "        if detected_law_name:\n",
    "            analysis['law_name_detected'] = True\n",
    "            analysis['specific_entities'].append(detected_law_name)\n",
    "            analysis['search_strategy'] = 'keyword_first'\n",
    "            analysis['confidence'] = 0.85\n",
    "            analysis['keyword_boost'] = 0.35\n",
    "            analysis['semantic_boost'] = 0.15\n",
    "            analysis['reasoning'] = f\"Common law name detected: {detected_law_name['name']}\"\n",
    "            analysis['metadata_hints']['about'] = detected_law_name['name']\n",
    "            return analysis\n",
    "        \n",
    "        # Step 4: Check query structure patterns\n",
    "        has_conceptual = any(ind in query_lower for ind in self.conceptual_indicators)\n",
    "        has_specific = any(ind in query_lower for ind in self.specific_indicators)\n",
    "        \n",
    "        if has_specific and not has_conceptual:\n",
    "            # \"tentang cipta kerja\" - specific but might not be a phrase\n",
    "            analysis['search_strategy'] = 'keyword_first'\n",
    "            analysis['confidence'] = 0.70\n",
    "            analysis['keyword_boost'] = 0.30\n",
    "            analysis['semantic_boost'] = 0.20\n",
    "            analysis['reasoning'] = 'Specific indicator detected without conceptual question'\n",
    "            \n",
    "            # Extract potential key terms after specific indicator\n",
    "            key_terms = self._extract_key_terms_after_indicator(query_lower)\n",
    "            if key_terms:\n",
    "                analysis['key_phrases'] = [{'phrase': term, 'priority': 0.70, 'context': 'extracted term'} for term in key_terms]\n",
    "            \n",
    "            return analysis\n",
    "        \n",
    "        if has_conceptual:\n",
    "            # \"bagaimana prosedur...\" - conceptual question\n",
    "            analysis['search_strategy'] = 'semantic_first'\n",
    "            analysis['confidence'] = 0.75\n",
    "            analysis['keyword_boost'] = 0.15\n",
    "            analysis['semantic_boost'] = 0.35\n",
    "            analysis['reasoning'] = 'Conceptual question detected'\n",
    "            return analysis\n",
    "        \n",
    "        # Step 5: Length-based heuristic\n",
    "        words = query_lower.split()\n",
    "        if len(words) <= 4 and not has_conceptual:\n",
    "            # Short queries are often specific\n",
    "            analysis['search_strategy'] = 'hybrid_balanced'\n",
    "            analysis['confidence'] = 0.60\n",
    "            analysis['keyword_boost'] = 0.25\n",
    "            analysis['semantic_boost'] = 0.25\n",
    "            analysis['reasoning'] = 'Short query - balanced approach'\n",
    "            \n",
    "            # Treat entire short query as potential key phrase\n",
    "            analysis['key_phrases'] = [{'phrase': query_lower, 'priority': 0.60, 'context': 'short query'}]\n",
    "            return analysis\n",
    "        \n",
    "        # Default: balanced hybrid\n",
    "        analysis['search_strategy'] = 'hybrid_balanced'\n",
    "        analysis['confidence'] = 0.50\n",
    "        analysis['keyword_boost'] = 0.20\n",
    "        analysis['semantic_boost'] = 0.30\n",
    "        analysis['reasoning'] = 'No clear indicators - using balanced hybrid search'\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _has_exact_regulation_ref(self, query: str) -> bool:\n",
    "        \"\"\"Check if query contains exact regulation reference (Type + Number + Year)\"\"\"\n",
    "        # Pattern: \"UU No. 13 Tahun 2003\" or similar\n",
    "        for reg_type, patterns in REGULATION_TYPE_PATTERNS.items():\n",
    "            for pattern in patterns:\n",
    "                if pattern in query:\n",
    "                    # Check if followed by number and year\n",
    "                    import re\n",
    "                    regex = rf'{re.escape(pattern)}\\s*(?:nomor|no\\.?)?\\s*(\\d+)\\s*(?:tahun|th\\.?)?\\s*(\\d{{4}})?'\n",
    "                    if re.search(regex, query, re.IGNORECASE):\n",
    "                        return True\n",
    "        return False\n",
    "    \n",
    "    def _detect_legal_phrases(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Detect specific multi-word legal phrases that should not be split\"\"\"\n",
    "        detected = []\n",
    "        \n",
    "        for phrase, info in self.legal_phrases.items():\n",
    "            if phrase in query:\n",
    "                detected.append({\n",
    "                    'phrase': phrase,\n",
    "                    'priority': info['priority'],\n",
    "                    'context': info['context'],\n",
    "                    'avoid_splitting': info['avoid_splitting']\n",
    "                })\n",
    "                \n",
    "                print(f\"DEBUG: Detected legal phrase: '{phrase}' - {info['context']}\")\n",
    "        \n",
    "        return detected\n",
    "    \n",
    "    def _detect_common_law_name(self, query: str) -> Optional[Dict]:\n",
    "        \"\"\"Detect if query contains a common law name (e.g., 'kepabeanan')\"\"\"\n",
    "        for law_name, aliases in self.common_law_names.items():\n",
    "            # Check main name\n",
    "            if law_name in query:\n",
    "                return {\n",
    "                    'name': law_name,\n",
    "                    'aliases': aliases,\n",
    "                    'confidence': 0.85\n",
    "                }\n",
    "            \n",
    "            # Check aliases\n",
    "            for alias in aliases:\n",
    "                if alias in query:\n",
    "                    return {\n",
    "                        'name': law_name,\n",
    "                        'aliases': aliases,\n",
    "                        'confidence': 0.75\n",
    "                    }\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _extract_key_terms_after_indicator(self, query: str) -> List[str]:\n",
    "        \"\"\"Extract key terms that appear after specific indicators\"\"\"\n",
    "        key_terms = []\n",
    "        \n",
    "        for indicator in self.specific_indicators:\n",
    "            if indicator in query:\n",
    "                # Extract text after indicator\n",
    "                parts = query.split(indicator, 1)\n",
    "                if len(parts) > 1:\n",
    "                    after_text = parts[1].strip()\n",
    "                    # Take first 2-4 words as potential key terms\n",
    "                    words = after_text.split()[:4]\n",
    "                    if words:\n",
    "                        key_term = ' '.join(words)\n",
    "                        # Remove common stop words from end\n",
    "                        while key_term.split()[-1] in INDONESIAN_STOPWORDS:\n",
    "                            words = key_term.split()[:-1]\n",
    "                            if not words:\n",
    "                                break\n",
    "                            key_term = ' '.join(words)\n",
    "                        \n",
    "                        if key_term:\n",
    "                            key_terms.append(key_term)\n",
    "        \n",
    "        return key_terms\n",
    "\n",
    "# =============================================================================\n",
    "# FIXED: IMPROVED RAG ACCURACY - SCORING AND FILTERING\n",
    "# =============================================================================\n",
    "\n",
    "class EnhancedKGSearchEngine:\n",
    "    def __init__(self, records, embeddings, embedding_model, embedding_tokenizer, knowledge_graph, dataset_loader):\n",
    "        self.records = records\n",
    "        self.embeddings = embeddings\n",
    "        self.embedding_model = embedding_model\n",
    "        self.embedding_tokenizer = embedding_tokenizer\n",
    "        self.kg = knowledge_graph\n",
    "        self.dataset_loader = dataset_loader\n",
    "        self.all_phase_results = {}\n",
    "        self.research_session_log = []\n",
    "        self._build_indexes()\n",
    "        \n",
    "        # *** NEW: Performance tracking dictionary ***\n",
    "        self.persona_performance = {\n",
    "            researcher_id: {\n",
    "                'query_types': {},  # Track performance per query type\n",
    "                'success_count': 0,\n",
    "                'total_queries': 0,\n",
    "                'accuracy_adjustment': 0.0,  # Learning parameter\n",
    "                'recent_performance': []  # Track last N queries for trend analysis\n",
    "            }\n",
    "            for researcher_id in RESEARCH_TEAM_PERSONAS.keys()\n",
    "        }\n",
    "        \n",
    "        self.context_manager = ConversationContextManager(knowledge_graph_instance=knowledge_graph)\n",
    "        self.community_detector = DynamicCommunityDetector(knowledge_graph)\n",
    "        self.query_analyzer = AdvancedQueryAnalyzer(knowledge_graph)\n",
    "\n",
    "    def _get_indices_for_filter(self, regulation_filter):\n",
    "        if not regulation_filter:\n",
    "            return None # Search all indices\n",
    "    \n",
    "        filtered_indices = []\n",
    "        # This loop is slow, you should pre-build an index for faster lookups,\n",
    "        # but for a proof-of-concept this demonstrates the logic.\n",
    "        for i, record in enumerate(self.records):\n",
    "            rec_type = str(record.get('regulation_type', '')).lower()\n",
    "            rec_number = str(record.get('regulation_number', ''))\n",
    "            rec_year = str(record.get('year', ''))\n",
    "    \n",
    "            filter_type = str(regulation_filter.get('regulation_type', '')).lower()\n",
    "            filter_number = str(regulation_filter.get('regulation_number', ''))\n",
    "            filter_year = str(regulation_filter.get('year', ''))\n",
    "    \n",
    "            # Use your existing matching logic\n",
    "            type_match = any(p in rec_type for p in REGULATION_TYPE_PATTERNS.get(filter_type, [filter_type]))\n",
    "            number_match = (filter_number == rec_number)\n",
    "            year_match = (not filter_year or filter_year == rec_year)\n",
    "    \n",
    "            if type_match and number_match and year_match:\n",
    "                filtered_indices.append(i)\n",
    "    \n",
    "        return filtered_indices if filtered_indices else None\n",
    "    \n",
    "    def _apply_regulation_filter(self, candidates, regulation_filter):\n",
    "        \"\"\"Filter candidates to specific regulation\"\"\"\n",
    "        if not regulation_filter:\n",
    "            return candidates\n",
    "        \n",
    "        filtered = []\n",
    "        for candidate in candidates:\n",
    "            try:\n",
    "                record = candidate['record']\n",
    "                \n",
    "                # Normalize for comparison\n",
    "                rec_type = str(record.get('regulation_type', '')).lower()\n",
    "                rec_number = str(record.get('regulation_number', ''))\n",
    "                rec_year = str(record.get('year', ''))\n",
    "                \n",
    "                filter_type = str(regulation_filter.get('regulation_type', '')).lower()\n",
    "                filter_number = str(regulation_filter.get('regulation_number', ''))\n",
    "                filter_year = str(regulation_filter.get('year', ''))\n",
    "                \n",
    "                # Match regulation type (flexible)\n",
    "                type_match = False\n",
    "                for patterns in REGULATION_TYPE_PATTERNS.values():\n",
    "                    if any(p in rec_type for p in patterns) and any(p in filter_type for p in patterns):\n",
    "                        type_match = True\n",
    "                        break\n",
    "                \n",
    "                # Match number (exact)\n",
    "                number_match = (filter_number in rec_number or rec_number in filter_number)\n",
    "                \n",
    "                # Match year (if provided)\n",
    "                year_match = (not filter_year or filter_year in rec_year)\n",
    "                \n",
    "                if type_match and number_match and year_match:\n",
    "                    filtered.append(candidate)\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    def _boost_contextual_documents(self, candidates):\n",
    "        \"\"\"Boost documents from conversation context\"\"\"\n",
    "        relevant_regs = self.context_manager.get_relevant_regulations(max_age=2)\n",
    "        \n",
    "        if not relevant_regs:\n",
    "            return candidates\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            try:\n",
    "                record = candidate['record']\n",
    "                rec_type = str(record.get('regulation_type', '')).lower()\n",
    "                rec_number = str(record.get('regulation_number', ''))\n",
    "                rec_year = str(record.get('year', ''))\n",
    "                \n",
    "                # Check if matches any regulation in context\n",
    "                for reg in relevant_regs:\n",
    "                    age = self.context_manager.exchange_count - reg.get('mentioned_in_exchange', 0)\n",
    "                    \n",
    "                    # Type match\n",
    "                    type_match = False\n",
    "                    for patterns in REGULATION_TYPE_PATTERNS.values():\n",
    "                        if any(p in rec_type for p in patterns) and any(p in reg['type'] for p in patterns):\n",
    "                            type_match = True\n",
    "                            break\n",
    "                    \n",
    "                    # Number and year match\n",
    "                    number_match = (reg['number'] in rec_number)\n",
    "                    year_match = (not reg['year'] or reg['year'] in rec_year)\n",
    "                    \n",
    "                    if type_match and number_match and year_match:\n",
    "                        # Apply conversation boost (decay with age)\n",
    "                        boost = 0.15 * max(0, (3 - age) / 3)\n",
    "                        candidate['composite_score'] = min(1.0, candidate.get('composite_score', 0) + boost)\n",
    "                        candidate['contextual_boost'] = boost\n",
    "                        break\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        return candidates\n",
    "\n",
    "    def update_persona_performance(self, query_type, successful_researchers, all_researchers):\n",
    "        \"\"\"\n",
    "        Update persona performance metrics based on query results.\n",
    "        Called after consensus building to learn from success/failure.\n",
    "        \n",
    "        Args:\n",
    "            query_type: Type of query (e.g., 'specific_article', 'procedural')\n",
    "            successful_researchers: List of researcher IDs who contributed to top results\n",
    "            all_researchers: List of all researcher IDs on the team\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for researcher_id in all_researchers:\n",
    "                if researcher_id not in self.persona_performance:\n",
    "                    continue\n",
    "                \n",
    "                perf = self.persona_performance[researcher_id]\n",
    "                perf['total_queries'] += 1\n",
    "                \n",
    "                # Track query type specific performance\n",
    "                if query_type not in perf['query_types']:\n",
    "                    perf['query_types'][query_type] = {'success': 0, 'total': 0}\n",
    "                \n",
    "                perf['query_types'][query_type]['total'] += 1\n",
    "                \n",
    "                # Record success if this researcher contributed\n",
    "                is_successful = researcher_id in successful_researchers\n",
    "                perf['recent_performance'].append(1 if is_successful else 0)\n",
    "                \n",
    "                # Keep only last 10 queries\n",
    "                if len(perf['recent_performance']) > 10:\n",
    "                    perf['recent_performance'].pop(0)\n",
    "                \n",
    "                if is_successful:\n",
    "                    perf['success_count'] += 1\n",
    "                    perf['query_types'][query_type]['success'] += 1\n",
    "                \n",
    "                # Calculate success rates\n",
    "                overall_success_rate = perf['success_count'] / perf['total_queries']\n",
    "                query_type_stats = perf['query_types'][query_type]\n",
    "                query_type_success_rate = query_type_stats['success'] / query_type_stats['total']\n",
    "                \n",
    "                # Recent trend\n",
    "                recent_trend = sum(perf['recent_performance']) / len(perf['recent_performance']) if perf['recent_performance'] else 0.5\n",
    "                \n",
    "                # ADAPTIVE LEARNING: Gradual adjustment\n",
    "                if overall_success_rate > 0.7:\n",
    "                    # High performers get gradual boost (max +0.10)\n",
    "                    adjustment_delta = 0.005 * (1 + recent_trend)\n",
    "                    perf['accuracy_adjustment'] = min(0.10, perf['accuracy_adjustment'] + adjustment_delta)\n",
    "                \n",
    "                elif overall_success_rate < 0.4:\n",
    "                    # Low performers get slight penalty (max -0.05)\n",
    "                    adjustment_delta = 0.003 * (1 - recent_trend)\n",
    "                    perf['accuracy_adjustment'] = max(-0.05, perf['accuracy_adjustment'] - adjustment_delta)\n",
    "                \n",
    "                else:\n",
    "                    # Middle performers: slow decay toward baseline\n",
    "                    perf['accuracy_adjustment'] *= 0.98\n",
    "                \n",
    "                # Query-type specialization bonus\n",
    "                if query_type_stats['total'] >= 3 and query_type_success_rate > 0.75:\n",
    "                    if 'specialization_bonus' not in perf:\n",
    "                        perf['specialization_bonus'] = {}\n",
    "                    perf['specialization_bonus'][query_type] = min(0.08, query_type_success_rate * 0.1)\n",
    "            \n",
    "            # Log performance update\n",
    "            if hasattr(self, 'research_session_log'):\n",
    "                self.research_session_log.append({\n",
    "                    'query_type': query_type,\n",
    "                    'successful_researchers': successful_researchers,\n",
    "                    'performance_snapshot': {\n",
    "                        rid: {\n",
    "                            'success_rate': perf['success_count'] / perf['total_queries'],\n",
    "                            'accuracy_adjustment': perf['accuracy_adjustment']\n",
    "                        }\n",
    "                        for rid, perf in self.persona_performance.items()\n",
    "                        if perf['total_queries'] > 0\n",
    "                    }\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error updating persona performance: {e}\")\n",
    "    \n",
    "    def get_adjusted_persona(self, researcher_id, query_type):\n",
    "        \"\"\"\n",
    "        Get persona with performance-based adjustments applied.\n",
    "        Creates a 'smarter' version of the base persona.\n",
    "        \n",
    "        Args:\n",
    "            researcher_id: ID of the researcher\n",
    "            query_type: Current query type\n",
    "            \n",
    "        Returns:\n",
    "            Adjusted persona dictionary with learned bonuses applied\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get base persona (never modify the original!)\n",
    "            base_persona = RESEARCH_TEAM_PERSONAS[researcher_id].copy()\n",
    "            \n",
    "            if researcher_id not in self.persona_performance:\n",
    "                return base_persona\n",
    "            \n",
    "            perf = self.persona_performance[researcher_id]\n",
    "            \n",
    "            # Apply learned accuracy adjustment\n",
    "            base_persona['accuracy_bonus'] += perf['accuracy_adjustment']\n",
    "            \n",
    "            # Apply query-type specialization bonus if exists\n",
    "            if 'specialization_bonus' in perf and query_type in perf['specialization_bonus']:\n",
    "                base_persona['accuracy_bonus'] += perf['specialization_bonus'][query_type]\n",
    "            \n",
    "            # Adjust search style based on query type success\n",
    "            if query_type in perf['query_types']:\n",
    "                query_stats = perf['query_types'][query_type]\n",
    "                if query_stats['total'] >= 3:\n",
    "                    success_rate = query_stats['success'] / query_stats['total']\n",
    "                    \n",
    "                    if success_rate > 0.7:\n",
    "                        # Boost confidence\n",
    "                        search_style_copy = base_persona['search_style'].copy()\n",
    "                        for key in search_style_copy:\n",
    "                            search_style_copy[key] *= 1.05\n",
    "                        base_persona['search_style'] = search_style_copy\n",
    "                    \n",
    "                    elif success_rate < 0.4:\n",
    "                        # Be more conservative\n",
    "                        search_style_copy = base_persona['search_style'].copy()\n",
    "                        for key in search_style_copy:\n",
    "                            search_style_copy[key] *= 0.95\n",
    "                        base_persona['search_style'] = search_style_copy\n",
    "            \n",
    "            # Add metadata\n",
    "            base_persona['_adjusted'] = True\n",
    "            base_persona['_original_accuracy_bonus'] = RESEARCH_TEAM_PERSONAS[researcher_id]['accuracy_bonus']\n",
    "            base_persona['_learned_adjustment'] = perf['accuracy_adjustment']\n",
    "            base_persona['_total_queries'] = perf['total_queries']\n",
    "            base_persona['_success_rate'] = perf['success_count'] / perf['total_queries'] if perf['total_queries'] > 0 else 0\n",
    "            \n",
    "            return base_persona\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting adjusted persona: {e}\")\n",
    "            return RESEARCH_TEAM_PERSONAS[researcher_id]\n",
    "\n",
    "    # Add these methods to EnhancedKGSearchEngine class\n",
    "\n",
    "    def _build_indexes(self):\n",
    "        \"\"\"Build search indexes - MISSING METHOD\"\"\"\n",
    "        try:\n",
    "            print(\"Building search indexes...\")\n",
    "            # Already built in dataset_loader, just reference them\n",
    "            self.authority_index = self.dataset_loader.authority_index\n",
    "            self.temporal_index = self.dataset_loader.temporal_index\n",
    "            self.kg_connectivity_index = self.dataset_loader.kg_connectivity_index\n",
    "            self.hierarchy_index = self.dataset_loader.hierarchy_index\n",
    "            self.domain_index = self.dataset_loader.domain_index\n",
    "            print(f\"‚úÖ Indexes built: {len(self.authority_index)} authority tiers\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error building indexes: {e}\")\n",
    "            self.authority_index = {}\n",
    "            self.temporal_index = {}\n",
    "            self.kg_connectivity_index = {}\n",
    "            self.hierarchy_index = {}\n",
    "            self.domain_index = {}\n",
    "    \n",
    "    def _detect_query_type(self, query):\n",
    "        \"\"\"Detect query type - MISSING METHOD\"\"\"\n",
    "        try:\n",
    "            query_lower = query.lower()\n",
    "            \n",
    "            # Check each pattern\n",
    "            for query_type, pattern_info in QUERY_PATTERNS.items():\n",
    "                if query_type == 'general':\n",
    "                    continue\n",
    "                indicators = pattern_info['indicators']\n",
    "                if any(indicator in query_lower for indicator in indicators):\n",
    "                    return query_type\n",
    "            \n",
    "            return 'general'\n",
    "        except Exception as e:\n",
    "            print(f\"Error detecting query type: {e}\")\n",
    "            return 'general'\n",
    "    \n",
    "    def _embed_query_with_context_and_kg(self, query, query_type):\n",
    "        \"\"\"Embed query with context - MODIFIED FOR CPU\"\"\"\n",
    "        try:\n",
    "            # Add query type context\n",
    "            context_map = {\n",
    "                'specific_article': 'pasal dan ayat spesifik',\n",
    "                'procedural': 'prosedur dan tata cara',\n",
    "                'definitional': 'definisi dan pengertian',\n",
    "                'sanctions': 'sanksi dan hukuman',\n",
    "                'general': 'informasi hukum'\n",
    "            }\n",
    "            \n",
    "            context = context_map.get(query_type, 'informasi hukum')\n",
    "            enhanced_query = f\"Mencari {context}: {query}\"\n",
    "            \n",
    "            # Embed on CPU\n",
    "            with torch.no_grad():\n",
    "                inputs = self.embedding_tokenizer(\n",
    "                    [enhanced_query], \n",
    "                    padding=True, \n",
    "                    truncation=True,\n",
    "                    max_length=MAX_LENGTH, \n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                inputs = {k: v.to('cpu') for k, v in inputs.items()}  # Force CPU\n",
    "                outputs = self.embedding_model(**inputs)\n",
    "                \n",
    "                attention_mask = inputs['attention_mask']\n",
    "                last_hidden_states = outputs.last_hidden_state\n",
    "                sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "                batch_size = last_hidden_states.shape[0]\n",
    "                \n",
    "                embedding = last_hidden_states[\n",
    "                    torch.arange(batch_size, device='cpu'), \n",
    "                    sequence_lengths\n",
    "                ]\n",
    "                \n",
    "                return embedding[0]\n",
    "        except Exception as e:\n",
    "            print(f\"Error embedding query: {e}\")\n",
    "            return torch.zeros(self.embeddings.shape[1])\n",
    "    \n",
    "    def _apply_enhanced_domain_bonus(self, record, query_type, kg_score):\n",
    "        \"\"\"Apply domain-specific bonuses - MISSING METHOD\"\"\"\n",
    "        try:\n",
    "            bonus = 0.0\n",
    "            \n",
    "            # Domain matching bonus\n",
    "            domain = record.get('kg_primary_domain', '').lower()\n",
    "            domain_confidence = record.get('kg_domain_confidence', 0)\n",
    "            \n",
    "            if domain_confidence > 0.7:\n",
    "                bonus += 0.05\n",
    "            \n",
    "            # Query-type specific bonuses\n",
    "            if query_type == 'sanctions':\n",
    "                if 'criminal' in domain or 'administrative' in domain:\n",
    "                    bonus += 0.08\n",
    "                if record.get('kg_has_prohibitions', False):\n",
    "                    bonus += 0.10\n",
    "            elif query_type == 'procedural':\n",
    "                if 'administrative' in domain or 'procedural' in domain:\n",
    "                    bonus += 0.08\n",
    "                if record.get('kg_has_obligations', False):\n",
    "                    bonus += 0.10\n",
    "            elif query_type == 'specific_article':\n",
    "                if record.get('article', 'N/A') != 'N/A':\n",
    "                    bonus += 0.12\n",
    "            \n",
    "            # KG enhancement bonus\n",
    "            if kg_score > 0.5:\n",
    "                bonus += 0.06\n",
    "            \n",
    "            return min(0.20, bonus)  # Cap at 0.20\n",
    "        except Exception as e:\n",
    "            return 0.0\n",
    "    \n",
    "    def _apply_researcher_expertise_bonus(self, record, persona, query_type):\n",
    "        \"\"\"Apply researcher expertise bonus - MISSING METHOD\"\"\"\n",
    "        try:\n",
    "            bonus = 0.0\n",
    "            \n",
    "            # Check if query type matches researcher specialty\n",
    "            specialties = persona.get('specialties', [])\n",
    "            \n",
    "            if query_type == 'specific_article' and 'precedent_analysis' in specialties:\n",
    "                bonus += 0.05\n",
    "            elif query_type == 'procedural' and 'procedural_law' in specialties:\n",
    "                bonus += 0.06\n",
    "            elif query_type == 'definitional' and 'constitutional_law' in specialties:\n",
    "                bonus += 0.04\n",
    "            elif 'knowledge_graphs' in specialties:\n",
    "                if record.get('kg_entity_count', 0) > 5:\n",
    "                    bonus += 0.05\n",
    "            \n",
    "            # Authority preference bonus\n",
    "            if persona.get('bias_towards') == 'established_precedents':\n",
    "                if record.get('kg_authority_score', 0) > 0.8:\n",
    "                    bonus += 0.04\n",
    "            \n",
    "            return min(0.10, bonus)  # Cap at 0.10\n",
    "        except Exception as e:\n",
    "            return 0.0\n",
    "    \n",
    "    # ========================\n",
    "    # FIX 2: Improved Individual Research Method\n",
    "    # ========================\n",
    "    # Replace _conduct_individual_research in EnhancedKGSearchEngine\n",
    "    \n",
    "    def _conduct_individual_research(\n",
    "        self, researcher_id, persona, query, query_type,\n",
    "        query_embedding, query_entities, config, regulation_filter=None,\n",
    "        query_analysis=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        FIXED: Strict metadata-first search with graceful fallback to semantic search.\n",
    "        No early returns - always contributes candidates to consensus.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get adapted persona\n",
    "            if hasattr(self, 'get_adjusted_persona'):\n",
    "                persona = self.get_adjusted_persona(researcher_id, query_type)\n",
    "            \n",
    "            # ========================================================================\n",
    "            # CRITICAL FIX: STRICT METADATA EXTRACTION & FILTERING\n",
    "            # ========================================================================\n",
    "            exact_regulation_ref = None\n",
    "            exact_match_mode = False\n",
    "            metadata_candidates = []  # Initialize here to preserve across paths\n",
    "            \n",
    "            if self.kg:\n",
    "                regulation_refs = self.kg.extract_regulation_references_with_confidence(query)\n",
    "                \n",
    "                if regulation_refs:\n",
    "                    best_ref = regulation_refs[0]\n",
    "                    \n",
    "                    # **STRICT REQUIREMENT**: Confidence >= 0.9 AND all three components exist\n",
    "                    if (best_ref['confidence'] >= 0.9 and \n",
    "                        best_ref['specificity'] == 'complete' and\n",
    "                        best_ref['regulation'].get('type') and\n",
    "                        best_ref['regulation'].get('number') and\n",
    "                        best_ref['regulation'].get('year')):\n",
    "                        \n",
    "                        exact_regulation_ref = best_ref['regulation']\n",
    "                        exact_match_mode = True\n",
    "                        \n",
    "                        print(f\"üéØ STRICT METADATA MODE ACTIVATED: {exact_regulation_ref['type']} \"\n",
    "                              f\"{exact_regulation_ref['number']}/{exact_regulation_ref['year']} \"\n",
    "                              f\"(confidence: {best_ref['confidence']:.2%})\")\n",
    "            \n",
    "            # ========================================================================\n",
    "            # PATH 1: STRICT METADATA-ONLY SEARCH (PERFECT SCORE OVERRIDE)\n",
    "            # ========================================================================\n",
    "            \n",
    "            if exact_match_mode and exact_regulation_ref:\n",
    "                print(f\"   üîí Executing STRICT metadata filter with PERFECT SCORE override...\")\n",
    "                \n",
    "                ref_type = exact_regulation_ref['type'].lower()\n",
    "                ref_number = exact_regulation_ref['number']\n",
    "                ref_year = exact_regulation_ref['year']\n",
    "                \n",
    "                # **CRITICAL**: Direct exact-match filtering at record level\n",
    "                for i, record in enumerate(self.records):\n",
    "                    try:\n",
    "                        rec_type = str(record.get('regulation_type', '')).lower()\n",
    "                        rec_number = str(record.get('regulation_number', ''))\n",
    "                        rec_year = str(record.get('year', ''))\n",
    "                        \n",
    "                        # **TYPE MATCH** (flexible pattern matching)\n",
    "                        type_match = False\n",
    "                        for patterns in REGULATION_TYPE_PATTERNS.values():\n",
    "                            if any(p in rec_type for p in patterns) and any(p in ref_type for p in patterns):\n",
    "                                type_match = True\n",
    "                                break\n",
    "                        \n",
    "                        if not type_match:\n",
    "                            continue\n",
    "                        \n",
    "                        # **NUMBER MATCH** (EXACT - case sensitive for numbers)\n",
    "                        if ref_number != rec_number:\n",
    "                            continue\n",
    "                        \n",
    "                        # **YEAR MATCH** (EXACT - THIS IS THE CRITICAL FIX)\n",
    "                        if ref_year != rec_year:\n",
    "                            continue  # ‚Üê THIS LINE ENSURES YEAR FILTERING\n",
    "                        \n",
    "                        # ‚úÖ TRIPLE MATCH FOUND - APPLY PERFECT SCORE OVERRIDE\n",
    "                        print(f\"   ‚úÖ EXACT MATCH FOUND: {rec_type} {rec_number}/{rec_year}\")\n",
    "                        \n",
    "                        # **PERFECT SCORE OVERRIDE**: Set to 1.0 to guarantee top ranking\n",
    "                        candidate = {\n",
    "                            'record': record,\n",
    "                            'composite_score': 1.0,  # ‚Üê PERFECT SCORE OVERRIDE\n",
    "                            'semantic_score': 0.0,\n",
    "                            'keyword_score': 0.0,\n",
    "                            'kg_score': float(record.get('kg_connectivity_score', 0.5)),\n",
    "                            'metadata_match': True,\n",
    "                            'exact_metadata_mode': True,\n",
    "                            'match_type': 'STRICT_TRIPLE_MATCH',\n",
    "                            'researcher_bias_applied': persona['name'],\n",
    "                            'perfect_score_override': True  # Flag for transparency\n",
    "                        }\n",
    "                        \n",
    "                        metadata_candidates.append(candidate)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"   Error processing record {i}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                print(f\"   üìä Strict filter result: {len(metadata_candidates)} exact matches\")\n",
    "                \n",
    "                # *** FIXED: NO EARLY RETURN - Fallback to semantic if no matches ***\n",
    "                if not metadata_candidates:\n",
    "                    print(f\"   ‚ö†Ô∏è NO EXACT MATCH FOUND for {ref_type} {ref_number}/{ref_year}\")\n",
    "                    print(f\"   üîÑ FALLING BACK to semantic search for this researcher...\")\n",
    "                    exact_match_mode = False  # Disable to trigger PATH 2\n",
    "            \n",
    "            # ========================================================================\n",
    "            # PATH 2: NORMAL SEMANTIC SEARCH (Original logic + metadata candidates)\n",
    "            # ========================================================================\n",
    "            \n",
    "            # Only execute semantic search if:\n",
    "            # 1. exact_match_mode was never triggered, OR\n",
    "            # 2. exact_match_mode failed to find candidates (fallback)\n",
    "            if not exact_match_mode or not metadata_candidates:\n",
    "                print(f\"   üîç Executing normal semantic search...\")\n",
    "                \n",
    "                # Adjust search weights based on query analysis\n",
    "                if query_analysis:\n",
    "                    search_style = persona['search_style'].copy()\n",
    "                    \n",
    "                    if query_analysis['search_strategy'] == 'keyword_first':\n",
    "                        keyword_boost = query_analysis.get('keyword_boost', 0.30)\n",
    "                        semantic_boost = query_analysis.get('semantic_boost', 0.15)\n",
    "                        \n",
    "                        search_style['semantic_weight'] = semantic_boost\n",
    "                        search_style['kg_weight'] = search_style.get('kg_weight', 0.25) + 0.10\n",
    "                        search_style['authority_weight'] = 1.0 - (semantic_boost + search_style['kg_weight'] + keyword_boost + search_style.get('temporal_weight', 0.15))\n",
    "                        \n",
    "                    elif query_analysis['search_strategy'] == 'semantic_first':\n",
    "                        semantic_boost = query_analysis.get('semantic_boost', 0.35)\n",
    "                        keyword_boost = query_analysis.get('keyword_boost', 0.15)\n",
    "                        \n",
    "                        search_style['semantic_weight'] = semantic_boost\n",
    "                        search_style['authority_weight'] = max(0.10, search_style.get('authority_weight', 0.25) - 0.05)\n",
    "                    \n",
    "                    else:  # hybrid_balanced\n",
    "                        keyword_boost = query_analysis.get('keyword_boost', 0.25)\n",
    "                        semantic_boost = query_analysis.get('semantic_boost', 0.25)\n",
    "                        search_style['semantic_weight'] = semantic_boost\n",
    "                else:\n",
    "                    search_style = persona['search_style']\n",
    "                \n",
    "                # Calculate semantic similarities\n",
    "                semantic_sims = F.cosine_similarity(\n",
    "                    query_embedding.unsqueeze(0),\n",
    "                    self.embeddings,\n",
    "                    dim=1\n",
    "                )\n",
    "                semantic_sims = semantic_sims.cpu().numpy()\n",
    "                \n",
    "                phase_preference = persona['phases_preference']\n",
    "                speed_mult = persona['speed_multiplier']\n",
    "                \n",
    "                search_phases = config.get('search_phases', DEFAULT_SEARCH_PHASES)\n",
    "                \n",
    "                phase_results = {}\n",
    "                all_candidates = []\n",
    "                \n",
    "                # Execute search phases\n",
    "                for phase_name in phase_preference:\n",
    "                    if phase_name not in search_phases:\n",
    "                        continue\n",
    "                    \n",
    "                    phase_config = search_phases[phase_name]\n",
    "                    if not phase_config.get('enabled', True):\n",
    "                        continue\n",
    "                    \n",
    "                    adjusted_config = phase_config.copy()\n",
    "                    adjusted_config['candidates'] = int(phase_config['candidates'] * speed_mult)\n",
    "                    \n",
    "                    # Calculate keyword similarities\n",
    "                    keyword_sims = None\n",
    "                    if self.dataset_loader.tfidf_matrix is not None:\n",
    "                        try:\n",
    "                            query_tfidf = self.dataset_loader.tfidf_vectorizer.transform([query])\n",
    "                            keyword_sims = cosine_similarity(\n",
    "                                query_tfidf,\n",
    "                                self.dataset_loader.tfidf_matrix\n",
    "                            )[0]\n",
    "                        except Exception:\n",
    "                            keyword_sims = None\n",
    "                    \n",
    "                    # Score candidates (normal semantic scoring)\n",
    "                    phase_candidates = self._score_candidates_with_enhanced_kg(\n",
    "                        semantic_sims, keyword_sims, query_entities, query_type,\n",
    "                        search_style, adjusted_config, persona,\n",
    "                        regulation_filter=regulation_filter,\n",
    "                        query_analysis=query_analysis\n",
    "                    )\n",
    "                    \n",
    "                    phase_key = f\"{researcher_id}_{phase_name}\"\n",
    "                    phase_results[phase_key] = {\n",
    "                        'phase': phase_name,\n",
    "                        'researcher': researcher_id,\n",
    "                        'researcher_name': persona['name'],\n",
    "                        'candidates': phase_candidates,\n",
    "                        'confidence': self._calculate_phase_confidence(phase_candidates, phase_config),\n",
    "                        'persona_adjusted': persona.get('_adjusted', False),\n",
    "                        'learned_bonus': persona.get('_learned_adjustment', 0.0),\n",
    "                        'query_strategy': query_analysis.get('search_strategy') if query_analysis else 'default',\n",
    "                        'exact_match_mode': False\n",
    "                    }\n",
    "                    \n",
    "                    all_candidates.extend(phase_candidates)\n",
    "            \n",
    "            # ========================================================================\n",
    "            # UNIFIED RETURN: Metadata candidates (if any) + Semantic candidates\n",
    "            # ========================================================================\n",
    "            \n",
    "            # *** FIXED: Combine metadata candidates with semantic candidates ***\n",
    "            combined_candidates = metadata_candidates + all_candidates\n",
    "            \n",
    "            # If we had exact matches, create a separate phase for them\n",
    "            if metadata_candidates:\n",
    "                phase_results[f\"{researcher_id}_metadata_strict\"] = {\n",
    "                    'phase': 'metadata_strict',\n",
    "                    'researcher': researcher_id,\n",
    "                    'researcher_name': persona['name'],\n",
    "                    'candidates': metadata_candidates,\n",
    "                    'confidence': 1.0,  # Perfect confidence\n",
    "                    'exact_metadata_mode': True,\n",
    "                    'query_strategy': 'strict_metadata_filter_perfect_score'\n",
    "                }\n",
    "            \n",
    "            return {\n",
    "                'researcher_id': researcher_id,\n",
    "                'phase_results': phase_results,\n",
    "                'all_candidates': combined_candidates,\n",
    "                'exact_match_mode': bool(metadata_candidates),\n",
    "                'query_type': query_type,\n",
    "                'persona_metadata': {\n",
    "                    'adjusted': persona.get('_adjusted', False),\n",
    "                    'success_rate': persona.get('_success_rate', 0),\n",
    "                    'learned_adjustment': persona.get('_learned_adjustment', 0)\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in individual research for {researcher_id}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return {\n",
    "                'researcher_id': researcher_id,\n",
    "                'phase_results': {},\n",
    "                'all_candidates': [],\n",
    "                'error': str(e)\n",
    "            }\n",
    "            \n",
    "    def _calculate_phase_confidence(self, candidates, phase_config):\n",
    "        \"\"\"Calculate confidence for phase results\"\"\"\n",
    "        try:\n",
    "            if not candidates:\n",
    "                return 0.0\n",
    "            \n",
    "            # Average score above threshold\n",
    "            avg_score = np.mean([c.get('composite_score', 0) for c in candidates])\n",
    "            \n",
    "            # Normalize by thresholds\n",
    "            sem_threshold = phase_config.get('semantic_threshold', 0.3)\n",
    "            confidence = min(1.0, avg_score / (sem_threshold + 0.1))\n",
    "            \n",
    "            return confidence\n",
    "        except Exception:\n",
    "            return 0.5\n",
    "    \n",
    "    # ========================\n",
    "    # FIX 3: Add Missing Cross-Validation and Devil's Advocate Methods\n",
    "    # ========================\n",
    "    \n",
    "    def _conduct_cross_validation(self, individual_results, team_members, query_entities):\n",
    "        \"\"\"Cross-validate findings between researchers\"\"\"\n",
    "        try:\n",
    "            consensus_docs = {}\n",
    "            \n",
    "            # Collect all document IDs and their appearances\n",
    "            doc_appearances = defaultdict(list)\n",
    "            \n",
    "            for researcher_id, results in individual_results.items():\n",
    "                for phase_key, phase_data in results.get('phase_results', {}).items():\n",
    "                    for candidate in phase_data.get('candidates', []):\n",
    "                        doc_id = candidate['record']['global_id']\n",
    "                        doc_appearances[doc_id].append({\n",
    "                            'researcher': researcher_id,\n",
    "                            'score': candidate.get('composite_score', 0),\n",
    "                            'candidate': candidate\n",
    "                        })\n",
    "            \n",
    "            # Find documents with agreement from multiple researchers\n",
    "            for doc_id, appearances in doc_appearances.items():\n",
    "                if len(appearances) >= 2:  # At least 2 researchers found it\n",
    "                    avg_score = np.mean([a['score'] for a in appearances])\n",
    "                    researchers = [a['researcher'] for a in appearances]\n",
    "                    \n",
    "                    consensus_docs[doc_id] = {\n",
    "                        'agreement_count': len(appearances),\n",
    "                        'researchers': researchers,\n",
    "                        'avg_score': avg_score,\n",
    "                        'candidate': appearances[0]['candidate']  # Use first instance\n",
    "                    }\n",
    "            \n",
    "            return consensus_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in cross-validation: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _conduct_devils_advocate_review(self, individual_results, query, query_type):\n",
    "        \"\"\"Devil's advocate critical review\"\"\"\n",
    "        try:\n",
    "            challenges = {}\n",
    "            \n",
    "            # Collect all high-scoring candidates\n",
    "            all_candidates = []\n",
    "            for researcher_id, results in individual_results.items():\n",
    "                for phase_key, phase_data in results.get('phase_results', {}).items():\n",
    "                    all_candidates.extend(phase_data.get('candidates', []))\n",
    "            \n",
    "            # Sort by score\n",
    "            all_candidates.sort(key=lambda x: x.get('composite_score', 0), reverse=True)\n",
    "            \n",
    "            # Challenge top candidates\n",
    "            for candidate in all_candidates[:10]:  # Review top 10\n",
    "                doc_id = candidate['record']['global_id']\n",
    "                record = candidate['record']\n",
    "                \n",
    "                challenge_points = []\n",
    "                \n",
    "                # Check for potential issues\n",
    "                if candidate.get('kg_score', 0) < 0.3:\n",
    "                    challenge_points.append(\"Low KG relevance\")\n",
    "                \n",
    "                if candidate.get('semantic_score', 0) < 0.4:\n",
    "                    challenge_points.append(\"Low semantic match\")\n",
    "                \n",
    "                if record.get('kg_authority_score', 0) < 0.5:\n",
    "                    challenge_points.append(\"Lower authority source\")\n",
    "                \n",
    "                if record.get('kg_temporal_score', 0) < 0.4:\n",
    "                    challenge_points.append(\"Potentially outdated\")\n",
    "                \n",
    "                if query_type == 'specific_article' and record.get('article', 'N/A') == 'N/A':\n",
    "                    challenge_points.append(\"No specific article reference\")\n",
    "                \n",
    "                if challenge_points:\n",
    "                    challenges[doc_id] = {\n",
    "                        'candidate': candidate,\n",
    "                        'challenge_points': challenge_points,\n",
    "                        'severity': len(challenge_points) / 5.0  # Normalize to 0-1\n",
    "                    }\n",
    "            \n",
    "            return challenges\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in devil's advocate review: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _apply_devils_advocate_filters(self, individual_results, challenges):\n",
    "        \"\"\"Apply devil's advocate filters to results\"\"\"\n",
    "        try:\n",
    "            if not challenges:\n",
    "                return individual_results\n",
    "            \n",
    "            # Apply penalties to challenged documents\n",
    "            for researcher_id, results in individual_results.items():\n",
    "                for phase_key, phase_data in results.get('phase_results', {}).items():\n",
    "                    filtered_candidates = []\n",
    "                    \n",
    "                    for candidate in phase_data.get('candidates', []):\n",
    "                        doc_id = candidate['record']['global_id']\n",
    "                        \n",
    "                        if doc_id in challenges:\n",
    "                            challenge_info = challenges[doc_id]\n",
    "                            severity = challenge_info['severity']\n",
    "                            \n",
    "                            # Apply penalty\n",
    "                            penalty = severity * 0.15  # Up to 15% penalty\n",
    "                            candidate['composite_score'] *= (1 - penalty)\n",
    "                            candidate['devils_advocate_challenged'] = True\n",
    "                            candidate['challenge_points'] = challenge_info['challenge_points']\n",
    "                            \n",
    "                            # Only keep if still above threshold\n",
    "                            if candidate['composite_score'] >= 0.4:\n",
    "                                filtered_candidates.append(candidate)\n",
    "                        else:\n",
    "                            filtered_candidates.append(candidate)\n",
    "                    \n",
    "                    phase_data['candidates'] = filtered_candidates\n",
    "            \n",
    "            return individual_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error applying devil's advocate filters: {e}\")\n",
    "            return individual_results\n",
    "    \n",
    "    def _score_candidates_with_enhanced_kg(self, semantic_sims, keyword_sims,\n",
    "                                 query_entities, query_type, search_style,\n",
    "                                 phase_config, persona, regulation_filter=None,\n",
    "                                 query_analysis=None):  # NEW PARAMETER\n",
    "        \"\"\"\n",
    "        MODIFIED: Scoring with query-analysis-driven adjustments\n",
    "        \"\"\"\n",
    "        try:\n",
    "            candidates = []\n",
    "            \n",
    "            if keyword_sims is None:\n",
    "                keyword_sims = np.zeros_like(semantic_sims)\n",
    "            \n",
    "            query_entity_set = set(str(e).lower() for e in query_entities) if query_entities else set()\n",
    "            \n",
    "            # *** NEW: Extract key phrases for exact matching ***\n",
    "            key_phrases_set = set()\n",
    "            if query_analysis and query_analysis.get('key_phrases'):\n",
    "                key_phrases_set = {p['phrase'].lower() for p in query_analysis['key_phrases']}\n",
    "            \n",
    "            for i, (sem_score, key_score) in enumerate(zip(\n",
    "                semantic_sims.tolist() if torch.is_tensor(semantic_sims) else semantic_sims, \n",
    "                keyword_sims\n",
    "            )):\n",
    "                try:\n",
    "                    if sem_score < phase_config['semantic_threshold'] * 0.8 and key_score < phase_config['keyword_threshold'] * 0.8:\n",
    "                        continue\n",
    "                    \n",
    "                    record = self.records[i]\n",
    "                    \n",
    "                    # *** NEW: Key phrase exact matching bonus ***\n",
    "                    key_phrase_bonus = 0.0\n",
    "                    if key_phrases_set:\n",
    "                        content_lower = record.get('content', '').lower()\n",
    "                        about_lower = record.get('about', '').lower()\n",
    "                        \n",
    "                        for phrase in key_phrases_set:\n",
    "                            if phrase in content_lower:\n",
    "                                key_phrase_bonus += 0.25  # Strong bonus for content match\n",
    "                            elif phrase in about_lower:\n",
    "                                key_phrase_bonus += 0.20  # Good bonus for about match\n",
    "                    \n",
    "                    # Calculate comprehensive KG score\n",
    "                    kg_score = self.kg.calculate_enhanced_kg_score(query_entities, record, query_type)\n",
    "                    \n",
    "                    # Normalize scores\n",
    "                    norm_sem_score = max(0, min(1, sem_score))\n",
    "                    norm_key_score = max(0, min(1, (key_score + 1) / 2))\n",
    "                    \n",
    "                    # *** MODIFIED: Weighted combination respects query analysis ***\n",
    "                    keyword_weight = search_style.get('semantic_weight', 0.25)  # Will be adjusted if keyword_first\n",
    "                    if query_analysis and query_analysis['search_strategy'] == 'keyword_first':\n",
    "                        # For keyword-first queries, increase keyword contribution\n",
    "                        keyword_weight = 0.35\n",
    "                    \n",
    "                    composite_score = (\n",
    "                        norm_sem_score * search_style['semantic_weight'] +\n",
    "                        norm_key_score * keyword_weight +  # Dynamic keyword weight\n",
    "                        record['kg_authority_score'] * search_style['authority_weight'] +\n",
    "                        record['kg_temporal_score'] * search_style['temporal_weight'] +\n",
    "                        kg_score * search_style['kg_weight'] +\n",
    "                        record['kg_legal_richness'] * 0.08 +\n",
    "                        record.get('kg_pagerank', 0.0) * 0.05 +\n",
    "                        record['kg_completeness_score'] * 0.07 +\n",
    "                        key_phrase_bonus  # *** NEW: Add key phrase bonus ***\n",
    "                    )\n",
    "                    \n",
    "                    # Apply other bonuses (domain, expertise, entity overlap)\n",
    "                    domain_bonus = self._apply_enhanced_domain_bonus(record, query_type, kg_score)\n",
    "                    expertise_bonus = self._apply_researcher_expertise_bonus(record, persona, query_type)\n",
    "                    \n",
    "                    entity_overlap_bonus = 0.0\n",
    "                    if query_entity_set and record.get('kg_entity_count', 0) > 0:\n",
    "                        doc_entities_json = record.get('kg_entities_json', '[]')\n",
    "                        if doc_entities_json != '[]':\n",
    "                            try:\n",
    "                                doc_entities = json.loads(doc_entities_json)\n",
    "                                doc_entity_set = set()\n",
    "                                for entity in doc_entities[:10]:\n",
    "                                    if isinstance(entity, dict):\n",
    "                                        doc_entity_set.add(str(entity.get('text', '')).lower())\n",
    "                                    else:\n",
    "                                        doc_entity_set.add(str(entity).lower())\n",
    "                                \n",
    "                                overlap = query_entity_set & doc_entity_set\n",
    "                                if overlap:\n",
    "                                    entity_overlap_bonus = min(0.15, len(overlap) * 0.05)\n",
    "                            except:\n",
    "                                pass\n",
    "                    \n",
    "                    composite_score += domain_bonus + expertise_bonus + entity_overlap_bonus\n",
    "                    \n",
    "                    # Contextual boost (regulation filter)\n",
    "                    contextual_boost = 0.0\n",
    "                    if regulation_filter:\n",
    "                        try:\n",
    "                            rec_type = str(record.get('regulation_type', '')).lower()\n",
    "                            rec_number = str(record.get('regulation_number', ''))\n",
    "                            rec_year = str(record.get('year', ''))\n",
    "    \n",
    "                            filter_type = str(regulation_filter.get('type') or regulation_filter.get('regulation_type', '')).lower()\n",
    "                            filter_number = str(regulation_filter.get('number') or regulation_filter.get('regulation_number', ''))\n",
    "                            filter_year = str(regulation_filter.get('year', ''))\n",
    "    \n",
    "                            type_match = any(p in rec_type for p in REGULATION_TYPE_PATTERNS.get(filter_type, [filter_type]))\n",
    "                            number_match = (filter_number == rec_number)\n",
    "                            year_match = (not filter_year or filter_year == rec_year)\n",
    "    \n",
    "                            if type_match and number_match and year_match:\n",
    "                                contextual_boost = 0.25\n",
    "                            elif type_match and number_match:\n",
    "                                contextual_boost = 0.15\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error applying contextual boost: {e}\")\n",
    "                            contextual_boost = 0.0\n",
    "                    \n",
    "                    composite_score += contextual_boost\n",
    "                    \n",
    "                    # *** NEW: Law name metadata matching bonus ***\n",
    "                    law_name_bonus = 0.0\n",
    "                    if query_analysis and query_analysis.get('law_name_detected'):\n",
    "                        law_name = query_analysis['specific_entities'][0]['name']\n",
    "                        about_lower = record.get('about', '').lower()\n",
    "                        reg_type_lower = record.get('regulation_type', '').lower()\n",
    "                        \n",
    "                        if law_name in about_lower or law_name in reg_type_lower:\n",
    "                            law_name_bonus = 0.20  # Strong bonus for law name match\n",
    "                    \n",
    "                    composite_score += law_name_bonus\n",
    "                    composite_score = min(1.0, composite_score)\n",
    "    \n",
    "                    candidate_data = {\n",
    "                        'record': record,\n",
    "                        'composite_score': composite_score,\n",
    "                        'semantic_score': norm_sem_score,\n",
    "                        'keyword_score': norm_key_score,\n",
    "                        'kg_score': kg_score,\n",
    "                        'researcher_bias_applied': persona['name'],\n",
    "                        'entity_overlap_bonus': entity_overlap_bonus,\n",
    "                        'contextual_boost': contextual_boost,\n",
    "                        'key_phrase_bonus': key_phrase_bonus,  # NEW\n",
    "                        'law_name_bonus': law_name_bonus,  # NEW\n",
    "                        'query_strategy': query_analysis.get('search_strategy') if query_analysis else 'default'  # NEW\n",
    "                    }\n",
    "    \n",
    "                    candidates.append(candidate_data)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing candidate {i}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Sort and apply diversity\n",
    "            candidates.sort(key=lambda x: x['composite_score'], reverse=True)\n",
    "            diverse_candidates = self._apply_diversity_filter(\n",
    "                candidates[:phase_config['candidates'] * 2],\n",
    "                phase_config['candidates']\n",
    "            )\n",
    "            \n",
    "            return diverse_candidates\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in candidate scoring: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return []\n",
    "    \n",
    "    def _apply_diversity_filter(self, candidates, target_count):\n",
    "        \"\"\"FIXED: Ensure diversity in results\"\"\"\n",
    "        if len(candidates) <= target_count:\n",
    "            return candidates\n",
    "        \n",
    "        diverse_results = []\n",
    "        seen_reg_types = set()\n",
    "        seen_domains = set()\n",
    "        seen_hierarchy = set()\n",
    "        \n",
    "        # First pass: high-scoring diverse candidates\n",
    "        for candidate in candidates:\n",
    "            if len(diverse_results) >= target_count:\n",
    "                break\n",
    "            \n",
    "            record = candidate['record']\n",
    "            reg_type = record['regulation_type']\n",
    "            domain = record.get('kg_primary_domain', 'Unknown')\n",
    "            hierarchy = record.get('kg_hierarchy_level', 5)\n",
    "            \n",
    "            # Add if brings diversity\n",
    "            if (reg_type not in seen_reg_types or \n",
    "                domain not in seen_domains or \n",
    "                hierarchy not in seen_hierarchy or\n",
    "                len(diverse_results) < target_count // 2):\n",
    "                \n",
    "                diverse_results.append(candidate)\n",
    "                seen_reg_types.add(reg_type)\n",
    "                seen_domains.add(domain)\n",
    "                seen_hierarchy.add(hierarchy)\n",
    "        \n",
    "        # Second pass: fill remaining slots with highest scores\n",
    "        remaining = target_count - len(diverse_results)\n",
    "        for candidate in candidates:\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "            if candidate not in diverse_results:\n",
    "                diverse_results.append(candidate)\n",
    "                remaining -= 1\n",
    "        \n",
    "        return diverse_results[:target_count]\n",
    "    \n",
    "    def parallel_legal_research(self, query, query_type, config, progress_callback=None):\n",
    "        \"\"\"\n",
    "        MODIFIED: Context-aware parallel research with advanced query analysis\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if progress_callback:\n",
    "                progress_callback(f\"üîç Assembling legal research team for {query_type} query...\")\n",
    "            \n",
    "            # *** NEW: Advanced Query Analysis ***\n",
    "            query_analysis = self.query_analyzer.analyze_query(query)\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(\n",
    "                    f\"   üß† Query Strategy: {query_analysis['search_strategy']} \"\n",
    "                    f\"(confidence: {query_analysis['confidence']:.0%})\"\n",
    "                )\n",
    "                if query_analysis['reasoning']:\n",
    "                    progress_callback(f\"   üí° Reasoning: {query_analysis['reasoning']}\")\n",
    "                if query_analysis.get('key_phrases'):\n",
    "                    phrases = [p['phrase'] for p in query_analysis['key_phrases']]\n",
    "                    progress_callback(f\"   üéØ Key Phrases: {', '.join(phrases)}\")\n",
    "            \n",
    "            # ... (rest of the method remains the same until individual research)\n",
    "            \n",
    "            # Detect query intent\n",
    "            try:\n",
    "                query_intent = self.context_manager.detect_query_type(query)\n",
    "            except Exception as e:\n",
    "                print(f\"Error detecting query intent: {e}\")\n",
    "                query_intent = 'new_query'\n",
    "            \n",
    "            if progress_callback and query_intent != 'new_query':\n",
    "                progress_callback(f\"   üîÑ Detected {query_intent} - using conversation context...\")\n",
    "            \n",
    "            # Expand query with context\n",
    "            try:\n",
    "                expanded_query = self.context_manager.expand_query_with_context(query)\n",
    "            except Exception as e:\n",
    "                print(f\"Error expanding query: {e}\")\n",
    "                expanded_query = query\n",
    "            \n",
    "            if expanded_query != query and progress_callback:\n",
    "                progress_callback(f\"   ‚ûï Query expanded with context\")\n",
    "            \n",
    "            # Extract entities from expanded query\n",
    "            query_entities = [entity for entity, _ in self.kg.extract_entities_from_text(expanded_query)]\n",
    "            \n",
    "            # Update context\n",
    "            try:\n",
    "                self.context_manager.update_from_query(\n",
    "                    expanded_query, \n",
    "                    [(e, 'regulation_reference') for e in query_entities]\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating context: {e}\")\n",
    "            \n",
    "            # Get regulation filter if applicable\n",
    "            regulation_filter = None\n",
    "            try:\n",
    "                if query_intent in ['followup', 'pronoun_reference', 'continuing_discussion']:\n",
    "                    regulation_filter = self.context_manager.get_regulation_filter()\n",
    "                    if regulation_filter:\n",
    "                        print(f\"DEBUG: Got regulation filter: {regulation_filter}\")\n",
    "                        \n",
    "                        if progress_callback:\n",
    "                            reg_type = regulation_filter.get('type') or regulation_filter.get('regulation_type', 'N/A')\n",
    "                            reg_number = regulation_filter.get('number') or regulation_filter.get('regulation_number', 'N/A')\n",
    "                            progress_callback(f\"   üéØ Context filter: {reg_type} {reg_number}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting regulation filter: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                regulation_filter = None\n",
    "            \n",
    "            # Select team\n",
    "            team_composition = QUERY_TEAM_COMPOSITIONS.get(query_type, QUERY_TEAM_COMPOSITIONS['general'])\n",
    "            selected_researchers = team_composition[:config['research_team_size']]\n",
    "            \n",
    "            if config.get('enable_devil_advocate', True) and 'devils_advocate' not in selected_researchers:\n",
    "                if len(selected_researchers) < 5:\n",
    "                    selected_researchers.append('devils_advocate')\n",
    "                else:\n",
    "                    selected_researchers[-1] = 'devils_advocate'\n",
    "            \n",
    "            if progress_callback:\n",
    "                team_names = [RESEARCH_TEAM_PERSONAS[r]['name'] for r in selected_researchers]\n",
    "                progress_callback(f\"   üë• Team: {', '.join(team_names)}\")\n",
    "            \n",
    "            self.all_phase_results = {}\n",
    "            \n",
    "            query_embedding = self._embed_query_with_context_and_kg(expanded_query, query_type)\n",
    "            \n",
    "            # Phase 1: Individual Research with Query Analysis\n",
    "            individual_research_results = {}\n",
    "            for researcher_id in selected_researchers:\n",
    "                researcher_persona = RESEARCH_TEAM_PERSONAS[researcher_id]\n",
    "                \n",
    "                if progress_callback:\n",
    "                    progress_callback(f\"     {researcher_persona['name']} conducting research...\")\n",
    "                \n",
    "                # *** MODIFIED: Pass query_analysis to individual research ***\n",
    "                individual_results = self._conduct_individual_research(\n",
    "                    researcher_id, researcher_persona, expanded_query, query_type, \n",
    "                    query_embedding, query_entities, config,\n",
    "                    regulation_filter=regulation_filter,\n",
    "                    query_analysis=query_analysis  # NEW PARAMETER\n",
    "                )\n",
    "                individual_research_results[researcher_id] = individual_results\n",
    "                \n",
    "                for phase_key, phase_data in individual_results.get('phase_results', {}).items():\n",
    "                    self.all_phase_results[phase_key] = phase_data\n",
    "            \n",
    "            # ... (rest of method remains unchanged - cross-validation, devil's advocate, consensus)\n",
    "            \n",
    "            # Phase 2: Cross-Validation\n",
    "            consensus_documents = {}\n",
    "            if config.get('enable_cross_validation', True) and len(selected_researchers) > 1:\n",
    "                if progress_callback:\n",
    "                    progress_callback(\"   üîÑ Cross-validation between researchers...\")\n",
    "                \n",
    "                consensus_documents = self._conduct_cross_validation(\n",
    "                    individual_research_results, selected_researchers, query_entities\n",
    "                )\n",
    "            \n",
    "            # Phase 3: Devil's Advocate\n",
    "            if config.get('enable_devil_advocate', True) and 'devils_advocate' in selected_researchers:\n",
    "                if progress_callback:\n",
    "                    progress_callback(\"   üëø Devil's advocate review...\")\n",
    "                \n",
    "                challenge_results = self._conduct_devils_advocate_review(\n",
    "                    individual_research_results, expanded_query, query_type\n",
    "                )\n",
    "                individual_research_results = self._apply_devils_advocate_filters(\n",
    "                    individual_research_results, challenge_results\n",
    "                )\n",
    "            \n",
    "            # Phase 4: Build Consensus\n",
    "            if progress_callback:\n",
    "                progress_callback(\"   ü§ù Building team consensus...\")\n",
    "            \n",
    "            final_consensus = self._build_team_consensus(\n",
    "                individual_research_results, selected_researchers, config, \n",
    "                consensus_documents, query_type\n",
    "            )\n",
    "            \n",
    "            # Update context with results\n",
    "            try:\n",
    "                if final_consensus and len(final_consensus) > 0:\n",
    "                    self.context_manager.update_from_results(final_consensus[:5])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not update context from results: {e}\")\n",
    "            \n",
    "            return final_consensus\n",
    "            \n",
    "        except Exception as e:\n",
    "            if progress_callback:\n",
    "                progress_callback(f\"Error in research: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return []\n",
    "    \n",
    "    def _build_team_consensus(\n",
    "        self, individual_results, team_members, config, consensus_docs=None, query_type='general'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ENHANCED: Build consensus and UPDATE PERFORMANCE METRICS.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            all_candidates_with_attribution = {}\n",
    "            \n",
    "            for researcher_id, results in individual_results.items():\n",
    "                researcher_persona = RESEARCH_TEAM_PERSONAS[researcher_id]\n",
    "                \n",
    "                for phase_key, phase_data in results.get('phase_results', {}).items():\n",
    "                    for candidate in phase_data['candidates']:\n",
    "                        doc_id = candidate['record']['global_id']\n",
    "                        \n",
    "                        if doc_id not in all_candidates_with_attribution:\n",
    "                            all_candidates_with_attribution[doc_id] = {\n",
    "                                'candidate': candidate,\n",
    "                                'researcher_scores': {},\n",
    "                                'supporting_researchers': [],\n",
    "                                'researcher_types': set(),\n",
    "                                'cross_validated': doc_id in (consensus_docs or {})\n",
    "                            }\n",
    "                        \n",
    "                        attribution = all_candidates_with_attribution[doc_id]\n",
    "                        attribution['researcher_scores'][researcher_id] = candidate['composite_score']\n",
    "                        attribution['supporting_researchers'].append(researcher_id)\n",
    "                        attribution['researcher_types'].add(researcher_persona['approach'])\n",
    "            \n",
    "            final_candidates = []\n",
    "            consensus_threshold = config.get('consensus_threshold', 0.6)\n",
    "            \n",
    "            for doc_id, attribution in all_candidates_with_attribution.items():\n",
    "                try:\n",
    "                    total_weight = 0\n",
    "                    weighted_score = 0\n",
    "                    \n",
    "                    for researcher_id, score in attribution['researcher_scores'].items():\n",
    "                        researcher_persona = RESEARCH_TEAM_PERSONAS[researcher_id]\n",
    "                        weight = (researcher_persona['experience_years'] / 15.0) + researcher_persona.get('accuracy_bonus', 0)\n",
    "                        weighted_score += score * weight\n",
    "                        total_weight += weight\n",
    "                    \n",
    "                    final_weighted_score = weighted_score / total_weight if total_weight > 0 else np.mean(list(attribution['researcher_scores'].values()))\n",
    "                    \n",
    "                    # Consensus bonuses\n",
    "                    if len(attribution['supporting_researchers']) > 1:\n",
    "                        consensus_bonus = min(0.10, 0.03 * (len(attribution['supporting_researchers']) - 1))\n",
    "                        final_weighted_score += consensus_bonus\n",
    "                    \n",
    "                    if len(attribution['researcher_types']) > 1:\n",
    "                        final_weighted_score += 0.05\n",
    "                    \n",
    "                    if attribution.get('cross_validated', False):\n",
    "                        final_weighted_score += 0.08\n",
    "                    \n",
    "                    adjusted_threshold = consensus_threshold\n",
    "                    if len(attribution['supporting_researchers']) >= 3:\n",
    "                        adjusted_threshold *= 0.9\n",
    "                    \n",
    "                    if final_weighted_score >= adjusted_threshold:\n",
    "                        candidate = attribution['candidate'].copy()\n",
    "                        candidate['final_consensus_score'] = min(1.0, final_weighted_score)\n",
    "                        candidate['team_consensus'] = True\n",
    "                        candidate['supporting_researchers'] = attribution['supporting_researchers']\n",
    "                        candidate['researcher_agreement'] = len(attribution['supporting_researchers'])\n",
    "                        candidate['cross_validated'] = attribution.get('cross_validated', False)\n",
    "                        \n",
    "                        final_candidates.append(candidate)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing candidate consensus: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            final_candidates.sort(key=lambda x: x.get('final_consensus_score', 0), reverse=True)\n",
    "            \n",
    "            final_candidates = self._apply_hierarchical_review(final_candidates, team_members)\n",
    "            final_candidates = self._resolve_team_conflicts(final_candidates, team_members, config)\n",
    "            \n",
    "            # *** NEW: ADAPTIVE LEARNING - Update performance metrics ***\n",
    "            successful_researchers = []\n",
    "            for candidate in final_candidates[:config.get('final_top_k', 3)]:\n",
    "                successful_researchers.extend(candidate.get('supporting_researchers', []))\n",
    "            \n",
    "            successful_researchers = list(set(successful_researchers))\n",
    "            \n",
    "            # Call learning update\n",
    "            if hasattr(self, 'update_persona_performance'):\n",
    "                self.update_persona_performance(query_type, successful_researchers, team_members)\n",
    "            \n",
    "            return final_candidates\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error building consensus: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return []\n",
    "\n",
    "\n",
    "    def _apply_hierarchical_review(self, consensus_candidates, team_members):\n",
    "        \"\"\"Apply hierarchical review: junior findings reviewed by seniors\"\"\"\n",
    "        try:\n",
    "            if not consensus_candidates:\n",
    "                return consensus_candidates\n",
    "            \n",
    "            # Classify researchers by experience\n",
    "            junior_researchers = []\n",
    "            senior_researchers = []\n",
    "            \n",
    "            for researcher_id in team_members:\n",
    "                persona = RESEARCH_TEAM_PERSONAS[researcher_id]\n",
    "                exp_years = persona['experience_years']\n",
    "                if exp_years <= 5:\n",
    "                    junior_researchers.append(researcher_id)\n",
    "                elif exp_years >= 10:\n",
    "                    senior_researchers.append(researcher_id)\n",
    "            \n",
    "            # If no hierarchy, skip\n",
    "            if not junior_researchers or not senior_researchers:\n",
    "                return consensus_candidates\n",
    "            \n",
    "            # Senior validation: flag documents only found by juniors\n",
    "            reviewed_candidates = []\n",
    "            \n",
    "            for candidate in consensus_candidates:\n",
    "                supporting = candidate.get('supporting_researchers', [])\n",
    "                \n",
    "                # Check if any senior found this\n",
    "                has_senior_support = any(r in senior_researchers for r in supporting)\n",
    "                only_junior_support = all(r in junior_researchers for r in supporting)\n",
    "                \n",
    "                if only_junior_support and not has_senior_support:\n",
    "                    # Apply penalty for lack of senior validation\n",
    "                    original_score = candidate.get('final_consensus_score', 0)\n",
    "                    candidate['final_consensus_score'] = original_score * 0.85\n",
    "                    candidate['needs_senior_review'] = True\n",
    "                    candidate['review_note'] = 'Found by junior researchers only'\n",
    "                else:\n",
    "                    candidate['senior_validated'] = has_senior_support\n",
    "                \n",
    "                reviewed_candidates.append(candidate)\n",
    "            \n",
    "            # Re-sort after review adjustments\n",
    "            reviewed_candidates.sort(key=lambda x: x.get('final_consensus_score', 0), reverse=True)\n",
    "            \n",
    "            return reviewed_candidates\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in hierarchical review: {e}\")\n",
    "            return consensus_candidates\n",
    "\n",
    "    def _resolve_team_conflicts(self, candidates, team_members, config):\n",
    "        \"\"\"Resolve conflicts when team strongly disagrees on results\"\"\"\n",
    "        try:\n",
    "            consensus_threshold = config.get('consensus_threshold', 0.6)\n",
    "            conflict_cases = []\n",
    "            \n",
    "            for candidate in candidates:\n",
    "                supporting = candidate.get('supporting_researchers', [])\n",
    "                scores = candidate.get('researcher_scores', {})\n",
    "                \n",
    "                if len(scores) < 2:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate score variance\n",
    "                score_values = list(scores.values())\n",
    "                mean_score = np.mean(score_values)\n",
    "                variance = np.var(score_values)\n",
    "                \n",
    "                # High variance = conflict\n",
    "                if variance > 0.04:  # Significant disagreement\n",
    "                    # Get senior opinion as tiebreaker\n",
    "                    senior_scores = []\n",
    "                    for researcher_id, score in scores.items():\n",
    "                        persona = RESEARCH_TEAM_PERSONAS[researcher_id]\n",
    "                        if persona['experience_years'] >= 10:\n",
    "                            senior_scores.append(score)\n",
    "                    \n",
    "                    if senior_scores:\n",
    "                        # Use senior consensus as tiebreaker\n",
    "                        senior_consensus = np.mean(senior_scores)\n",
    "                        candidate['conflict_resolved'] = True\n",
    "                        candidate['resolution_method'] = 'senior_tiebreaker'\n",
    "                        candidate['original_score'] = candidate.get('final_consensus_score', 0)\n",
    "                        candidate['final_consensus_score'] = senior_consensus * 0.95  # Slight penalty for conflict\n",
    "                    else:\n",
    "                        # No senior opinion - use supermajority\n",
    "                        if len(supporting) >= len(team_members) * 0.66:  # 2/3 support\n",
    "                            candidate['conflict_resolved'] = True\n",
    "                            candidate['resolution_method'] = 'supermajority'\n",
    "                        else:\n",
    "                            # Insufficient support - penalize\n",
    "                            candidate['final_consensus_score'] *= 0.80\n",
    "                            candidate['conflict_unresolved'] = True\n",
    "            \n",
    "            return candidates\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error resolving conflicts: {e}\")\n",
    "            return candidates\n",
    "            \n",
    "    def research_rounds_with_quality_degradation(\n",
    "        self, query, query_type, initial_candidates, config, progress_callback=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ENHANCED: Multi-round research with community detection and boosting.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if progress_callback:\n",
    "                progress_callback(f\"üîÑ Starting research rounds (max: {config['max_rounds']})...\")\n",
    "            \n",
    "            all_rounds_metadata = dict(getattr(self, 'all_phase_results', {}))\n",
    "            \n",
    "            current_quality = config['initial_quality']\n",
    "            all_rounds_results = []\n",
    "            \n",
    "            # Check if initial results are sufficient\n",
    "            high_quality_candidates = [\n",
    "                c for c in initial_candidates \n",
    "                if c.get('final_consensus_score', c.get('composite_score', 0)) >= current_quality\n",
    "            ]\n",
    "            \n",
    "            if len(high_quality_candidates) >= config['final_top_k']:\n",
    "                if progress_callback:\n",
    "                    progress_callback(f\"   ‚úÖ Initial search sufficient: {len(high_quality_candidates)} high-quality results\")\n",
    "                all_rounds_results.extend(high_quality_candidates)\n",
    "            else:\n",
    "                # Add initial high-quality candidates\n",
    "                all_rounds_results.extend(high_quality_candidates)\n",
    "                \n",
    "                # Conduct additional rounds if needed\n",
    "                for round_num in range(config['max_rounds']):\n",
    "                    if progress_callback:\n",
    "                        progress_callback(f\"   Round {round_num + 1}/{config['max_rounds']} (quality threshold: {current_quality:.2f})...\")\n",
    "                    \n",
    "                    self.all_phase_results = {}\n",
    "                    \n",
    "                    round_results = self.parallel_legal_research(\n",
    "                        query, query_type, config, progress_callback\n",
    "                    )\n",
    "                    \n",
    "                    # Merge metadata\n",
    "                    for phase_key, phase_data in self.all_phase_results.items():\n",
    "                        round_phase_key = f\"round{round_num + 1}_{phase_key}\"\n",
    "                        all_rounds_metadata[round_phase_key] = phase_data\n",
    "                    \n",
    "                    quality_filtered = [\n",
    "                        c for c in round_results \n",
    "                        if c.get('final_consensus_score', c.get('composite_score', 0)) >= current_quality\n",
    "                    ]\n",
    "                    \n",
    "                    if progress_callback:\n",
    "                        progress_callback(f\"      Found {len(quality_filtered)} candidates above threshold\")\n",
    "                    \n",
    "                    all_rounds_results.extend(quality_filtered)\n",
    "                    \n",
    "                    # Citation chain boosting\n",
    "                    if all_rounds_results:\n",
    "                        top_seed_docs = [c['record']['global_id'] for c in all_rounds_results[:5]]\n",
    "                        citation_network = self.kg.follow_citation_chain(top_seed_docs, max_depth=2)\n",
    "                        all_rounds_results = self.kg.boost_cited_documents(all_rounds_results, citation_network)\n",
    "                    \n",
    "                    unique_results = self._deduplicate_candidates(all_rounds_results)\n",
    "                    \n",
    "                    if len(unique_results) >= config['final_top_k']:\n",
    "                        if progress_callback:\n",
    "                            progress_callback(f\"   ‚úÖ Research successful after {round_num + 1} rounds ({len(unique_results)} unique results)\")\n",
    "                        break\n",
    "                    \n",
    "                    current_quality = max(\n",
    "                        config['min_quality'],\n",
    "                        current_quality - (config['quality_degradation'] * 0.7)\n",
    "                    )\n",
    "                    \n",
    "                    if progress_callback:\n",
    "                        progress_callback(f\"      Lowering threshold to {current_quality:.2f} for next round\")\n",
    "                    \n",
    "                    if current_quality <= config['min_quality']:\n",
    "                        if progress_callback:\n",
    "                            progress_callback(f\"   ‚ö†Ô∏è Minimum quality threshold reached\")\n",
    "                        break\n",
    "            \n",
    "            # Deduplicate\n",
    "            unique_results = self._deduplicate_candidates(all_rounds_results)\n",
    "            \n",
    "            # *** NEW: DYNAMIC COMMUNITY DETECTION ***\n",
    "            community_summary = \"Community detection disabled\"\n",
    "            \n",
    "            if hasattr(self, 'community_detector') and self.community_detector.enabled:\n",
    "                if progress_callback:\n",
    "                    progress_callback(f\"üîç Running community detection on {len(unique_results)} documents...\")\n",
    "                \n",
    "                try:\n",
    "                    # Detect communities\n",
    "                    community_analysis = self.community_detector.detect_communities(\n",
    "                        unique_results, \n",
    "                        top_n=min(100, len(unique_results))\n",
    "                    )\n",
    "                    \n",
    "                    # Boost documents in main community\n",
    "                    if community_analysis.get('largest_communities'):\n",
    "                        unique_results = self.community_detector.boost_community_documents(\n",
    "                            unique_results, \n",
    "                            community_analysis\n",
    "                        )\n",
    "                        \n",
    "                        if progress_callback:\n",
    "                            progress_callback(f\"   ‚úÖ Boosted {community_analysis['largest_communities'][0][1]} docs in main community\")\n",
    "                    \n",
    "                    # Get summary\n",
    "                    community_summary = community_analysis.get('summary', 'Community detection completed')\n",
    "                    \n",
    "                    if progress_callback:\n",
    "                        progress_callback(f\"   üìä {community_summary}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if progress_callback:\n",
    "                        progress_callback(f\"   ‚ö†Ô∏è Community detection failed: {e}\")\n",
    "                    community_summary = f\"Community detection failed: {str(e)}\"\n",
    "            \n",
    "            # Sort by final score\n",
    "            unique_results.sort(key=lambda x: x.get('final_consensus_score', x.get('composite_score', 0)), reverse=True)\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(f\"   üèÅ Research completed: {len(unique_results)} final unique results\")\n",
    "            \n",
    "            return unique_results, config['max_rounds'], all_rounds_metadata, community_summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            if progress_callback:\n",
    "                progress_callback(f\"‚ùå Error in research rounds: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            return initial_candidates[:config['final_top_k']], 0, self.all_phase_results or {}, \"Research failed\"\n",
    "    \n",
    "\n",
    "    def _deduplicate_candidates(self, candidates):\n",
    "        \"\"\"Remove duplicates - ORIGINAL\"\"\"\n",
    "        try:\n",
    "            seen_ids = {}\n",
    "            unique_candidates = []\n",
    "            \n",
    "            for candidate in candidates:\n",
    "                doc_id = candidate['record']['global_id']\n",
    "                score = candidate.get('final_consensus_score', candidate.get('composite_score', 0))\n",
    "                \n",
    "                if doc_id not in seen_ids or score > seen_ids[doc_id]['score']:\n",
    "                    seen_ids[doc_id] = {'candidate': candidate, 'score': score}\n",
    "            \n",
    "            for doc_id, data in seen_ids.items():\n",
    "                unique_candidates.append(data['candidate'])\n",
    "            \n",
    "            return unique_candidates\n",
    "        except Exception:\n",
    "            return candidates\n",
    "    \n",
    "    def direct_metadata_search(self, regulation_ref, top_k=20):\n",
    "        \"\"\"\n",
    "        Direct search by regulation metadata.\n",
    "        Returns: List of matching candidates with high confidence.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # FIXED: Handle both key naming conventions\n",
    "            filter_type = ''\n",
    "            filter_number = ''\n",
    "            filter_year = ''\n",
    "            \n",
    "            if regulation_ref:\n",
    "                # Try both naming conventions\n",
    "                filter_type = str(\n",
    "                    regulation_ref.get('type') or \n",
    "                    regulation_ref.get('regulation_type', '')\n",
    "                ).lower()\n",
    "                \n",
    "                filter_number = str(\n",
    "                    regulation_ref.get('number') or \n",
    "                    regulation_ref.get('regulation_number', '')\n",
    "                )\n",
    "                \n",
    "                filter_year = str(regulation_ref.get('year', ''))\n",
    "            \n",
    "            if not filter_number:\n",
    "                print(\"Warning: No regulation number provided for metadata search\")\n",
    "                print(f\"DEBUG: regulation_ref structure: {regulation_ref}\")\n",
    "                return []\n",
    "            \n",
    "            print(f\"DEBUG: Searching for - Type: '{filter_type}', Number: '{filter_number}', Year: '{filter_year}'\")\n",
    "            \n",
    "            matching_candidates = []\n",
    "            \n",
    "            for i, record in enumerate(self.records):\n",
    "                try:\n",
    "                    rec_type = str(record.get('regulation_type', '')).lower()\n",
    "                    rec_number = str(record.get('regulation_number', ''))\n",
    "                    rec_year = str(record.get('year', ''))\n",
    "                    \n",
    "                    if not rec_type or not rec_number:\n",
    "                        continue\n",
    "                    \n",
    "                    # Type matching (flexible)\n",
    "                    type_match = False\n",
    "                    for patterns in REGULATION_TYPE_PATTERNS.values():\n",
    "                        if any(p in rec_type for p in patterns) and any(p in filter_type for p in patterns):\n",
    "                            type_match = True\n",
    "                            break\n",
    "                    \n",
    "                    if not type_match:\n",
    "                        continue\n",
    "                    \n",
    "                    # Number matching (exact)\n",
    "                    number_match = (filter_number == rec_number)\n",
    "                    \n",
    "                    if not number_match:\n",
    "                        continue\n",
    "                    \n",
    "                    # Year matching (exact if provided, otherwise accept)\n",
    "                    year_match = (not filter_year or filter_year == rec_year)\n",
    "                    \n",
    "                    if not year_match:\n",
    "                        continue\n",
    "                    \n",
    "                    # This is a match!\n",
    "                    candidate = {\n",
    "                        'record': record,\n",
    "                        'composite_score': 0.95,\n",
    "                        'semantic_score': 0.0,\n",
    "                        'keyword_score': 0.0,\n",
    "                        'kg_score': float(record.get('kg_connectivity_score', 0.5)),\n",
    "                        'metadata_match': True,\n",
    "                        'match_quality': 'exact' if filter_year else 'partial'\n",
    "                    }\n",
    "                    \n",
    "                    matching_candidates.append(candidate)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing record {i} in metadata search: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"DEBUG: Found {len(matching_candidates)} metadata matches\")\n",
    "            \n",
    "            if not matching_candidates:\n",
    "                print(f\"No metadata matches found for: {filter_type} {filter_number} {filter_year}\")\n",
    "            \n",
    "            # Sort by relevance\n",
    "            matching_candidates.sort(\n",
    "                key=lambda x: (\n",
    "                    x['match_quality'] == 'exact',\n",
    "                    x.get('kg_score', 0),\n",
    "                    x['record'].get('kg_authority_score', 0)\n",
    "                ),\n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            return matching_candidates[:top_k]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in direct_metadata_search: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return []\n",
    "    \n",
    "    def hybrid_search_strategy(self, query, query_type, config, progress_callback=None):\n",
    "        \"\"\"\n",
    "        Hybrid search: metadata-first for specific refs, semantic for concepts.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Extract regulation references with confidence\n",
    "            regulation_refs = self.kg.extract_regulation_references_with_confidence(query)\n",
    "            \n",
    "            if progress_callback:\n",
    "                if regulation_refs:\n",
    "                    best_ref = regulation_refs[0]\n",
    "                    reg_info = best_ref.get('regulation', {})\n",
    "                    progress_callback(\n",
    "                        f\"   üéØ Detected specific regulation: {reg_info.get('type', 'N/A')} \"\n",
    "                        f\"{reg_info.get('number', 'N/A')} \"\n",
    "                        f\"(confidence: {best_ref.get('confidence', 0):.0%})\"\n",
    "                    )\n",
    "            \n",
    "            # Step 2: Decide search strategy based on confidence\n",
    "            use_metadata_search = False\n",
    "            primary_regulation = None\n",
    "            \n",
    "            if regulation_refs:\n",
    "                best_ref = regulation_refs[0]\n",
    "                confidence = best_ref.get('confidence', 0)\n",
    "                specificity = best_ref.get('specificity', 'vague')\n",
    "                \n",
    "                # Use metadata search if complete or partial with high confidence\n",
    "                if specificity == 'complete' or (specificity == 'partial' and confidence >= 0.7):\n",
    "                    use_metadata_search = True\n",
    "                    primary_regulation = best_ref.get('regulation', {})\n",
    "                    \n",
    "                    # Validate primary_regulation has required fields\n",
    "                    if not primary_regulation.get('number'):\n",
    "                        print(f\"Warning: Regulation reference missing number field\")\n",
    "                        use_metadata_search = False\n",
    "                        primary_regulation = None\n",
    "            \n",
    "            # Step 3: Execute appropriate search strategy\n",
    "            if use_metadata_search and primary_regulation:\n",
    "                if progress_callback:\n",
    "                    progress_callback(f\"   üîç Using METADATA search for specific regulation\")\n",
    "                \n",
    "                try:\n",
    "                    return self._metadata_first_search(\n",
    "                        query, query_type, primary_regulation, config, progress_callback\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Metadata search failed, falling back to semantic: {e}\")\n",
    "                    if progress_callback:\n",
    "                        progress_callback(f\"   ‚ö†Ô∏è Metadata search failed, using semantic search\")\n",
    "                    \n",
    "                    return self._semantic_first_search(\n",
    "                        query, query_type, config, progress_callback\n",
    "                    )\n",
    "            else:\n",
    "                if progress_callback:\n",
    "                    progress_callback(f\"   üîç Using SEMANTIC search for conceptual query\")\n",
    "                \n",
    "                return self._semantic_first_search(\n",
    "                    query, query_type, config, progress_callback\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            if progress_callback:\n",
    "                progress_callback(f\"‚ùå Error in hybrid search: {e}\")\n",
    "            print(f\"Error in hybrid search strategy: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            # Fallback to semantic search\n",
    "            try:\n",
    "                return self._semantic_first_search(query, query_type, config, progress_callback)\n",
    "            except Exception as fallback_error:\n",
    "                print(f\"Fallback also failed: {fallback_error}\")\n",
    "                return []\n",
    "    \n",
    "    def _metadata_first_search(self, query, query_type, regulation_ref, config, progress_callback=None):\n",
    "        \"\"\"\n",
    "        Metadata-first search: Find the specific regulation, then find relevant chunks.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if progress_callback:\n",
    "                progress_callback(f\"   Step 1/3: Direct metadata search...\")\n",
    "            \n",
    "            # Validate regulation_ref\n",
    "            if not regulation_ref or not isinstance(regulation_ref, dict):\n",
    "                if progress_callback:\n",
    "                    progress_callback(f\"   ‚ö†Ô∏è Invalid regulation reference, falling back to semantic search\")\n",
    "                return self._semantic_first_search(query, query_type, config, progress_callback)\n",
    "            \n",
    "            # Check if regulation_ref has required fields\n",
    "            if not regulation_ref.get('number'):\n",
    "                if progress_callback:\n",
    "                    progress_callback(f\"   ‚ö†Ô∏è Missing regulation number, falling back to semantic search\")\n",
    "                return self._semantic_first_search(query, query_type, config, progress_callback)\n",
    "            \n",
    "            # Step 1: Direct metadata search\n",
    "            metadata_matches = self.direct_metadata_search(regulation_ref, top_k=50)\n",
    "            \n",
    "            if not metadata_matches:\n",
    "                if progress_callback:\n",
    "                    progress_callback(f\"   ‚ö†Ô∏è No direct matches found, falling back to semantic search\")\n",
    "                \n",
    "                # Fallback to semantic search\n",
    "                return self._semantic_first_search(query, query_type, config, progress_callback)\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(f\"   ‚úÖ Found {len(metadata_matches)} matching documents\")\n",
    "            \n",
    "            # Step 2: Semantic ranking within matched documents\n",
    "            if progress_callback:\n",
    "                progress_callback(f\"   Step 2/3: Semantic ranking within matched documents...\")\n",
    "            \n",
    "            try:\n",
    "                # Extract indices of matched documents\n",
    "                matched_indices = []\n",
    "                for candidate in metadata_matches:\n",
    "                    try:\n",
    "                        idx = self.records.index(candidate['record'])\n",
    "                        matched_indices.append(idx)\n",
    "                    except ValueError:\n",
    "                        # Record not in list, skip\n",
    "                        continue\n",
    "                \n",
    "                if not matched_indices:\n",
    "                    if progress_callback:\n",
    "                        progress_callback(f\"   ‚ö†Ô∏è Could not index matched documents, using base scores\")\n",
    "                    # Just return metadata matches as-is\n",
    "                    return metadata_matches[:config.get('final_top_k', 10)]\n",
    "                \n",
    "                # Embed query\n",
    "                query_embedding = self._embed_query_with_context_and_kg(query, query_type)\n",
    "                \n",
    "                # Get embeddings for matched documents only\n",
    "                matched_embeddings = self.embeddings[matched_indices]\n",
    "                \n",
    "                # Calculate semantic similarity within matches\n",
    "                semantic_sims = F.cosine_similarity(\n",
    "                    query_embedding.unsqueeze(0),\n",
    "                    matched_embeddings,\n",
    "                    dim=1\n",
    "                ).cpu().numpy()\n",
    "                \n",
    "                # Update candidates with semantic scores\n",
    "                for i, candidate in enumerate(metadata_matches):\n",
    "                    if i < len(semantic_sims):\n",
    "                        candidate['semantic_score'] = float(semantic_sims[i])\n",
    "                        \n",
    "                        # Recalculate composite score\n",
    "                        candidate['composite_score'] = (\n",
    "                            0.60 +  # Base score for metadata match\n",
    "                            candidate['semantic_score'] * 0.25 +\n",
    "                            candidate.get('kg_score', 0) * 0.15\n",
    "                        )\n",
    "                \n",
    "                # Sort by composite score\n",
    "                metadata_matches.sort(key=lambda x: x['composite_score'], reverse=True)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in semantic ranking: {e}\")\n",
    "                # Continue with metadata matches only\n",
    "                if progress_callback:\n",
    "                    progress_callback(f\"   ‚ö†Ô∏è Semantic ranking failed, using metadata scores only\")\n",
    "            \n",
    "            # Step 3: Light team review for quality assurance\n",
    "            if progress_callback:\n",
    "                progress_callback(f\"   Step 3/3: Team consensus on top candidates...\")\n",
    "            \n",
    "            # Select top candidates for team review\n",
    "            top_candidates = metadata_matches[:min(30, len(metadata_matches))]\n",
    "            \n",
    "            # Simple team consensus\n",
    "            query_entities = [entity for entity, _ in self.kg.extract_entities_from_text(query)]\n",
    "            final_candidates = self._light_team_consensus(\n",
    "                top_candidates, query_entities, query_type, config\n",
    "            )\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(f\"   ‚úÖ Metadata-first search completed: {len(final_candidates)} results\")\n",
    "            \n",
    "            return final_candidates\n",
    "            \n",
    "        except Exception as e:\n",
    "            if progress_callback:\n",
    "                progress_callback(f\"   ‚ùå Error in metadata-first search: {e}\")\n",
    "            print(f\"Error in metadata-first search: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            # Final fallback\n",
    "            return self._semantic_first_search(query, query_type, config, progress_callback)\n",
    "    \n",
    "    def _light_team_consensus(self, candidates, query_entities, query_type, config):\n",
    "        \"\"\"\n",
    "        Lightweight team consensus for metadata matches.\n",
    "        Focus on validation rather than discovery.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Apply KG scoring\n",
    "            for candidate in candidates:\n",
    "                record = candidate['record']\n",
    "                kg_score = self.kg.calculate_enhanced_kg_score(\n",
    "                    query_entities, record, query_type\n",
    "                )\n",
    "                candidate['kg_score'] = kg_score\n",
    "                \n",
    "                # Update composite score with KG\n",
    "                candidate['composite_score'] = min(1.0, (\n",
    "                    candidate['composite_score'] * 0.7 +\n",
    "                    kg_score * 0.3\n",
    "                ))\n",
    "            \n",
    "            # Sort by final score\n",
    "            candidates.sort(key=lambda x: x['composite_score'], reverse=True)\n",
    "            \n",
    "            # Mark as team validated\n",
    "            for candidate in candidates:\n",
    "                candidate['team_consensus'] = True\n",
    "                candidate['metadata_match'] = True\n",
    "            \n",
    "            return candidates[:config.get('final_top_k', 10)]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in light team consensus: {e}\")\n",
    "            return candidates[:config.get('final_top_k', 10)]\n",
    "    \n",
    "    def _semantic_first_search(self, query, query_type, config, progress_callback=None):\n",
    "        \"\"\"\n",
    "        Original semantic search - for conceptual queries.\n",
    "        This calls the existing parallel_legal_research logic.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.parallel_legal_research(query, query_type, config, progress_callback)\n",
    "        except Exception as e:\n",
    "            if progress_callback:\n",
    "                progress_callback(f\"Error in semantic search: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return []\n",
    "\n",
    "# =============================================================================\n",
    "# RERANKER WITH ENHANCED KG (KEEP ORIGINAL + ENHANCE)\n",
    "# =============================================================================\n",
    "\n",
    "class EnhancedKGReranker:\n",
    "    \"\"\"Reranker with ENHANCED KG context - keeps original logic\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, prefix_tokens, suffix_tokens, \n",
    "                 token_true_id, token_false_id, max_length, knowledge_graph):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prefix_tokens = prefix_tokens\n",
    "        self.suffix_tokens = suffix_tokens\n",
    "        self.token_true_id = token_true_id\n",
    "        self.token_false_id = token_false_id\n",
    "        self.max_length = max_length\n",
    "        self.kg = knowledge_graph\n",
    "    \n",
    "    def format_instruction_with_kg(self, query, doc, query_type):\n",
    "        \"\"\"Format instruction with ENHANCED KG-context\"\"\"\n",
    "        try:\n",
    "            query_entities = [entity for entity, _ in self.kg.extract_entities_from_text(query)]\n",
    "            \n",
    "            # ENHANCED: Get advanced KG data\n",
    "            if isinstance(doc, dict):\n",
    "                record = doc\n",
    "                doc_id = record.get('global_id')\n",
    "                \n",
    "                # Parse advanced KG features\n",
    "                doc_entities = self.kg.get_parsed_kg_data(doc_id, 'entities') or []\n",
    "                cross_refs = self.kg.get_parsed_kg_data(doc_id, 'cross_refs') or []\n",
    "                domains = self.kg.get_parsed_kg_data(doc_id, 'domains') or []\n",
    "                \n",
    "                kg_connections = []\n",
    "                \n",
    "                # Entity matches\n",
    "                for q_entity in query_entities[:3]:\n",
    "                    for d_entity in doc_entities[:3]:\n",
    "                        try:\n",
    "                            d_entity_text = d_entity.get('text', '') if isinstance(d_entity, dict) else str(d_entity)\n",
    "                            if str(q_entity).lower() == str(d_entity_text).lower():\n",
    "                                kg_connections.append(f\"Exact match: {q_entity}\")\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                \n",
    "                # Cross-reference info\n",
    "                if cross_refs:\n",
    "                    kg_connections.append(f\"{len(cross_refs)} cross-references\")\n",
    "                \n",
    "                # Domain info\n",
    "                if record.get('kg_primary_domain'):\n",
    "                    kg_connections.append(f\"Domain: {record['kg_primary_domain']}\")\n",
    "            else:\n",
    "                kg_connections = []\n",
    "            \n",
    "            instruction_map = {\n",
    "                'specific_article': '''Sebagai ahli hukum, evaluasi apakah dokumen peraturan ini berisi pasal/ayat spesifik yang diminta. \n",
    "                Fokus pada: (1) Kecocokan nomor pasal/ayat, (2) Relevansi isi ketentuan, (3) Kesesuaian konteks hukum.''',\n",
    "                'procedural': '''Sebagai praktisi hukum, evaluasi apakah dokumen ini menjelaskan prosedur yang ditanyakan. \n",
    "                Fokus pada: (1) Kelengkapan langkah-langkah, (2) Kejelasan persyaratan, (3) Ketepatan implementasi.''',\n",
    "                'definitional': '''Sebagai akademisi hukum, evaluasi apakah dokumen ini memberikan definisi yang ditanyakan. \n",
    "                Fokus pada: (1) Kejelasan definisi, (2) Otoritas sumber, (3) Konteks penggunaan istilah.''',\n",
    "                'sanctions': '''Sebagai penegak hukum, evaluasi apakah dokumen ini mengatur sanksi yang ditanyakan. \n",
    "                Fokus pada: (1) Jenis sanksi, (2) Besaran/bentuk hukuman, (3) Kondisi penerapan.''',\n",
    "                'general': '''Sebagai konsultan hukum, evaluasi relevansi dokumen peraturan ini terhadap pertanyaan hukum. \n",
    "                Pertimbangkan: (1) Kesesuaian topik, (2) Otoritas regulasi, (3) Kemutakhiran ketentuan, (4) Kelengkapan informasi.'''}\n",
    "            \n",
    "            instruction = instruction_map.get(query_type, instruction_map['general'])\n",
    "            \n",
    "            kg_context = \"\"\n",
    "            if kg_connections:\n",
    "                kg_context = f\"\\nHubungan konseptual: {'; '.join(kg_connections[:3])}\"\n",
    "            \n",
    "            if isinstance(doc, dict):\n",
    "                doc_text = doc.get('content', '')\n",
    "            else:\n",
    "                doc_text = str(doc)\n",
    "                \n",
    "            if len(doc_text) > 1000:\n",
    "                doc_text = doc_text[:1000] + \"...\"\n",
    "            \n",
    "            return f\"<Instruct>: {instruction}{kg_context}\\n<Query>: {query}\\n<Document>: {doc_text}\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error in KG formatting: {e}\")\n",
    "            return f\"<Instruct>: Evaluate document relevance\\n<Query>: {query}\\n<Document>: {str(doc)[:1000]}\"\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def rerank_with_kg(self, query, candidates, query_type, config, top_k=None, progress_callback=None):\n",
    "        \"\"\"Rerank with ENHANCED KG - MODIFIED FOR CPU\"\"\"\n",
    "        try:\n",
    "            if not candidates:\n",
    "                return candidates\n",
    "            \n",
    "            if top_k:\n",
    "                candidates = candidates[:top_k]\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(f\"   ‚öñÔ∏è KG-enhanced reranking {len(candidates)} candidates...\")\n",
    "            \n",
    "            query_entities = [entity for entity, _ in self.kg.extract_entities_from_text(query)]\n",
    "            \n",
    "            pairs = []\n",
    "            for candidate in candidates:\n",
    "                try:\n",
    "                    record = candidate['record']\n",
    "                    doc_context = self._create_rich_document_context(record, query_entities)\n",
    "                    pair = self.format_instruction_with_kg(query, doc_context, query_type)\n",
    "                    pairs.append(pair)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error creating pair: {e}\")\n",
    "                    pairs.append(f\"<Query>: {query}\\n<Document>: {record.get('content', '')[:500]}\")\n",
    "            \n",
    "            batch_size = 2\n",
    "            all_rerank_scores = []\n",
    "            \n",
    "            for i in range(0, len(pairs), batch_size):\n",
    "                batch_pairs = pairs[i:i+batch_size]\n",
    "                \n",
    "                try:\n",
    "                    inputs = self.tokenizer(\n",
    "                        batch_pairs, padding=False, truncation='longest_first',\n",
    "                        return_attention_mask=False, \n",
    "                        max_length=self.max_length - len(self.prefix_tokens) - len(self.suffix_tokens)\n",
    "                    )\n",
    "                    \n",
    "                    for j, ele in enumerate(inputs['input_ids']):\n",
    "                        inputs['input_ids'][j] = self.prefix_tokens + ele + self.suffix_tokens\n",
    "                    \n",
    "                    inputs = self.tokenizer.pad(inputs, padding=True, return_tensors=\"pt\", max_length=self.max_length)\n",
    "                    inputs = {k: v.to('cpu') for k, v in inputs.items()}  # Force CPU\n",
    "                    \n",
    "                    batch_scores = self.model(**inputs).logits[:, -1, :]\n",
    "                    true_vector = batch_scores[:, self.token_true_id]\n",
    "                    false_vector = batch_scores[:, self.token_false_id]\n",
    "                    batch_scores = torch.stack([false_vector, true_vector], dim=1)\n",
    "                    batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
    "                    scores = batch_scores[:, 1].exp().tolist()\n",
    "                    all_rerank_scores.extend(scores)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Reranking batch error: {e}\")\n",
    "                    all_rerank_scores.extend([0.4] * len(batch_pairs))\n",
    "                \n",
    "                clear_cache()\n",
    "            \n",
    "            enhanced_candidates = []\n",
    "            for i, candidate in enumerate(candidates):\n",
    "                try:\n",
    "                    base_rerank_score = all_rerank_scores[i] if i < len(all_rerank_scores) else 0.4\n",
    "                    \n",
    "                    # ENHANCED: Apply new KG bonuses\n",
    "                    enhanced_score = self._apply_enhanced_kg_bonuses(\n",
    "                        candidate, base_rerank_score, query_type, query_entities\n",
    "                    )\n",
    "                    \n",
    "                    candidate['base_rerank_score'] = base_rerank_score\n",
    "                    candidate['enhanced_rerank_score'] = enhanced_score\n",
    "                    candidate['final_score'] = enhanced_score\n",
    "                    enhanced_candidates.append(candidate)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error enhancing candidate: {e}\")\n",
    "                    candidate['final_score'] = 0.4\n",
    "                    enhanced_candidates.append(candidate)\n",
    "            \n",
    "            enhanced_candidates.sort(key=lambda x: x.get('final_score', 0), reverse=True)\n",
    "            return enhanced_candidates\n",
    "        except Exception as e:\n",
    "            print(f\"Reranking error: {e}\")\n",
    "            return candidates\n",
    "    \n",
    "    def _create_rich_document_context(self, record, query_entities):\n",
    "        \"\"\"Create rich document context with ENHANCED features\"\"\"\n",
    "        try:\n",
    "            context_parts = []\n",
    "            \n",
    "            context_parts.append(f\"[{record['regulation_type']} No. {record['regulation_number']}/{record['year']}]\")\n",
    "            context_parts.append(f\"Badan: {record['enacting_body']}\")\n",
    "            context_parts.append(f\"Tentang: {record['about']}\")\n",
    "            \n",
    "            if record.get('chapter', 'N/A') != 'N/A' or record.get('article', 'N/A') != 'N/A':\n",
    "                context_parts.append(f\"Referensi: Bab {record.get('chapter', 'N/A')} - Pasal {record.get('article', 'N/A')}\")\n",
    "            \n",
    "            # ENHANCED: Add KG metadata\n",
    "            if record.get('kg_primary_domain'):\n",
    "                context_parts.append(f\"Domain: {record['kg_primary_domain']}\")\n",
    "            \n",
    "            if record.get('kg_cross_ref_count', 0) > 0:\n",
    "                context_parts.append(f\"Cross-refs: {record['kg_cross_ref_count']}\")\n",
    "            \n",
    "            context_parts.append(f\"Isi: {record['content']}\")\n",
    "            \n",
    "            return \" | \".join(context_parts)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating document context: {e}\")\n",
    "            return record.get('content', 'No content')\n",
    "    \n",
    "    def _apply_enhanced_kg_bonuses(self, candidate, base_score, query_type, query_entities):\n",
    "        \"\"\"Apply ENHANCED KG bonuses with new features\"\"\"\n",
    "        try:\n",
    "            record = candidate['record']\n",
    "            enhanced_score = base_score\n",
    "            \n",
    "            # Original bonuses\n",
    "            enhanced_score += record['kg_authority_score'] * 0.15\n",
    "            \n",
    "            if record['kg_temporal_score'] > 0.8:\n",
    "                enhanced_score += 0.1\n",
    "            elif record['kg_temporal_score'] < 0.4:\n",
    "                enhanced_score -= 0.05\n",
    "            \n",
    "            enhanced_score += record['kg_legal_richness'] * 0.08\n",
    "            \n",
    "            # ENHANCED: New feature bonuses\n",
    "            if record.get('kg_hierarchy_level', 5) <= 3:\n",
    "                enhanced_score += 0.10\n",
    "            \n",
    "            if record.get('kg_cross_ref_count', 0) > 5:\n",
    "                enhanced_score += 0.08\n",
    "            \n",
    "            if record.get('kg_pagerank', 0.0) > 0.01:\n",
    "                enhanced_score += min(0.12, record['kg_pagerank'] * 10)\n",
    "            \n",
    "            if record.get('kg_connectivity_score', 0) > 0.7:\n",
    "                enhanced_score += 0.09\n",
    "            \n",
    "            if record.get('kg_domain_confidence', 0) > 0.8:\n",
    "                enhanced_score += 0.07\n",
    "            \n",
    "            # Query-type specific bonuses\n",
    "            kg_score = candidate.get('kg_score', 0)\n",
    "            if query_type == 'specific_article':\n",
    "                if any(term in record['content'].lower() for term in ['pasal', 'ayat']):\n",
    "                    enhanced_score += 0.1\n",
    "                    if kg_score > 0.4:\n",
    "                        enhanced_score += 0.05\n",
    "            elif query_type == 'sanctions':\n",
    "                if record.get('kg_has_prohibitions', False):\n",
    "                    enhanced_score += 0.15\n",
    "                if any(term in record['content'].lower() for term in ['pidana', 'denda', 'sanksi']):\n",
    "                    enhanced_score += 0.12\n",
    "            elif query_type == 'procedural':\n",
    "                if record.get('kg_has_obligations', False):\n",
    "                    enhanced_score += 0.12\n",
    "            \n",
    "            # Team consensus bonus\n",
    "            if candidate.get('team_consensus', False):\n",
    "                enhanced_score += 0.08\n",
    "            \n",
    "            if record['kg_completeness_score'] > 0.8:\n",
    "                enhanced_score += 0.06\n",
    "            \n",
    "            return max(0.0, min(1.0, enhanced_score))\n",
    "        except Exception as e:\n",
    "            print(f\"Error applying bonuses: {e}\")\n",
    "            return base_score\n",
    "\n",
    "# LLM Generator and Conversation Manager remain UNCHANGED from original\n",
    "# (keeping them exactly as in original code to maintain compatibility)\n",
    "\n",
    "# =============================================================================\n",
    "# LLM GENERATOR (UNCHANGED)\n",
    "# LLM GENERATOR (UNCHANGED - exactly as original)\n",
    "# =============================================================================\n",
    "\n",
    "class KGEnhancedLLMGenerator:\n",
    "    \"\"\"LLM generator with KG context preservation - UNCHANGED\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, knowledge_graph):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kg = knowledge_graph\n",
    "        \n",
    "    def format_context_with_kg_protection(self, query, results, query_type, config, all_phase_metadata=None):\n",
    "        \"\"\"Format context with KG enhancement and BALANCED metadata protection\"\"\"\n",
    "        try:\n",
    "            context_parts = []\n",
    "            \n",
    "            # UPDATED: More nuanced anti-hallucination instruction\n",
    "            context_parts.append(\"\"\"CRITICAL INSTRUCTIONS - READ CAREFULLY:\n",
    "    \n",
    "    You are a professional Indonesian legal assistant. Your primary duty is to be accurate, factual, and helpful.\n",
    "    \n",
    "    RULE 1 - SOURCE RESTRICTION (STRICT):\n",
    "    You MUST base your entire answer only on the text from the \"Legal References\" provided below. You MUST NOT mention, cite, or allude to any law, regulation, or external information that is not in this provided list.\n",
    "    \n",
    "    RULE 2 - SYNTHESIS RULE (IMPORTANT):\n",
    "    If a user's question is conceptual (e.g., \"is there equality?\", \"what is the relationship?\", \"how does this work?\"), you MUST synthesize a comprehensive answer by combining and reasoning over the provided facts. Do not just look for exact keyword matches from the question. Use logical reasoning to connect the dots between multiple pieces of information in the provided documents.\n",
    "    \n",
    "    RULE 3 - FACT-FINDING RULE (IMPORTANT):\n",
    "    If a user asks for a specific, hard fact (e.g., \"what is the exact penalty amount?\", \"what is the effective date?\", \"what is Article 25?\") and that specific fact is not explicitly stated in the provided text, you MUST clearly state that the specific information is not present in the available documents.\n",
    "    \n",
    "    RULE 4 - METADATA ACCURACY (STRICT):\n",
    "    You MUST NOT modify, invent, or alter any metadata such as regulation numbers, years, enacting bodies, or article numbers. Use ONLY the exact information provided in the documents below.\n",
    "    \n",
    "    RULE 5 - WHEN CONTEXT IS INSUFFICIENT:\n",
    "    If the provided documents are genuinely insufficient to answer the user's question (either conceptually or factually), clearly state: \"The provided legal references do not contain sufficient information to answer this question.\" Then suggest the user rephrase or ask a different question.\n",
    "    \n",
    "    BALANCE: Be confident in synthesizing and reasoning when the facts support it. Be honest when specific facts are missing.\"\"\")\n",
    "            \n",
    "            query_entities = [entity for entity, _ in self.kg.extract_entities_from_text(query)]\n",
    "            \n",
    "            instruction_map = {\n",
    "                'specific_article': \"Anda akan memberikan penjelasan detail tentang pasal/ayat spesifik berdasarkan peraturan hukum Indonesia yang tersedia. Manfaatkan hubungan antar konsep struktural dalam analisis.\",\n",
    "                'procedural': \"Anda akan menjelaskan prosedur dan tata cara berdasarkan peraturan hukum Indonesia yang tersedia. Pertimbangkan keterkaitan antar tahapan prosedur.\",\n",
    "                'definitional': \"Anda akan memberikan definisi komprehensif berdasarkan ketentuan hukum Indonesia yang tersedia. Jelaskan hubungan antar konsep terkait.\",\n",
    "                'sanctions': \"Anda akan menjelaskan sanksi dan konsekuensi hukum berdasarkan peraturan Indonesia yang tersedia. Analisis keterkaitan jenis sanksi dengan pelanggaran.\",\n",
    "                'general': \"Anda akan memberikan penjelasan komprehensif berdasarkan peraturan hukum Indonesia yang tersedia. Manfaatkan hubungan semantik antar konsep hukum.\"\n",
    "            }\n",
    "            \n",
    "            context_parts.append(instruction_map.get(query_type, instruction_map['general']))\n",
    "            \n",
    "            if query_entities:\n",
    "                context_parts.append(f\"\\nKonsep kunci yang relevan: {', '.join(query_entities[:5])}\")\n",
    "            \n",
    "            context_parts.append(f\"\\nDokumen hukum yang relevan dengan pertanyaan: '{query}'\\n\")\n",
    "            \n",
    "            for i, result in enumerate(results[:config['final_top_k']], 1):\n",
    "                try:\n",
    "                    record = result['record']\n",
    "                    context_parts.append(f\"DOKUMEN {i}:\")\n",
    "                    context_parts.append(f\"Jenis: {record['regulation_type']} No. {record['regulation_number']}/{record['year']}\")\n",
    "                    context_parts.append(f\"Tentang: {record['about']}\")\n",
    "                    context_parts.append(f\"Badan: {record['enacting_body']}\")\n",
    "                    \n",
    "                    if record['chapter'] != 'N/A':\n",
    "                        context_parts.append(f\"Bab: {record['chapter']}\")\n",
    "                    if record['article'] != 'N/A':\n",
    "                        context_parts.append(f\"Pasal: {record['article']}\")\n",
    "                    \n",
    "                    kg_score = result.get('kg_score', 0)\n",
    "                    if kg_score > 0.3:\n",
    "                        context_parts.append(f\"Relevansi konseptual: {kg_score:.2f}\")\n",
    "                    \n",
    "                    context_parts.append(f\"Isi: {record['content']}\")\n",
    "                    context_parts.append(\"\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error formatting document {i}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            context_parts.append(\"INSTRUKSI AKHIR:\")\n",
    "            context_parts.append(\"1. Gunakan format <think>...</think> untuk proses berpikir Anda\")\n",
    "            context_parts.append(\"2. Dalam <think>, analisis apakah pertanyaan memerlukan sintesis konseptual atau fakta spesifik\")\n",
    "            context_parts.append(\"3. Jika konseptual: gabungkan dan sintesis informasi dari dokumen untuk menjawab\")\n",
    "            context_parts.append(\"4. Jika fakta spesifik: cari fakta tersebut secara eksplisit dalam teks\")\n",
    "            context_parts.append(\"5. JANGAN kutip atau rujuk peraturan yang tidak ada dalam daftar dokumen di atas\")\n",
    "            context_parts.append(\"6. Jelaskan konteks dan implikasi hukum dengan mempertimbangkan keterkaitan konsep\")\n",
    "            context_parts.append(\"7. Berikan jawaban dalam bahasa Indonesia yang jelas dan profesional\")\n",
    "            \n",
    "            return \"\\n\".join(context_parts)\n",
    "        except Exception as e:\n",
    "            print(f\"Error formatting context: {e}\")\n",
    "            return f\"Berikan penjelasan tentang: {query}\\n\\nGunakan format <think>...</think> untuk proses berpikir.\"\n",
    "            \n",
    "    def generate_with_kg(self, query, results, query_type, config, all_phase_metadata=None):\n",
    "        \"\"\"Generate response with KG enhancement using streaming - UNCHANGED\"\"\"\n",
    "        try:\n",
    "            context = self.format_context_with_kg_protection(query, results, query_type, config, all_phase_metadata)\n",
    "            \n",
    "            kg_enhanced_system_prompt = SYSTEM_PROMPT + \" Manfaatkan hubungan semantik dan konseptual antar elemen hukum untuk memberikan analisis yang lebih komprehensif dan kontekstual.\"\n",
    "            \n",
    "            input_ids = self.tokenizer.apply_chat_template([\n",
    "                {'role': 'system', 'content': kg_enhanced_system_prompt},\n",
    "                {'role': 'user', 'content': f\"{context}\\n\\nPertanyaan: {query}\"}\n",
    "            ], tokenize=True, add_generation_prompt=True, return_tensors='pt').to(self.model.device)\n",
    "            \n",
    "            return input_ids, {\n",
    "                'query_type': query_type,\n",
    "                'num_sources': len(results),\n",
    "                'kg_enhanced': True,\n",
    "                'metadata_protected': True\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in LLM generation setup: {e}\")\n",
    "            simple_prompt = f\"Jelaskan tentang: {query}\"\n",
    "            input_ids = self.tokenizer([simple_prompt], return_tensors='pt')['input_ids'].to(self.model.device)\n",
    "            return input_ids, {'query_type': query_type, 'num_sources': 0, 'error': str(e)}\n",
    "\n",
    "    def generate_with_kg_and_context(self, query, results, query_type, config, conversation_context=\"\", all_phase_metadata=None):\n",
    "        \"\"\"Generate response with KG enhancement and conversation context - UNCHANGED\"\"\"\n",
    "        try:\n",
    "            context = self.format_context_with_kg_protection(query, results, query_type, config, all_phase_metadata)\n",
    "            \n",
    "            if conversation_context.strip():\n",
    "                context = f\"{conversation_context}\\n\\n---\\n\\n{context}\"\n",
    "            \n",
    "            kg_enhanced_system_prompt = SYSTEM_PROMPT + \" Manfaatkan hubungan semantik dan konseptual antar elemen hukum untuk memberikan analisis yang lebih komprehensif dan kontekstual. Pertimbangkan konteks percakapan sebelumnya untuk memberikan jawaban yang relevan dan berkesinambungan.\"\n",
    "            \n",
    "            input_ids = self.tokenizer.apply_chat_template([\n",
    "                {'role': 'system', 'content': kg_enhanced_system_prompt},\n",
    "                {'role': 'user', 'content': f\"{context}\\n\\nPertanyaan: {query}\"}\n",
    "            ], tokenize=True, add_generation_prompt=True, return_tensors='pt').to(self.model.device)\n",
    "            \n",
    "            return input_ids, {\n",
    "                'query_type': query_type,\n",
    "                'num_sources': len(results),\n",
    "                'kg_enhanced': True,\n",
    "                'metadata_protected': True,\n",
    "                'context_aware': bool(conversation_context.strip())\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in context-aware LLM generation setup: {e}\")\n",
    "            return self.generate_with_kg(query, results, query_type, config, all_phase_metadata)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Dynamic Community Detection Module\n",
    "# ============================================================================\n",
    "\n",
    "class DynamicCommunityDetector:\n",
    "    \"\"\"\n",
    "    Discovers hidden thematic clusters in search results using network analysis.\n",
    "    Uses cross-references between documents to build a graph and detect communities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, knowledge_graph):\n",
    "        self.kg = knowledge_graph\n",
    "        self.enabled = True\n",
    "    \n",
    "    def detect_communities(self, candidates, top_n=100):\n",
    "        \"\"\"\n",
    "        Detect thematic communities in retrieved documents.\n",
    "        \n",
    "        Args:\n",
    "            candidates: List of candidate documents with records\n",
    "            top_n: Number of top candidates to analyze\n",
    "            \n",
    "        Returns:\n",
    "            Dict with community assignments and metadata\n",
    "        \"\"\"\n",
    "        if not self.enabled:\n",
    "            return {'communities': {}, 'summary': 'Community detection disabled'}\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Select top candidates\n",
    "            top_candidates = candidates[:min(top_n, len(candidates))]\n",
    "            \n",
    "            if len(top_candidates) < 3:\n",
    "                return {'communities': {}, 'summary': 'Insufficient documents for community detection'}\n",
    "            \n",
    "            print(f\"üîç Analyzing {len(top_candidates)} documents for communities...\")\n",
    "            \n",
    "            # Step 2: Build document graph\n",
    "            graph_data = self._build_document_graph(top_candidates)\n",
    "            \n",
    "            if graph_data['edge_count'] == 0:\n",
    "                return {'communities': {}, 'summary': 'No cross-references found between documents'}\n",
    "            \n",
    "            # Step 3: Detect communities using Louvain algorithm\n",
    "            communities = self._run_louvain_algorithm(graph_data)\n",
    "            \n",
    "            # Step 4: Analyze communities\n",
    "            community_analysis = self._analyze_communities(communities, top_candidates)\n",
    "            \n",
    "            print(f\"‚úÖ Found {len(community_analysis['community_sizes'])} communities\")\n",
    "            \n",
    "            return community_analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in community detection: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return {'communities': {}, 'summary': f'Community detection failed: {str(e)}'}\n",
    "    \n",
    "    def _build_document_graph(self, candidates):\n",
    "        \"\"\"Build an igraph graph from document cross-references\"\"\"\n",
    "        try:\n",
    "            # Create document ID to index mapping\n",
    "            doc_id_to_idx = {}\n",
    "            idx_to_doc_id = {}\n",
    "            \n",
    "            for idx, candidate in enumerate(candidates):\n",
    "                doc_id = candidate['record']['global_id']\n",
    "                doc_id_to_idx[doc_id] = idx\n",
    "                idx_to_doc_id[idx] = doc_id\n",
    "            \n",
    "            # Build edge list from cross-references\n",
    "            edges = []\n",
    "            edge_weights = []\n",
    "            \n",
    "            for candidate in candidates:\n",
    "                doc_id = candidate['record']['global_id']\n",
    "                source_idx = doc_id_to_idx[doc_id]\n",
    "                \n",
    "                # Get cross-references for this document\n",
    "                cross_refs = self.kg.get_parsed_kg_data(doc_id, 'cross_refs')\n",
    "                \n",
    "                if cross_refs:\n",
    "                    for ref in cross_refs:\n",
    "                        try:\n",
    "                            # Extract target document ID\n",
    "                            if isinstance(ref, dict):\n",
    "                                target_id = ref.get('target_id')\n",
    "                            else:\n",
    "                                target_id = str(ref)\n",
    "                            \n",
    "                            # Check if target is in our candidate set\n",
    "                            if target_id in doc_id_to_idx:\n",
    "                                target_idx = doc_id_to_idx[target_id]\n",
    "                                edges.append((source_idx, target_idx))\n",
    "                                edge_weights.append(1.0)\n",
    "                        except Exception:\n",
    "                            continue\n",
    "            \n",
    "            # Create igraph graph\n",
    "            g = ig.Graph(n=len(candidates), edges=edges, directed=True)\n",
    "            g.es['weight'] = edge_weights\n",
    "            \n",
    "            # Convert to undirected for community detection\n",
    "            g_undirected = g.as_undirected(mode='collapse', combine_edges='sum')\n",
    "            \n",
    "            return {\n",
    "                'graph': g_undirected,\n",
    "                'doc_id_to_idx': doc_id_to_idx,\n",
    "                'idx_to_doc_id': idx_to_doc_id,\n",
    "                'edge_count': len(edges),\n",
    "                'node_count': len(candidates)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error building document graph: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _run_louvain_algorithm(self, graph_data):\n",
    "        \"\"\"Run Louvain community detection algorithm\"\"\"\n",
    "        try:\n",
    "            g = graph_data['graph']\n",
    "            \n",
    "            # Run Louvain algorithm (built into igraph)\n",
    "            communities = g.community_multilevel(weights='weight')\n",
    "            \n",
    "            # Convert to dict: doc_id -> community_id\n",
    "            community_assignments = {}\n",
    "            \n",
    "            for comm_id, members in enumerate(communities):\n",
    "                for member_idx in members:\n",
    "                    doc_id = graph_data['idx_to_doc_id'][member_idx]\n",
    "                    community_assignments[doc_id] = comm_id\n",
    "            \n",
    "            return {\n",
    "                'assignments': community_assignments,\n",
    "                'modularity': communities.modularity,\n",
    "                'num_communities': len(communities),\n",
    "                'sizes': [len(comm) for comm in communities]\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error running Louvain algorithm: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _analyze_communities(self, communities, candidates):\n",
    "        \"\"\"Analyze detected communities and extract insights\"\"\"\n",
    "        try:\n",
    "            assignments = communities['assignments']\n",
    "            \n",
    "            # Group candidates by community\n",
    "            community_docs = {}\n",
    "            for candidate in candidates:\n",
    "                doc_id = candidate['record']['global_id']\n",
    "                if doc_id in assignments:\n",
    "                    comm_id = assignments[doc_id]\n",
    "                    if comm_id not in community_docs:\n",
    "                        community_docs[comm_id] = []\n",
    "                    community_docs[comm_id].append(candidate)\n",
    "            \n",
    "            # Find largest communities\n",
    "            community_sizes = {k: len(v) for k, v in community_docs.items()}\n",
    "            largest_communities = sorted(community_sizes.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Extract themes from largest communities\n",
    "            themes = {}\n",
    "            for comm_id, size in largest_communities[:3]:  # Top 3 communities\n",
    "                docs = community_docs[comm_id]\n",
    "                theme = self._extract_community_theme(docs)\n",
    "                themes[comm_id] = {\n",
    "                    'size': size,\n",
    "                    'theme': theme,\n",
    "                    'documents': docs\n",
    "                }\n",
    "            \n",
    "            return {\n",
    "                'communities': assignments,\n",
    "                'community_sizes': community_sizes,\n",
    "                'largest_communities': largest_communities,\n",
    "                'themes': themes,\n",
    "                'modularity': communities['modularity'],\n",
    "                'num_communities': communities['num_communities'],\n",
    "                'summary': self._generate_community_summary(themes)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing communities: {e}\")\n",
    "            return {\n",
    "                'communities': assignments,\n",
    "                'summary': 'Could not analyze community themes'\n",
    "            }\n",
    "    \n",
    "    def _extract_community_theme(self, docs):\n",
    "        \"\"\"Extract common theme from a community of documents\"\"\"\n",
    "        try:\n",
    "            # Collect domains, regulation types, and key terms\n",
    "            domains = []\n",
    "            reg_types = []\n",
    "            topics = []\n",
    "            \n",
    "            for doc in docs:\n",
    "                record = doc['record']\n",
    "                \n",
    "                # Domain\n",
    "                if record.get('kg_primary_domain'):\n",
    "                    domains.append(record['kg_primary_domain'])\n",
    "                \n",
    "                # Regulation type\n",
    "                if record.get('regulation_type'):\n",
    "                    reg_types.append(record['regulation_type'])\n",
    "                \n",
    "                # Topic from \"about\"\n",
    "                if record.get('about'):\n",
    "                    topics.append(record['about'][:50])\n",
    "            \n",
    "            # Find most common elements\n",
    "            from collections import Counter\n",
    "            \n",
    "            most_common_domain = Counter(domains).most_common(1)\n",
    "            most_common_type = Counter(reg_types).most_common(1)\n",
    "            \n",
    "            theme = {\n",
    "                'primary_domain': most_common_domain[0][0] if most_common_domain else 'Mixed',\n",
    "                'regulation_type': most_common_type[0][0] if most_common_type else 'Various',\n",
    "                'topic_sample': topics[0] if topics else 'Legal documents'\n",
    "            }\n",
    "            \n",
    "            return theme\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'primary_domain': 'Unknown', 'regulation_type': 'Various', 'topic_sample': 'Legal documents'}\n",
    "    \n",
    "    def _generate_community_summary(self, themes):\n",
    "        \"\"\"Generate human-readable summary of communities\"\"\"\n",
    "        try:\n",
    "            if not themes:\n",
    "                return \"No distinct thematic communities detected.\"\n",
    "            \n",
    "            summary_parts = [f\"Detected {len(themes)} major thematic clusters:\"]\n",
    "            \n",
    "            for comm_id, theme_data in themes.items():\n",
    "                size = theme_data['size']\n",
    "                theme = theme_data['theme']\n",
    "                \n",
    "                summary_parts.append(\n",
    "                    f\"\\n‚Ä¢ **Cluster {comm_id + 1}** ({size} docs): \"\n",
    "                    f\"{theme['primary_domain']} - {theme['regulation_type']} \"\n",
    "                    f\"({theme['topic_sample']}...)\"\n",
    "                )\n",
    "            \n",
    "            return \" \".join(summary_parts)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return \"Community analysis completed but summary generation failed.\"\n",
    "    \n",
    "    def boost_community_documents(self, candidates, community_analysis):\n",
    "        \"\"\"Boost scores of documents in the largest community\"\"\"\n",
    "        try:\n",
    "            if not community_analysis or not community_analysis.get('largest_communities'):\n",
    "                return candidates\n",
    "            \n",
    "            # Get largest community ID\n",
    "            largest_comm_id = community_analysis['largest_communities'][0][0]\n",
    "            assignments = community_analysis.get('communities', {})\n",
    "            \n",
    "            boost_amount = 0.10  # 10% boost for being in main community\n",
    "            \n",
    "            for candidate in candidates:\n",
    "                doc_id = candidate['record']['global_id']\n",
    "                \n",
    "                if doc_id in assignments:\n",
    "                    if assignments[doc_id] == largest_comm_id:\n",
    "                        # Apply boost\n",
    "                        if 'final_consensus_score' in candidate:\n",
    "                            candidate['final_consensus_score'] = min(1.0, \n",
    "                                candidate['final_consensus_score'] + boost_amount)\n",
    "                        if 'composite_score' in candidate:\n",
    "                            candidate['composite_score'] = min(1.0,\n",
    "                                candidate['composite_score'] + boost_amount)\n",
    "                        \n",
    "                        candidate['in_main_community'] = True\n",
    "                        candidate['community_id'] = largest_comm_id\n",
    "            \n",
    "            print(f\"‚úÖ Boosted documents in community {largest_comm_id}\")\n",
    "            \n",
    "            return candidates\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error boosting community documents: {e}\")\n",
    "            return candidates\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Automated Context Management (No Hard-Coded Keywords)\n",
    "# ============================================================================\n",
    "\n",
    "class AutomatedConversationContextManager:\n",
    "    \"\"\"\n",
    "    REFACTORED: Fully automated context detection using semantic similarity\n",
    "    and entity extraction. NO hard-coded keyword lists.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, knowledge_graph_instance=None):\n",
    "        self.active_regulations = []\n",
    "        self.active_entities = []\n",
    "        self.exchange_count = 0\n",
    "        self.primary_regulation = None\n",
    "        self.regulation_confidence = 0.0\n",
    "        self.kg = knowledge_graph_instance\n",
    "        \n",
    "        # Semantic tracking for automated detection\n",
    "        self.last_query_embedding = None\n",
    "        self.recent_topic_embeddings = []\n",
    "        self.max_topic_history = 3\n",
    "        \n",
    "        # Thresholds (adjustable)\n",
    "        self.topic_shift_threshold = 0.65  # Cosine similarity threshold\n",
    "        self.context_age_limit = 4  # Exchanges before context expires\n",
    "    \n",
    "    def set_knowledge_graph(self, kg_instance):\n",
    "        \"\"\"Set KG instance after initialization\"\"\"\n",
    "        self.kg = kg_instance\n",
    "    \n",
    "    def _calculate_semantic_similarity(self, emb1, emb2):\n",
    "        \"\"\"Calculate cosine similarity between embeddings\"\"\"\n",
    "        try:\n",
    "            if emb1 is None or emb2 is None:\n",
    "                return 0.0\n",
    "            \n",
    "            # Ensure tensors\n",
    "            if not isinstance(emb1, torch.Tensor):\n",
    "                emb1 = torch.tensor(emb1)\n",
    "            if not isinstance(emb2, torch.Tensor):\n",
    "                emb2 = torch.tensor(emb2)\n",
    "            \n",
    "            # Normalize and compute\n",
    "            emb1_norm = F.normalize(emb1.unsqueeze(0), p=2, dim=1)\n",
    "            emb2_norm = F.normalize(emb2.unsqueeze(0), p=2, dim=1)\n",
    "            \n",
    "            similarity = F.cosine_similarity(emb1_norm, emb2_norm).item()\n",
    "            return similarity\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating similarity: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def detect_query_type(self, query, query_embedding=None):\n",
    "        \"\"\"\n",
    "        AUTOMATED: Detect if query is new topic or follow-up.\n",
    "        Uses ONLY semantic similarity and entity extraction.\n",
    "        NO hard-coded keyword lists.\n",
    "        \n",
    "        Returns: 'new_query', 'followup', or 'continuing_discussion'\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check 1: Explicit different regulation mentioned\n",
    "        if self.kg:\n",
    "            regulation_refs = self.kg.extract_regulation_references_with_confidence(query)\n",
    "            \n",
    "            if regulation_refs and regulation_refs[0]['confidence'] >= 0.5:\n",
    "                mentioned_reg = regulation_refs[0]['regulation']\n",
    "                \n",
    "                if self.primary_regulation:\n",
    "                    same_type = mentioned_reg['type'] == self.primary_regulation['type']\n",
    "                    same_number = mentioned_reg['number'] == self.primary_regulation['number']\n",
    "                    \n",
    "                    if not (same_type and same_number):\n",
    "                        print(f\"üÜï Different regulation detected ‚Üí NEW QUERY\")\n",
    "                        return 'new_query'\n",
    "        \n",
    "        # Check 2: Semantic similarity (primary automated method)\n",
    "        if query_embedding is not None and self.last_query_embedding is not None:\n",
    "            semantic_similarity = self._calculate_semantic_similarity(\n",
    "                query_embedding, \n",
    "                self.last_query_embedding\n",
    "            )\n",
    "            \n",
    "            print(f\"üìä Semantic similarity: {semantic_similarity:.3f}\")\n",
    "            \n",
    "            # Low similarity = topic change\n",
    "            if semantic_similarity < self.topic_shift_threshold:\n",
    "                print(f\"üîÑ Semantic topic shift detected ‚Üí NEW QUERY\")\n",
    "                return 'new_query'\n",
    "            \n",
    "            # Check with recent history\n",
    "            if self.recent_topic_embeddings and len(self.recent_topic_embeddings) > 1:\n",
    "                avg_similarity = np.mean([\n",
    "                    self._calculate_semantic_similarity(query_embedding, topic_emb)\n",
    "                    for topic_emb in self.recent_topic_embeddings\n",
    "                ])\n",
    "                \n",
    "                print(f\"üìä Avg similarity with history: {avg_similarity:.3f}\")\n",
    "                \n",
    "                if avg_similarity < self.topic_shift_threshold:\n",
    "                    print(f\"üîÑ Query diverges from topic history ‚Üí NEW QUERY\")\n",
    "                    return 'new_query'\n",
    "        \n",
    "        # Check 3: Context age\n",
    "        if self.primary_regulation:\n",
    "            age = self.exchange_count - self.primary_regulation.get('mentioned_in_exchange', 0)\n",
    "            if age > self.context_age_limit:\n",
    "                print(f\"‚è∞ Context too old ({age} exchanges) ‚Üí NEW QUERY\")\n",
    "                return 'new_query'\n",
    "        \n",
    "        # Check 4: No context exists\n",
    "        if not self.primary_regulation and self.exchange_count == 0:\n",
    "            return 'new_query'\n",
    "        \n",
    "        # Default: Follow-up if we have context\n",
    "        if self.primary_regulation:\n",
    "            return 'followup'\n",
    "        \n",
    "        return 'new_query'\n",
    "    \n",
    "    def update_from_query(self, query, query_entities, query_embedding=None):\n",
    "        \"\"\"Update context with automated topic change detection\"\"\"\n",
    "        \n",
    "        # Detect if new topic BEFORE updating\n",
    "        is_new_topic = False\n",
    "        \n",
    "        # Check 1: Different regulation\n",
    "        if self.kg:\n",
    "            regulation_refs = self.kg.extract_regulation_references_with_confidence(query)\n",
    "            if regulation_refs and regulation_refs[0]['confidence'] >= 0.5:\n",
    "                mentioned_reg = regulation_refs[0]['regulation']\n",
    "                \n",
    "                if self.primary_regulation:\n",
    "                    same_type = mentioned_reg['type'] == self.primary_regulation['type']\n",
    "                    same_number = mentioned_reg['number'] == self.primary_regulation['number']\n",
    "                    \n",
    "                    if not (same_type and same_number):\n",
    "                        is_new_topic = True\n",
    "        \n",
    "        # Check 2: Semantic similarity\n",
    "        if not is_new_topic and query_embedding is not None and self.last_query_embedding is not None:\n",
    "            semantic_similarity = self._calculate_semantic_similarity(\n",
    "                query_embedding, \n",
    "                self.last_query_embedding\n",
    "            )\n",
    "            \n",
    "            if semantic_similarity < self.topic_shift_threshold:\n",
    "                is_new_topic = True\n",
    "        \n",
    "        # Check 3: Context age\n",
    "        if not is_new_topic and self.primary_regulation:\n",
    "            age = self.exchange_count - self.primary_regulation.get('mentioned_in_exchange', 0)\n",
    "            if age > self.context_age_limit:\n",
    "                is_new_topic = True\n",
    "        \n",
    "        # Increment exchange count\n",
    "        self.exchange_count += 1\n",
    "        \n",
    "        # Clear context if new topic\n",
    "        if is_new_topic:\n",
    "            print(f\"‚ú® CLEARING CONTEXT for new topic\")\n",
    "            self.active_regulations = []\n",
    "            self.active_entities = []\n",
    "            self.primary_regulation = None\n",
    "            self.regulation_confidence = 0.0\n",
    "            self.recent_topic_embeddings = []\n",
    "        \n",
    "        # Update embeddings\n",
    "        if query_embedding is not None:\n",
    "            self.last_query_embedding = query_embedding\n",
    "            self.recent_topic_embeddings.append(query_embedding)\n",
    "            if len(self.recent_topic_embeddings) > self.max_topic_history:\n",
    "                self.recent_topic_embeddings.pop(0)\n",
    "        \n",
    "        # Add entities\n",
    "        for entity, entity_type in query_entities:\n",
    "            if entity not in [e[0] for e in self.active_entities]:\n",
    "                self.active_entities.append((entity, entity_type, self.exchange_count))\n",
    "        \n",
    "        # Extract regulation references\n",
    "        if self.kg:\n",
    "            regulation_refs = self.kg.extract_regulation_references_with_confidence(query)\n",
    "            \n",
    "            if regulation_refs:\n",
    "                best_ref = regulation_refs[0]\n",
    "                confidence = best_ref['confidence']\n",
    "                regulation = best_ref['regulation']\n",
    "                \n",
    "                if confidence >= 0.5:\n",
    "                    self.primary_regulation = {\n",
    "                        'type': regulation['type'],\n",
    "                        'number': regulation['number'],\n",
    "                        'year': regulation['year'],\n",
    "                        'mentioned_in_exchange': self.exchange_count,\n",
    "                        'full_text': regulation['full_text']\n",
    "                    }\n",
    "                    self.regulation_confidence = confidence\n",
    "                    print(f\"‚úÖ Primary regulation: {regulation['type']} {regulation['number']} (confidence: {confidence:.0%})\")\n",
    "                \n",
    "                # Add to active regulations\n",
    "                reg_info = {\n",
    "                    'type': regulation['type'],\n",
    "                    'number': regulation['number'],\n",
    "                    'year': regulation['year'],\n",
    "                    'mentioned_in_exchange': self.exchange_count,\n",
    "                    'confidence': confidence\n",
    "                }\n",
    "                \n",
    "                existing = False\n",
    "                for i, active_reg in enumerate(self.active_regulations):\n",
    "                    if (active_reg['type'] == reg_info['type'] and \n",
    "                        active_reg['number'] == reg_info['number']):\n",
    "                        if confidence >= active_reg.get('confidence', 0):\n",
    "                            self.active_regulations[i] = reg_info\n",
    "                        existing = True\n",
    "                        break\n",
    "                \n",
    "                if not existing:\n",
    "                    self.active_regulations.append(reg_info)\n",
    "    \n",
    "    def expand_query_with_context(self, query):\n",
    "        \"\"\"SMART: Only expand if genuinely needed\"\"\"\n",
    "        if not self.primary_regulation or self.regulation_confidence < 0.5:\n",
    "            return query\n",
    "        \n",
    "        age = self.exchange_count - self.primary_regulation.get('mentioned_in_exchange', 0)\n",
    "        if age > 3:\n",
    "            return query\n",
    "        \n",
    "        reg_text = f\"{self.primary_regulation['type']} {self.primary_regulation['number']}\"\n",
    "        if self.primary_regulation.get('year'):\n",
    "            reg_text += f\" tahun {self.primary_regulation['year']}\"\n",
    "        \n",
    "        if reg_text.lower() in query.lower():\n",
    "            return query\n",
    "        \n",
    "        if len(query.split()) < 8:\n",
    "            return f\"{query} (mengacu pada {reg_text})\"\n",
    "        \n",
    "        return query\n",
    "    \n",
    "    def get_regulation_filter(self):\n",
    "        \"\"\"Get regulation filter if context is still relevant\"\"\"\n",
    "        try:\n",
    "            if not self.primary_regulation or self.regulation_confidence < 0.5:\n",
    "                return None\n",
    "            \n",
    "            age = self.exchange_count - self.primary_regulation.get('mentioned_in_exchange', 0)\n",
    "            if age > 3:\n",
    "                return None\n",
    "            \n",
    "            return {\n",
    "                'regulation_type': str(self.primary_regulation.get('type', '')),\n",
    "                'regulation_number': str(self.primary_regulation.get('number', '')),\n",
    "                'year': str(self.primary_regulation.get('year', '')),\n",
    "                'type': str(self.primary_regulation.get('type', '')),\n",
    "                'number': str(self.primary_regulation.get('number', '')),\n",
    "                'confidence': float(self.regulation_confidence)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in get_regulation_filter: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_relevant_regulations(self, max_age=3):\n",
    "        \"\"\"Get regulations mentioned in recent exchanges\"\"\"\n",
    "        current_exchange = self.exchange_count\n",
    "        relevant = []\n",
    "        \n",
    "        for reg in self.active_regulations:\n",
    "            age = current_exchange - reg.get('mentioned_in_exchange', 0)\n",
    "            if age <= max_age:\n",
    "                relevant.append(reg)\n",
    "        \n",
    "        relevant.sort(key=lambda x: (\n",
    "            -abs(current_exchange - x.get('mentioned_in_exchange', 0)),\n",
    "            -x.get('confidence', 0)\n",
    "        ))\n",
    "        \n",
    "        return relevant\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear all context\"\"\"\n",
    "        self.active_regulations = []\n",
    "        self.active_entities = []\n",
    "        self.exchange_count = 0\n",
    "        self.primary_regulation = None\n",
    "        self.regulation_confidence = 0.0\n",
    "        self.last_query_embedding = None\n",
    "        self.recent_topic_embeddings = []\n",
    "\n",
    "# ============================================================================\n",
    "# CONVERSATION CONTEXT MANAGER (NEW CLASS)\n",
    "# ============================================================================\n",
    "\n",
    "class ConversationContextManager:\n",
    "    \"\"\"Manages conversation context across exchanges with AUTOMATIC topic change detection\"\"\"\n",
    "    \n",
    "    def __init__(self, knowledge_graph_instance=None):\n",
    "        self.active_regulations = []\n",
    "        self.active_entities = []\n",
    "        self.topic_keywords = []\n",
    "        self.last_query_type = None\n",
    "        self.exchange_count = 0\n",
    "        self.primary_regulation = None\n",
    "        self.regulation_confidence = 0.0\n",
    "        self.kg = knowledge_graph_instance\n",
    "        \n",
    "        # NEW: Track semantic topic shifts\n",
    "        self.last_query_embedding = None\n",
    "        self.topic_shift_threshold = 0.65  # Cosine similarity threshold for topic change\n",
    "        self.recent_topics = []  # Track recent topic vectors\n",
    "        self.max_topic_history = 3\n",
    "    \n",
    "    def set_knowledge_graph(self, kg_instance):\n",
    "        \"\"\"Set the knowledge graph instance after initialization\"\"\"\n",
    "        self.kg = kg_instance\n",
    "    \n",
    "    def _calculate_semantic_similarity(self, query1_embedding, query2_embedding):\n",
    "        \"\"\"Calculate semantic similarity between two query embeddings\"\"\"\n",
    "        try:\n",
    "            if query1_embedding is None or query2_embedding is None:\n",
    "                return 0.0\n",
    "            \n",
    "            # Ensure tensors\n",
    "            if not isinstance(query1_embedding, torch.Tensor):\n",
    "                query1_embedding = torch.tensor(query1_embedding)\n",
    "            if not isinstance(query2_embedding, torch.Tensor):\n",
    "                query2_embedding = torch.tensor(query2_embedding)\n",
    "            \n",
    "            # Normalize and compute cosine similarity\n",
    "            query1_normalized = F.normalize(query1_embedding.unsqueeze(0), p=2, dim=1)\n",
    "            query2_normalized = F.normalize(query2_embedding.unsqueeze(0), p=2, dim=1)\n",
    "            \n",
    "            similarity = F.cosine_similarity(query1_normalized, query2_normalized).item()\n",
    "            return similarity\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating semantic similarity: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def detect_query_type(self, query, query_embedding=None):\n",
    "        \"\"\"ENHANCED: Automatic topic detection using semantic similarity\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Step 1: Check for explicit new regulation mention FIRST\n",
    "        if self.kg:\n",
    "            regulation_refs = self.kg.extract_regulation_references_with_confidence(query)\n",
    "        else:\n",
    "            regulation_refs = []\n",
    "        \n",
    "        # CRITICAL: If new regulation explicitly mentioned, check if different\n",
    "        if regulation_refs and regulation_refs[0]['confidence'] >= 0.5:  # Lowered from 0.7\n",
    "            mentioned_reg = regulation_refs[0]['regulation']\n",
    "            \n",
    "            # Check if it's different from primary regulation\n",
    "            if self.primary_regulation:\n",
    "                same_type = mentioned_reg['type'] == self.primary_regulation['type']\n",
    "                same_number = mentioned_reg['number'] == self.primary_regulation['number']\n",
    "                \n",
    "                if not (same_type and same_number):\n",
    "                    print(f\"DEBUG: NEW regulation detected: {mentioned_reg['type']} {mentioned_reg['number']} (different from {self.primary_regulation['type']} {self.primary_regulation['number']})\")\n",
    "                    return 'new_query'\n",
    "                else:\n",
    "                    print(f\"DEBUG: Same regulation as context: {mentioned_reg['type']} {mentioned_reg['number']}\")\n",
    "                    return 'continuing_discussion'\n",
    "            else:\n",
    "                # First regulation mentioned - treat as new\n",
    "                print(f\"DEBUG: First regulation mention: {mentioned_reg['type']} {mentioned_reg['number']}\")\n",
    "                return 'new_query'\n",
    "        \n",
    "        # Step 2: If NO regulation mentioned but we have context, check semantic similarity\n",
    "        if query_embedding is not None and self.last_query_embedding is not None:\n",
    "            semantic_similarity = self._calculate_semantic_similarity(\n",
    "                query_embedding, \n",
    "                self.last_query_embedding\n",
    "            )\n",
    "            \n",
    "            print(f\"DEBUG: Semantic similarity with last query: {semantic_similarity:.3f}\")\n",
    "            \n",
    "            # Low similarity = topic change\n",
    "            if semantic_similarity < self.topic_shift_threshold:\n",
    "                print(f\"DEBUG: SEMANTIC topic shift detected (similarity: {semantic_similarity:.3f} < {self.topic_shift_threshold})\")\n",
    "                return 'new_query'\n",
    "            \n",
    "            # Check with recent topic history for more robust detection\n",
    "            if self.recent_topics and len(self.recent_topics) > 1:\n",
    "                avg_similarity = np.mean([\n",
    "                    self._calculate_semantic_similarity(query_embedding, topic_emb)\n",
    "                    for topic_emb in self.recent_topics\n",
    "                ])\n",
    "                \n",
    "                print(f\"DEBUG: Avg similarity with recent topics: {avg_similarity:.3f}\")\n",
    "                \n",
    "                if avg_similarity < self.topic_shift_threshold:\n",
    "                    print(f\"DEBUG: Query diverges from recent topic history\")\n",
    "                    return 'new_query'\n",
    "        \n",
    "        # Step 3: Check conversation age\n",
    "        if self.primary_regulation:\n",
    "            age = self.exchange_count - self.primary_regulation.get('mentioned_in_exchange', 0)\n",
    "            if age > 4:  # Reduced from 5\n",
    "                print(f\"DEBUG: Context too old ({age} exchanges), treating as new query\")\n",
    "                return 'new_query'\n",
    "        \n",
    "        # Step 4: If we have NO context at all, it's new\n",
    "        if not self.primary_regulation and self.exchange_count == 0:\n",
    "            print(f\"DEBUG: No context exists, first query\")\n",
    "            return 'new_query'\n",
    "        \n",
    "        # Step 5: Very short queries might be followups if we have context\n",
    "        if self.primary_regulation and len(query.split()) < 5:\n",
    "            return 'followup'\n",
    "        \n",
    "        # Step 6: Default - if we reach here with context, assume followup\n",
    "        if self.primary_regulation:\n",
    "            return 'followup'\n",
    "        \n",
    "        # Final default: new query\n",
    "        print(f\"DEBUG: Default case - treating as new query\")\n",
    "        return 'new_query'\n",
    "    \n",
    "    def update_from_query(self, query, query_entities, query_embedding=None):\n",
    "        \"\"\"ENHANCED: Update context with automatic topic change detection\"\"\"\n",
    "        \n",
    "        # FIRST: Detect if this is a new topic BEFORE updating anything\n",
    "        is_new_topic = False\n",
    "        \n",
    "        # Check 1: Explicit different regulation\n",
    "        if self.kg:\n",
    "            regulation_refs = self.kg.extract_regulation_references_with_confidence(query)\n",
    "            if regulation_refs and regulation_refs[0]['confidence'] >= 0.5:\n",
    "                mentioned_reg = regulation_refs[0]['regulation']\n",
    "                \n",
    "                if self.primary_regulation:\n",
    "                    same_type = mentioned_reg['type'] == self.primary_regulation['type']\n",
    "                    same_number = mentioned_reg['number'] == self.primary_regulation['number']\n",
    "                    \n",
    "                    if not (same_type and same_number):\n",
    "                        print(f\"DEBUG: Different regulation mentioned - NEW TOPIC\")\n",
    "                        is_new_topic = True\n",
    "        \n",
    "        # Check 2: Semantic similarity (if we have history)\n",
    "        if not is_new_topic and query_embedding is not None and self.last_query_embedding is not None:\n",
    "            semantic_similarity = self._calculate_semantic_similarity(\n",
    "                query_embedding, \n",
    "                self.last_query_embedding\n",
    "            )\n",
    "            \n",
    "            if semantic_similarity < self.topic_shift_threshold:\n",
    "                print(f\"DEBUG: Low semantic similarity ({semantic_similarity:.3f}) - NEW TOPIC\")\n",
    "                is_new_topic = True\n",
    "        \n",
    "        # Check 3: Context age\n",
    "        if not is_new_topic and self.primary_regulation:\n",
    "            age = self.exchange_count - self.primary_regulation.get('mentioned_in_exchange', 0)\n",
    "            if age > 4:\n",
    "                print(f\"DEBUG: Context too old ({age} exchanges) - NEW TOPIC\")\n",
    "                is_new_topic = True\n",
    "        \n",
    "        # NOW increment exchange count\n",
    "        self.exchange_count += 1\n",
    "        \n",
    "        # Clear context if new topic detected\n",
    "        if is_new_topic:\n",
    "            print(f\"DEBUG: ‚ú® CLEARING CONTEXT for new topic\")\n",
    "            self.active_regulations = []\n",
    "            self.active_entities = []\n",
    "            self.primary_regulation = None\n",
    "            self.regulation_confidence = 0.0\n",
    "            self.recent_topics = []\n",
    "        \n",
    "        # Update query embedding history\n",
    "        if query_embedding is not None:\n",
    "            self.last_query_embedding = query_embedding\n",
    "            \n",
    "            # Add to recent topics (keep last N)\n",
    "            self.recent_topics.append(query_embedding)\n",
    "            if len(self.recent_topics) > self.max_topic_history:\n",
    "                self.recent_topics.pop(0)\n",
    "        \n",
    "        # Add new entities\n",
    "        for entity, entity_type in query_entities:\n",
    "            if entity not in [e[0] for e in self.active_entities]:\n",
    "                self.active_entities.append((entity, entity_type, self.exchange_count))\n",
    "        \n",
    "        # Extract regulation references WITH CONFIDENCE\n",
    "        if self.kg:\n",
    "            regulation_refs = self.kg.extract_regulation_references_with_confidence(query)\n",
    "        else:\n",
    "            regulation_refs = []\n",
    "        \n",
    "        if regulation_refs:\n",
    "            best_ref = regulation_refs[0]\n",
    "            confidence = best_ref['confidence']\n",
    "            regulation = best_ref['regulation']\n",
    "            \n",
    "            # Update primary regulation if confidence is high\n",
    "            if confidence >= 0.5:  # Lowered threshold\n",
    "                self.primary_regulation = {\n",
    "                    'type': regulation['type'],\n",
    "                    'number': regulation['number'],\n",
    "                    'year': regulation['year'],\n",
    "                    'mentioned_in_exchange': self.exchange_count,\n",
    "                    'full_text': regulation['full_text']\n",
    "                }\n",
    "                self.regulation_confidence = confidence\n",
    "                print(f\"DEBUG: ‚úÖ Set primary regulation: {regulation['type']} {regulation['number']} (confidence: {confidence:.0%})\")\n",
    "            \n",
    "            # Add to active regulations\n",
    "            reg_info = {\n",
    "                'type': regulation['type'],\n",
    "                'number': regulation['number'],\n",
    "                'year': regulation['year'],\n",
    "                'mentioned_in_exchange': self.exchange_count,\n",
    "                'confidence': confidence\n",
    "            }\n",
    "            \n",
    "            # Check if already exists\n",
    "            existing = False\n",
    "            for i, active_reg in enumerate(self.active_regulations):\n",
    "                if (active_reg['type'] == reg_info['type'] and \n",
    "                    active_reg['number'] == reg_info['number']):\n",
    "                    if confidence >= active_reg.get('confidence', 0):\n",
    "                        self.active_regulations[i] = reg_info\n",
    "                    existing = True\n",
    "                    break\n",
    "            \n",
    "            if not existing:\n",
    "                self.active_regulations.append(reg_info)\n",
    "    \n",
    "    def expand_query_with_context(self, query):\n",
    "        \"\"\"SMART: Only expand if genuinely needed (not for new topics)\"\"\"\n",
    "        # Don't expand if this is a new topic\n",
    "        if not self.primary_regulation or self.regulation_confidence < 0.5:\n",
    "            return query\n",
    "        \n",
    "        # Check regulation age\n",
    "        age = self.exchange_count - self.primary_regulation.get('mentioned_in_exchange', 0)\n",
    "        if age > 3:\n",
    "            # Too old, don't expand\n",
    "            return query\n",
    "        \n",
    "        # Build regulation reference\n",
    "        reg_text = f\"{self.primary_regulation['type']} {self.primary_regulation['number']}\"\n",
    "        if self.primary_regulation.get('year'):\n",
    "            reg_text += f\" tahun {self.primary_regulation['year']}\"\n",
    "        \n",
    "        # Check if regulation already mentioned\n",
    "        if reg_text.lower() in query.lower():\n",
    "            return query  # Already explicit, no need to expand\n",
    "        \n",
    "        # Only expand for genuinely short/ambiguous queries\n",
    "        if len(query.split()) < 8:\n",
    "            return f\"{query} (mengacu pada {reg_text})\"\n",
    "        \n",
    "        return query\n",
    "    \n",
    "    def get_regulation_filter(self):\n",
    "        \"\"\"Get regulation filter for search - only if context is still relevant\"\"\"\n",
    "        try:\n",
    "            if not self.primary_regulation or self.regulation_confidence < 0.5:\n",
    "                return None\n",
    "            \n",
    "            # Check age - don't use if too old\n",
    "            age = self.exchange_count - self.primary_regulation.get('mentioned_in_exchange', 0)\n",
    "            if age > 3:\n",
    "                print(f\"DEBUG: Primary regulation too old for filter (age: {age})\")\n",
    "                return None\n",
    "            \n",
    "            reg_filter = {\n",
    "                'regulation_type': str(self.primary_regulation.get('type', '')),\n",
    "                'regulation_number': str(self.primary_regulation.get('number', '')),\n",
    "                'year': str(self.primary_regulation.get('year', '')),\n",
    "                'type': str(self.primary_regulation.get('type', '')),\n",
    "                'number': str(self.primary_regulation.get('number', '')),\n",
    "                'confidence': float(self.regulation_confidence)\n",
    "            }\n",
    "            \n",
    "            if not reg_filter['number']:\n",
    "                return None\n",
    "            \n",
    "            print(f\"DEBUG: Using regulation filter: {reg_filter['type']} {reg_filter['number']}\")\n",
    "            return reg_filter\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in get_regulation_filter: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def update_from_results(self, results):\n",
    "        \"\"\"Update context from search results\"\"\"\n",
    "        if not results:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            for result in results[:3]:\n",
    "                try:\n",
    "                    record = result.get('record', {})\n",
    "                    \n",
    "                    if not record:\n",
    "                        continue\n",
    "                    \n",
    "                    reg_type = str(record.get('regulation_type', ''))\n",
    "                    reg_number = str(record.get('regulation_number', ''))\n",
    "                    reg_year = str(record.get('year', ''))\n",
    "                    \n",
    "                    if not reg_type or not reg_number:\n",
    "                        continue\n",
    "                    \n",
    "                    # Normalize regulation type\n",
    "                    normalized_type = None\n",
    "                    for patterns in REGULATION_TYPE_PATTERNS.values():\n",
    "                        if any(p in reg_type.lower() for p in patterns):\n",
    "                            normalized_type = patterns[0]\n",
    "                            break\n",
    "                    \n",
    "                    if not normalized_type:\n",
    "                        normalized_type = reg_type.lower()\n",
    "                    \n",
    "                    reg_info = {\n",
    "                        'type': normalized_type,\n",
    "                        'number': reg_number,\n",
    "                        'year': reg_year,\n",
    "                        'about': str(record.get('about', '')),\n",
    "                        'mentioned_in_exchange': self.exchange_count,\n",
    "                        'confidence': 0.8\n",
    "                    }\n",
    "                    \n",
    "                    # Check if already exists\n",
    "                    existing = False\n",
    "                    for i, active_reg in enumerate(self.active_regulations):\n",
    "                        try:\n",
    "                            if (str(active_reg.get('type', '')) == reg_info['type'] and \n",
    "                                str(active_reg.get('number', '')) == reg_info['number'] and\n",
    "                                str(active_reg.get('year', '')) == reg_info['year']):\n",
    "                                \n",
    "                                self.active_regulations[i] = reg_info\n",
    "                                existing = True\n",
    "                                \n",
    "                                if not self.primary_regulation or reg_info['confidence'] > self.regulation_confidence:\n",
    "                                    self.primary_regulation = reg_info.copy()\n",
    "                                    self.regulation_confidence = reg_info['confidence']\n",
    "                                \n",
    "                                break\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error comparing regulations: {e}\")\n",
    "                            continue\n",
    "                    \n",
    "                    if not existing:\n",
    "                        self.active_regulations.append(reg_info)\n",
    "                        \n",
    "                        if not self.primary_regulation:\n",
    "                            self.primary_regulation = reg_info.copy()\n",
    "                            self.regulation_confidence = reg_info['confidence']\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing result for context: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in update_from_results: {e}\")\n",
    "    \n",
    "    def get_relevant_regulations(self, max_age=3):\n",
    "        \"\"\"Get regulations mentioned in recent exchanges\"\"\"\n",
    "        current_exchange = self.exchange_count\n",
    "        relevant = []\n",
    "        \n",
    "        for reg in self.active_regulations:\n",
    "            age = current_exchange - reg.get('mentioned_in_exchange', 0)\n",
    "            if age <= max_age:\n",
    "                relevant.append(reg)\n",
    "        \n",
    "        relevant.sort(key=lambda x: (\n",
    "            -abs(current_exchange - x.get('mentioned_in_exchange', 0)),\n",
    "            -x.get('confidence', 0)\n",
    "        ))\n",
    "        \n",
    "        return relevant\n",
    "    \n",
    "    def get_primary_regulation_filter(self):\n",
    "        \"\"\"Get the primary regulation for follow-up queries\"\"\"\n",
    "        if not self.primary_regulation:\n",
    "            return None, 0.0\n",
    "        \n",
    "        age = self.exchange_count - self.primary_regulation.get('mentioned_in_exchange', 0)\n",
    "        \n",
    "        if age > 3:\n",
    "            self.regulation_confidence *= 0.5\n",
    "            if self.regulation_confidence < 0.3:\n",
    "                self.primary_regulation = None\n",
    "                return None, 0.0\n",
    "        \n",
    "        return self.primary_regulation, self.regulation_confidence\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear conversation context\"\"\"\n",
    "        self.active_regulations = []\n",
    "        self.active_entities = []\n",
    "        self.topic_keywords = []\n",
    "        self.last_query_type = None\n",
    "        self.exchange_count = 0\n",
    "        self.primary_regulation = None\n",
    "        self.regulation_confidence = 0.0\n",
    "        self.last_query_embedding = None\n",
    "        self.recent_topics = []\n",
    "\n",
    "# =============================================================================\n",
    "# CONVERSATION MANAGER (UNCHANGED)\n",
    "# =============================================================================\n",
    "\n",
    "class KGEnhancedConversationManager:\n",
    "    \"\"\"Conversation manager with KG awareness - UNCHANGED\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_generator, knowledge_graph):\n",
    "        self.llm_generator = llm_generator\n",
    "        self.kg = knowledge_graph\n",
    "        self.conversation_history = []\n",
    "        self.context_memory = []\n",
    "        self.entity_memory = []\n",
    "        self.search_context_manager = None\n",
    "\n",
    "    def set_search_context_manager(self, context_manager):\n",
    "        \"\"\"Link to search engine's context manager\"\"\"\n",
    "        self.search_context_manager = context_manager\n",
    "        \n",
    "    def add_to_history(self, query, rag_result):\n",
    "        \"\"\"Add to conversation history with KG tracking\"\"\"\n",
    "        try:\n",
    "            query_entities = rag_result.get('query_entities', [])\n",
    "            \n",
    "            # Create history entry\n",
    "            history_entry = {\n",
    "                'query': query,\n",
    "                'query_type': rag_result.get('query_type', 'general'),\n",
    "                'query_entities': query_entities,\n",
    "                'response': rag_result.get('llm_response', {}).get('response', ''),\n",
    "                'thinking': rag_result.get('llm_response', {}).get('thinking', ''),\n",
    "                'sources': len(rag_result.get('results', [])),\n",
    "                'research_rounds': rag_result.get('research_rounds', 0),\n",
    "                'kg_enhanced': rag_result.get('kg_enhanced', False),\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'sources_used': []  # For export functionality\n",
    "            }\n",
    "            \n",
    "            # Extract and save source details (top 10 for export)\n",
    "            for result in rag_result.get('results', [])[:10]:\n",
    "                try:\n",
    "                    record = result['record']\n",
    "                    source_info = {\n",
    "                        'regulation_type': record.get('regulation_type', ''),\n",
    "                        'regulation_number': record.get('regulation_number', ''),\n",
    "                        'year': record.get('year', ''),\n",
    "                        'about': record.get('about', ''),\n",
    "                        'enacting_body': record.get('enacting_body', ''),\n",
    "                        'content': record.get('content', '')[:1000],\n",
    "                        'final_score': result.get('final_score', 0),\n",
    "                        'kg_score': result.get('kg_score', 0),\n",
    "                        'kg_primary_domain': record.get('kg_primary_domain', ''),\n",
    "                        'kg_hierarchy_level': record.get('kg_hierarchy_level', 0),\n",
    "                        'team_consensus': result.get('team_consensus', False),\n",
    "                        'researcher_agreement': result.get('researcher_agreement', 0)\n",
    "                    }\n",
    "                    history_entry['sources_used'].append(source_info)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving source: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Save research log if available\n",
    "            if rag_result.get('all_retrieved_metadata'):\n",
    "                history_entry['research_log'] = {\n",
    "                    'team_members': [p['name'] for p in RESEARCH_TEAM_PERSONAS.values()],\n",
    "                    'phase_results': rag_result['all_retrieved_metadata'],\n",
    "                    'total_documents_retrieved': sum(\n",
    "                        len(phase.get('candidates', [])) \n",
    "                        for phase in rag_result['all_retrieved_metadata'].values()\n",
    "                    )\n",
    "                }\n",
    "            \n",
    "            # Add to conversation history\n",
    "            self.conversation_history.append(history_entry)\n",
    "            \n",
    "            # Build context memory (for follow-up queries) - keep top 3\n",
    "            for result in rag_result.get('results', [])[:3]:\n",
    "                try:\n",
    "                    record = result['record']\n",
    "                    self.context_memory.append({\n",
    "                        'regulation': f\"{record['regulation_type']} No. {record['regulation_number']}/{record['year']}\",\n",
    "                        'about': record['about'],\n",
    "                        'content_snippet': record['content'][:200] + \"...\" if len(record['content']) > 200 else record['content']\n",
    "                    })\n",
    "                except Exception:\n",
    "                    continue\n",
    "            \n",
    "            # Update entity memory\n",
    "            self.entity_memory.extend(query_entities)\n",
    "            if len(self.entity_memory) > 30:\n",
    "                self.entity_memory = self.entity_memory[-30:]\n",
    "            \n",
    "            # Trim context memory\n",
    "            if len(self.context_memory) > 15:\n",
    "                self.context_memory = self.context_memory[-15:]\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding to history: {e}\")\n",
    "    \n",
    "    def get_conversation_context(self):\n",
    "        \"\"\"Get conversation context with KG - UNCHANGED\"\"\"\n",
    "        try:\n",
    "            if not self.conversation_history:\n",
    "                return \"\"\n",
    "            \n",
    "            context_parts = [\"KONTEKS PERCAKAPAN SEBELUMNYA:\"]\n",
    "            \n",
    "            recent_history = self.conversation_history[-3:]\n",
    "            for i, entry in enumerate(recent_history, 1):\n",
    "                context_parts.append(f\"Q{i}: {entry['query']}\")\n",
    "                \n",
    "                if entry.get('query_entities'):\n",
    "                    entities_text = ', '.join(entry['query_entities'][:3])\n",
    "                    context_parts.append(f\"Entitas Q{i}: {entities_text}\")\n",
    "                \n",
    "                if entry['thinking']:\n",
    "                    context_parts.append(f\"Thinking{i}: {entry['thinking'][:100]}...\")\n",
    "                response_preview = entry['response'][:150] + \"...\" if len(entry['response']) > 150 else entry['response']\n",
    "                context_parts.append(f\"A{i}: {response_preview}\")\n",
    "                context_parts.append(\"\")\n",
    "            \n",
    "            if self.entity_memory:\n",
    "                unique_entities = list(set(self.entity_memory[-10:]))\n",
    "                context_parts.append(f\"ENTITAS YANG TELAH DIBAHAS: {', '.join(unique_entities)}\")\n",
    "            \n",
    "            return \"\\n\".join(context_parts)\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting conversation context: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def generate_followup_response(self, query, query_type, config):\n",
    "        \"\"\"Generate follow-up response with KG awareness - UNCHANGED\"\"\"\n",
    "        try:\n",
    "            current_entities = [entity for entity, _ in self.kg.extract_entities_from_text(query)]\n",
    "            \n",
    "            conversation_context = self.get_conversation_context()\n",
    "            \n",
    "            memory_context = \"\"\n",
    "            if self.context_memory:\n",
    "                memory_context = \"DOKUMEN YANG TELAH DIBAHAS:\\n\"\n",
    "                for mem in self.context_memory[-5:]:\n",
    "                    memory_context += f\"- {mem['regulation']}: {mem['about']}\\n\"\n",
    "            \n",
    "            user_prompt = f\"\"\"{conversation_context}\n",
    "{memory_context}\n",
    "\n",
    "PERTANYAAN BARU: {query}\n",
    "\n",
    "Berikan jawaban berdasarkan konteks percakapan dan hubungan antar konsep hukum yang relevan.\"\"\"\n",
    "\n",
    "            kg_enhanced_system_prompt = SYSTEM_PROMPT + \" Manfaatkan hubungan semantik berdasarkan konteks percakapan sebelumnya.\"\n",
    "            \n",
    "            input_ids = self.llm_generator.tokenizer.apply_chat_template([\n",
    "                {'role': 'system', 'content': kg_enhanced_system_prompt},\n",
    "                {'role': 'user', 'content': user_prompt}\n",
    "            ], tokenize=True, add_generation_prompt=True, return_tensors='pt').to(self.llm_generator.model.device)\n",
    "            \n",
    "            return input_ids, {\n",
    "                'query_type': query_type,\n",
    "                'query_entities': current_entities,\n",
    "                'is_followup': True,\n",
    "                'kg_enhanced': True,\n",
    "                'context_used': True\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in followup generation: {e}\")\n",
    "            simple_prompt = f\"Pertanyaan lanjutan: {query}\"\n",
    "            input_ids = self.llm_generator.tokenizer([simple_prompt], return_tensors='pt')['input_ids'].to(self.llm_generator.model.device)\n",
    "            return input_ids, {'query_type': query_type, 'error': str(e)}\n",
    "\n",
    "    def prepare_export_data(self):\n",
    "        \"\"\"Prepare conversation data for export\"\"\"\n",
    "        # Since we're now saving everything in add_to_history,\n",
    "        # we can just return the history directly\n",
    "        return self.conversation_history\n",
    "    \n",
    "    def export_to_markdown(self, include_metadata=True, include_research_process=True):\n",
    "        \"\"\"Export conversation to Markdown format\"\"\"\n",
    "        export_data = self.prepare_export_data()\n",
    "        return export_conversation_to_markdown(export_data, include_metadata, include_research_process)\n",
    "    \n",
    "    def export_to_json(self, include_full_content=True):\n",
    "        \"\"\"Export conversation to JSON format\"\"\"\n",
    "        export_data = self.prepare_export_data()\n",
    "        return export_conversation_to_json(export_data, include_full_content)\n",
    "    \n",
    "    def export_to_html(self, include_metadata=True):\n",
    "        \"\"\"Export conversation to HTML format\"\"\"\n",
    "        export_data = self.prepare_export_data()\n",
    "        return export_conversation_to_html(export_data, include_metadata)\n",
    "    \n",
    "    def clear_conversation(self):\n",
    "        \"\"\"Clear conversation history - UNCHANGED\"\"\"\n",
    "        try:\n",
    "            self.conversation_history = []\n",
    "            self.context_memory = []\n",
    "            self.entity_memory = []\n",
    "            clear_cache()\n",
    "        except Exception as e:\n",
    "            print(f\"Error clearing conversation: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# INITIALIZATION FUNCTIONS (UPDATED FOR NEW DATASET)\n",
    "# =============================================================================\n",
    "\n",
    "def initialize_models_and_data(progress_callback=None):\n",
    "    \"\"\"Initialize all models and NEW enhanced KG data\"\"\"\n",
    "    global embedding_model, embedding_tokenizer, reranker_model, reranker_tokenizer\n",
    "    global llm_model, llm_tokenizer, dataset_loader, search_engine, knowledge_graph\n",
    "    global reranker, llm_generator, conversation_manager\n",
    "    global device, EMBEDDING_DIM, token_false_id, token_true_id, prefix_tokens, suffix_tokens\n",
    "    global initialization_complete\n",
    "    \n",
    "    with initialization_lock:\n",
    "        if initialization_complete:\n",
    "            if progress_callback:\n",
    "                progress_callback(\"System already initialized\")\n",
    "            return True\n",
    "    \n",
    "        try:\n",
    "            if progress_callback:\n",
    "                progress_callback(\"Initializing enhanced system...\")\n",
    "            \n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(\"Loading embedding model...\")\n",
    "            \n",
    "            embedding_tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL, padding_side='left')\n",
    "            try:\n",
    "                embedding_model = AutoModel.from_pretrained(\n",
    "                    EMBEDDING_MODEL, \n",
    "                    attn_implementation=\"flash_attention_2\", \n",
    "                    dtype=torch.float32,\n",
    "                    device_map=\"cpu\"\n",
    "                )\n",
    "            except Exception:\n",
    "                embedding_model = AutoModel.from_pretrained(EMBEDDING_MODEL, dtype=torch.float32, device_map=\"cpu\")\n",
    "            \n",
    "            embedding_model = embedding_model.eval()\n",
    "            \n",
    "            # Get embedding dimension\n",
    "            def get_embedding_dim():\n",
    "                try:\n",
    "                    with torch.no_grad():\n",
    "                        test_input = embedding_tokenizer([\"test\"], padding=True, truncation=True, \n",
    "                                                       max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "                        test_input = {k: v.to('cpu') for k, v in test_input.items()}\n",
    "                        test_output = embedding_model(**test_input)\n",
    "                        attention_mask = test_input['attention_mask']\n",
    "                        last_hidden_states = test_output.last_hidden_state\n",
    "                        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "                        batch_size = last_hidden_states.shape[0]\n",
    "                        embedding = last_hidden_states[torch.arange(batch_size, device='cpu'), sequence_lengths]\n",
    "                        return embedding.shape[1]\n",
    "                except Exception as e:\n",
    "                    print(f\"Error getting embedding dim: {e}\")\n",
    "                    return 768\n",
    "            \n",
    "            EMBEDDING_DIM = get_embedding_dim()\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(\"Loading reranker model...\")\n",
    "            \n",
    "            reranker_tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL, padding_side='left')\n",
    "            try:\n",
    "                reranker_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    RERANKER_MODEL,\n",
    "                    dtype=torch.float32,\n",
    "                    attn_implementation=\"flash_attention_2\",\n",
    "                    device_map=\"cpu\"\n",
    "                )\n",
    "            except Exception:\n",
    "                reranker_model = AutoModelForCausalLM.from_pretrained(RERANKER_MODEL, dtype=torch.float32, device_map=\"cpu\")\n",
    "            \n",
    "            reranker_model = reranker_model.eval()\n",
    "            \n",
    "            try:\n",
    "                token_false_id = reranker_tokenizer.convert_tokens_to_ids(\"no\")\n",
    "                token_true_id = reranker_tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "                prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "                suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "                prefix_tokens = reranker_tokenizer.encode(prefix, add_special_tokens=False)\n",
    "                suffix_tokens = reranker_tokenizer.encode(suffix, add_special_tokens=False)\n",
    "            except Exception as e:\n",
    "                print(f\"Error setting up reranker tokens: {e}\")\n",
    "                token_false_id = 0\n",
    "                token_true_id = 1\n",
    "                prefix_tokens = []\n",
    "                suffix_tokens = []\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(\"Loading LLM model on GPU with 4-bit quantization...\")\n",
    "\n",
    "            # 4-bit quantization config\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.float16\n",
    "            )\n",
    "            \n",
    "            llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "            llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "                LLM_MODEL,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"cuda\",  # Auto distribute across available GPU\n",
    "                dtype=torch.float16,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(\"üìä Loading ENHANCED KG dataset...\")\n",
    "            \n",
    "            dataset_loader = EnhancedKGDatasetLoader(DATASET_NAME, EMBEDDING_DIM)\n",
    "            if not dataset_loader.load_from_huggingface(progress_callback):\n",
    "                raise Exception(\"Failed to load enhanced KG dataset\")\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(\"üóÉÔ∏è Building enhanced search indexes...\")\n",
    "            \n",
    "            knowledge_graph = EnhancedKnowledgeGraph(dataset_loader)\n",
    "            \n",
    "            # *** FIXED: Create search engine properly ***\n",
    "            search_engine = EnhancedKGSearchEngine(\n",
    "                dataset_loader.all_records,\n",
    "                dataset_loader.embeddings,\n",
    "                embedding_model,\n",
    "                embedding_tokenizer,\n",
    "                knowledge_graph,\n",
    "                dataset_loader\n",
    "            )\n",
    "            \n",
    "            # *** NEW: Initialize query analyzer if not already done ***\n",
    "            if not hasattr(search_engine, 'query_analyzer') or search_engine.query_analyzer is None:\n",
    "                if progress_callback:\n",
    "                    progress_callback(\"üß† Initializing Advanced Query Analyzer...\")\n",
    "                search_engine.query_analyzer = AdvancedQueryAnalyzer(knowledge_graph)\n",
    "            \n",
    "            reranker = EnhancedKGReranker(\n",
    "                reranker_model, reranker_tokenizer, prefix_tokens, suffix_tokens,\n",
    "                token_true_id, token_false_id, MAX_LENGTH, knowledge_graph\n",
    "            )\n",
    "            \n",
    "            llm_generator = KGEnhancedLLMGenerator(llm_model, llm_tokenizer, knowledge_graph)\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(\"üîó Linking context manager with knowledge graph...\")\n",
    "            \n",
    "            # Ensure context manager has KG reference\n",
    "            search_engine.context_manager.set_knowledge_graph(knowledge_graph)\n",
    "            \n",
    "            # Create conversation manager\n",
    "            conversation_manager = KGEnhancedConversationManager(llm_generator, knowledge_graph)\n",
    "            conversation_manager.set_search_context_manager(search_engine.context_manager)\n",
    "            \n",
    "            clear_cache()\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(\"‚úÖ Enhanced system initialization complete!\")\n",
    "                progress_callback(\"üìå Embedding model: CPU\")\n",
    "                progress_callback(\"üìå Reranker model: CPU\")\n",
    "                progress_callback(\"üìå LLM model: GPU (4-bit)\")\n",
    "            \n",
    "            initialization_complete = True\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            if progress_callback:\n",
    "                progress_callback(f\"Initialization failed: {str(e)}\")\n",
    "            print(f\"Full initialization error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            initialization_complete = False\n",
    "            return False\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN RAG FUNCTIONS (KEEP ORIGINAL LOGIC)\n",
    "# =============================================================================\n",
    "\n",
    "def complete_kg_enhanced_rag_pipeline(query, config, progress_callback=None):\n",
    "    \"\"\"\n",
    "    ENHANCED v2: Complete pipeline with community detection and adaptive learning.\n",
    "    \"\"\"\n",
    "    if not initialization_complete:\n",
    "        return {\n",
    "            'query': query,\n",
    "            'results': [],\n",
    "            'community_summary': 'System not initialized',\n",
    "            'llm_response': {\n",
    "                'thinking': 'System belum siap',\n",
    "                'response': 'Sistem sedang dalam proses inisialisasi.',\n",
    "                'query_type': 'error',\n",
    "                'num_sources': 0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        if hasattr(search_engine, 'all_phase_results'):\n",
    "            search_engine.all_phase_results = {}\n",
    "        \n",
    "        query_type = search_engine._detect_query_type(query)\n",
    "        query_entities = [entity for entity, _ in knowledge_graph.extract_entities_from_text(query)]\n",
    "        \n",
    "        if progress_callback:\n",
    "            progress_callback(f\"Query type detected: {query_type}\")\n",
    "        \n",
    "        # Hybrid search\n",
    "        try:\n",
    "            initial_candidates = search_engine.hybrid_search_strategy(\n",
    "                query, query_type, config, progress_callback\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if progress_callback:\n",
    "                progress_callback(f\"Error in hybrid search: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            initial_candidates = []\n",
    "        \n",
    "        if progress_callback:\n",
    "            progress_callback(f\"Initial search completed: {len(initial_candidates)} candidates found\")\n",
    "        \n",
    "        # *** MODIFIED: Now returns community_summary ***\n",
    "        research_candidates, rounds_conducted, all_phase_metadata, community_summary = search_engine.research_rounds_with_quality_degradation(\n",
    "            query, query_type, initial_candidates, config, progress_callback\n",
    "        )\n",
    "        \n",
    "        if not research_candidates:\n",
    "            return {\n",
    "                'query': query,\n",
    "                'query_type': query_type,\n",
    "                'query_entities': query_entities,\n",
    "                'results': [],\n",
    "                'all_retrieved_metadata': all_phase_metadata,\n",
    "                'research_rounds': rounds_conducted,\n",
    "                'community_summary': community_summary,\n",
    "                'kg_enhanced': True,\n",
    "                'llm_response': {\n",
    "                    'thinking': 'Tidak ada dokumen yang ditemukan',\n",
    "                    'response': 'Maaf, tidak ditemukan dokumen hukum yang relevan dengan pertanyaan Anda.',\n",
    "                    'query_type': query_type,\n",
    "                    'num_sources': 0,\n",
    "                    'kg_enhanced': True,\n",
    "                    'metadata_protected': True\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        reranked_candidates = reranker.rerank_with_kg(\n",
    "            query, research_candidates, query_type, config, \n",
    "            top_k=min(30, len(research_candidates)), \n",
    "            progress_callback=progress_callback\n",
    "        )\n",
    "        \n",
    "        final_results = final_selection_with_kg(reranked_candidates, query_type, config)\n",
    "        \n",
    "        if progress_callback:\n",
    "            progress_callback(f\"Final selection completed: {len(final_results)} results\")\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'query_type': query_type,\n",
    "            'query_entities': query_entities,\n",
    "            'results': final_results,\n",
    "            'all_retrieved_metadata': all_phase_metadata,\n",
    "            'research_rounds': rounds_conducted,\n",
    "            'community_summary': community_summary,  # ‚Üê ENSURE THIS IS RETURNED\n",
    "            'kg_enhanced': True,\n",
    "            'input_ids_and_metadata': None\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"RAG pipeline error: {e}\")\n",
    "        return {\n",
    "            'query': query,\n",
    "            'query_type': 'error',\n",
    "            'query_entities': [],\n",
    "            'results': [],\n",
    "            'all_retrieved_metadata': {},\n",
    "            'research_rounds': 0,\n",
    "            'community_summary': 'Error in processing',\n",
    "            'kg_enhanced': False,\n",
    "            'llm_response': {\n",
    "                'thinking': f'Error: {str(e)}',\n",
    "                'response': 'Terjadi kesalahan dalam pemrosesan. Silakan coba lagi.',\n",
    "                'query_type': 'error',\n",
    "                'num_sources': 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "def final_selection_with_kg(candidates, query_type, config):\n",
    "    \"\"\"Final selection with KG consideration - ORIGINAL\"\"\"\n",
    "    try:\n",
    "        if not candidates:\n",
    "            return []\n",
    "        \n",
    "        quality_threshold = 0.5\n",
    "        if query_type in ['specific_article', 'sanctions']:\n",
    "            quality_threshold = 0.6\n",
    "        elif query_type == 'definitional':\n",
    "            quality_threshold = 0.55\n",
    "        \n",
    "        quality_candidates = [\n",
    "            c for c in candidates \n",
    "            if c.get('final_score', 0) >= quality_threshold\n",
    "        ]\n",
    "        \n",
    "        if len(quality_candidates) < max(2, config['final_top_k'] // 2):\n",
    "            quality_threshold *= 0.85\n",
    "            quality_candidates = [\n",
    "                c for c in candidates \n",
    "                if c.get('final_score', 0) >= quality_threshold\n",
    "            ]\n",
    "        \n",
    "        diverse_results = []\n",
    "        seen_regulation_types = set()\n",
    "        seen_authorities = set()\n",
    "        seen_kg_clusters = set()\n",
    "        \n",
    "        for candidate in quality_candidates:\n",
    "            if len(diverse_results) >= config['final_top_k']:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                record = candidate['record']\n",
    "                reg_type = record['regulation_type']\n",
    "                authority_level = int(record['kg_authority_score'] * 10)\n",
    "                kg_connectivity = int(record.get('kg_connectivity_score', 0) * 10)\n",
    "                \n",
    "                if (reg_type not in seen_regulation_types or \n",
    "                    authority_level not in seen_authorities or \n",
    "                    kg_connectivity not in seen_kg_clusters or\n",
    "                    len(diverse_results) < config['final_top_k'] // 2):\n",
    "                    diverse_results.append(candidate)\n",
    "                    seen_regulation_types.add(reg_type)\n",
    "                    seen_authorities.add(authority_level)\n",
    "                    seen_kg_clusters.add(kg_connectivity)\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        remaining_slots = config['final_top_k'] - len(diverse_results)\n",
    "        for candidate in quality_candidates:\n",
    "            if candidate not in diverse_results and remaining_slots > 0:\n",
    "                diverse_results.append(candidate)\n",
    "                remaining_slots -= 1\n",
    "        \n",
    "        diverse_results.sort(key=lambda x: x.get('final_score', 0), reverse=True)\n",
    "        return diverse_results[:config['final_top_k']]\n",
    "    except Exception as e:\n",
    "        print(f\"Final selection error: {e}\")\n",
    "        return candidates[:config.get('final_top_k', 3)]\n",
    "\n",
    "# =============================================================================\n",
    "# GRADIO INTERFACE FUNCTIONS (KEEP ALL ORIGINAL + UPDATE STATS)\n",
    "# =============================================================================\n",
    "\n",
    "def format_retrieved_metadata(all_metadata, config):\n",
    "    \"\"\"Format all retrieved documents metadata - UNCHANGED\"\"\"\n",
    "    if not all_metadata:\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        output = [\"## üìö ALL RETRIEVED DOCUMENTS METADATA\", \"\"]\n",
    "        \n",
    "        phase_order = ['initial_scan', 'focused_review', 'deep_analysis', 'verification', 'expert_review']\n",
    "        \n",
    "        phase_groups = {}\n",
    "        total_kg_enhanced = 0\n",
    "        \n",
    "        for phase_key, phase_data in all_metadata.items():\n",
    "            phase_name = phase_data['phase']\n",
    "            researcher = phase_data.get('researcher', 'unknown')\n",
    "            \n",
    "            if phase_name not in phase_groups:\n",
    "                phase_groups[phase_name] = {}\n",
    "            if researcher not in phase_groups[phase_name]:\n",
    "                phase_groups[phase_name][researcher] = []\n",
    "            \n",
    "            kg_candidates = [c for c in phase_data['candidates'] if c.get('kg_score', 0) > 0.3]\n",
    "            total_kg_enhanced += len(kg_candidates)\n",
    "            \n",
    "            phase_groups[phase_name][researcher].extend(phase_data['candidates'])\n",
    "        \n",
    "        total_retrieved = 0\n",
    "        for phase_name in phase_order:\n",
    "            if phase_name not in phase_groups:\n",
    "                continue\n",
    "                \n",
    "            researchers = phase_groups[phase_name]\n",
    "            output.append(f\"### üîç PHASE: {phase_name.upper()}\")\n",
    "            output.append(\"\")\n",
    "            \n",
    "            for researcher, candidates in researchers.items():\n",
    "                kg_count = len([c for c in candidates if c.get('kg_score', 0) > 0.3])\n",
    "                if researcher in RESEARCH_TEAM_PERSONAS:\n",
    "                    researcher_name = RESEARCH_TEAM_PERSONAS[researcher]['name']\n",
    "                else:\n",
    "                    researcher_name = researcher\n",
    "                output.append(f\"**{researcher_name}:** {len(candidates)} documents\")\n",
    "                total_retrieved += len(candidates)\n",
    "                \n",
    "                for i, candidate in enumerate(candidates[:5], 1):\n",
    "                    try:\n",
    "                        record = candidate['record']\n",
    "                        score = candidate.get('composite_score', 0)\n",
    "                        kg_score = candidate.get('kg_score', 0)\n",
    "                        \n",
    "                        if kg_score > 0:\n",
    "                            score_display = f\"Score: {score:.3f}, KG: {kg_score:.3f}\"\n",
    "                        else:\n",
    "                            score_display = f\"Score: {score:.3f}\"\n",
    "                        \n",
    "                        output.append(f\"   {i}. **{record['regulation_type']} No. {record['regulation_number']}/{record['year']}** ({score_display})\")\n",
    "                        output.append(f\"      About: {record['about'][:80]}...\")\n",
    "                        output.append(\"\")\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                \n",
    "                if len(candidates) > 5:\n",
    "                    output.append(f\"      ... and {len(candidates) - 5} more documents\")\n",
    "                output.append(\"\")\n",
    "        \n",
    "        output.append(\"### üìà RETRIEVAL SUMMARY\")\n",
    "        output.append(f\"- **Total Documents Retrieved:** {total_retrieved:,}\")\n",
    "        if total_kg_enhanced > 0:\n",
    "            output.append(f\"- **KG-Enhanced Documents:** {total_kg_enhanced:,}\")\n",
    "        output.append(f\"- **Research Phases Used:** {len(phase_groups)}\")\n",
    "        if total_retrieved > 0 and total_kg_enhanced > 0:\n",
    "            output.append(f\"- **KG Enhancement Rate:** {total_kg_enhanced/total_retrieved*100:.1f}%\")\n",
    "        \n",
    "        return \"\\n\".join(output)\n",
    "    except Exception as e:\n",
    "        return f\"Error formatting metadata: {e}\"\n",
    "\n",
    "def format_sources_info(results, config):\n",
    "    \"\"\"Format sources information with ENHANCED KG features\"\"\"\n",
    "    if not results:\n",
    "        return \"Tidak ada sumber yang ditemukan.\"\n",
    "    \n",
    "    try:\n",
    "        output = [f\"## üìñ SUMBER HUKUM UTAMA ({len(results)} dokumen)\", \"\"]\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            try:\n",
    "                record = result['record']\n",
    "                \n",
    "                output.append(f\"### SUMBER {i}\")\n",
    "                output.append(f\"**{record['regulation_type']} No. {record['regulation_number']}/{record['year']}**\")\n",
    "                output.append(f\"**Ditetapkan oleh:** {record['enacting_body']}\")\n",
    "                output.append(f\"**Tentang:** {record['about']}\")\n",
    "                \n",
    "                if record.get('chapter', 'N/A') != 'N/A' or record.get('article', 'N/A') != 'N/A':\n",
    "                    output.append(f\"**Referensi:** Bab {record.get('chapter', 'N/A')} - Pasal {record.get('article', 'N/A')}\")\n",
    "                \n",
    "                # Enhanced metadata display\n",
    "                metadata_parts = []\n",
    "                if 'final_score' in result:\n",
    "                    metadata_parts.append(f\"Final: {result['final_score']:.3f}\")\n",
    "                if 'enhanced_rerank_score' in result:\n",
    "                    metadata_parts.append(f\"Rerank: {result['enhanced_rerank_score']:.3f}\")\n",
    "                if 'composite_score' in result:\n",
    "                    metadata_parts.append(f\"Search: {result['composite_score']:.3f}\")\n",
    "                if 'kg_score' in result and result['kg_score'] > 0:\n",
    "                    metadata_parts.append(f\"KG: {result['kg_score']:.3f}\")\n",
    "                \n",
    "                if metadata_parts:\n",
    "                    output.append(f\"**Skor:** {' | '.join(metadata_parts)}\")\n",
    "                \n",
    "                # ENHANCED: Additional KG metadata\n",
    "                additional_info = []\n",
    "                if record.get('kg_primary_domain'):\n",
    "                    additional_info.append(f\"Domain: {record['kg_primary_domain']}\")\n",
    "                if record.get('kg_hierarchy_level', 0) <= 3:\n",
    "                    additional_info.append(f\"Hierarchy: Level {record['kg_hierarchy_level']}\")\n",
    "                if record.get('kg_cross_ref_count', 0) > 0:\n",
    "                    additional_info.append(f\"Cross-refs: {record['kg_cross_ref_count']}\")\n",
    "                if record.get('kg_pagerank', 0) > 0:\n",
    "                    additional_info.append(f\"PageRank: {record['kg_pagerank']:.4f}\")\n",
    "                if record.get('kg_connectivity_score', 0) > 0:\n",
    "                    additional_info.append(f\"Connectivity: {record['kg_connectivity_score']:.3f}\")\n",
    "                \n",
    "                if additional_info:\n",
    "                    output.append(f\"**Enhanced KG Metadata:** {' | '.join(additional_info)}\")\n",
    "                \n",
    "                # Team consensus info\n",
    "                if result.get('team_consensus', False):\n",
    "                    consensus_info = f\"Team Consensus: Yes\"\n",
    "                    if 'researcher_agreement' in result:\n",
    "                        consensus_info += f\" (Agreement: {result['researcher_agreement']})\"\n",
    "                    if 'supporting_researchers' in result:\n",
    "                        researchers = result['supporting_researchers']\n",
    "                        researcher_names = [RESEARCH_TEAM_PERSONAS.get(r, {}).get('name', r) for r in researchers[:3]]\n",
    "                        consensus_info += f\" | Researchers: {', '.join(researcher_names)}\"\n",
    "                    output.append(f\"**{consensus_info}**\")\n",
    "                \n",
    "                # Devils advocate info\n",
    "                if result.get('devils_advocate_challenged', False):\n",
    "                    challenge_points = result.get('challenge_points', [])\n",
    "                    output.append(f\"**üîç Challenged by Devil's Advocate:** {'; '.join(challenge_points[:2])}\")\n",
    "                \n",
    "                content = record.get('content', '')\n",
    "                if len(content) > 500:\n",
    "                    content = content[:500] + \"...\"\n",
    "                \n",
    "                output.append(f\"**Isi:** {content}\")\n",
    "                output.append(\"\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                output.append(f\"Error formatting source {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return \"\\n\".join(output)\n",
    "    except Exception as e:\n",
    "        return f\"Error formatting sources: {e}\"\n",
    "\n",
    "# KEEP ALL ORIGINAL CHAT FUNCTION - chat_with_legal_rag remains UNCHANGED\n",
    "# (Due to length, I'll indicate this should remain exactly as in original)\n",
    "\n",
    "# =============================================================================\n",
    "# MODIFIED CHAT FUNCTION - Add Query Analysis Display\n",
    "# =============================================================================\n",
    "\n",
    "def chat_with_legal_rag(message, history, config_dict, show_thinking=True, show_sources=True, show_metadata=True):\n",
    "    \"\"\"Main chat function with ADVANCED QUERY ANALYSIS and HYBRID SEARCH\"\"\"\n",
    "    if not message.strip():\n",
    "        return history, \"\"\n",
    "    \n",
    "    try:\n",
    "        current_progress = []\n",
    "        \n",
    "        def add_progress(msg):\n",
    "            current_progress.append(msg)\n",
    "            progress_display = \"\\n\".join([f\"üîÑ {m}\" for m in current_progress])\n",
    "            return history + [[message, f\"**Mencari dan menganalisis...**\\n\\n{progress_display}\"]]\n",
    "        \n",
    "        yield add_progress(\"üöÄ Memulai analisis query...\"), \"\"\n",
    "        \n",
    "        # *** NEW: Advanced Query Analysis Display ***\n",
    "        try:\n",
    "            query_analysis = search_engine.query_analyzer.analyze_query(message)\n",
    "            \n",
    "            # Display analysis\n",
    "            yield add_progress(f\"üß† Strategy: {query_analysis['search_strategy']} ({query_analysis['confidence']:.0%})\"), \"\"\n",
    "            \n",
    "            if query_analysis['reasoning']:\n",
    "                yield add_progress(f\"üí° {query_analysis['reasoning']}\"), \"\"\n",
    "            \n",
    "            if query_analysis.get('key_phrases'):\n",
    "                phrases = [p['phrase'] for p in query_analysis['key_phrases']]\n",
    "                yield add_progress(f\"üéØ Key phrases: {', '.join(phrases)}\"), \"\"\n",
    "            \n",
    "            if query_analysis.get('law_name_detected'):\n",
    "                law_name = query_analysis['specific_entities'][0]['name']\n",
    "                yield add_progress(f\"üìú Law name detected: {law_name}\"), \"\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in query analysis display: {e}\")\n",
    "            query_analysis = None\n",
    "        \n",
    "        # Ensure context manager has KG reference\n",
    "        if not search_engine.context_manager.kg:\n",
    "            search_engine.context_manager.set_knowledge_graph(knowledge_graph)\n",
    "        \n",
    "        # Detect query intent using context manager\n",
    "        try:\n",
    "            query_intent = search_engine.context_manager.detect_query_type(message)\n",
    "            yield add_progress(f\"üéØ Query intent: {query_intent}\"), \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error detecting query intent: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            query_intent = 'new_query'\n",
    "        \n",
    "        # Update context BEFORE search\n",
    "        try:\n",
    "            query_entities_raw = knowledge_graph.extract_entities_from_text(message)\n",
    "            search_engine.context_manager.update_from_query(message, query_entities_raw)\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating context from query: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            query_entities_raw = []\n",
    "        \n",
    "        # Get regulation filter if applicable\n",
    "        regulation_filter = None\n",
    "        try:\n",
    "            if query_intent in ['followup', 'pronoun_reference', 'continuing_discussion']:\n",
    "                regulation_filter, confidence = search_engine.context_manager.get_primary_regulation_filter()\n",
    "                if regulation_filter and confidence >= 0.5:\n",
    "                    print(f\"DEBUG: regulation_filter = {regulation_filter}\")\n",
    "                    \n",
    "                    yield add_progress(\n",
    "                        f\"üìå Primary regulation: {regulation_filter.get('regulation_type', 'N/A')} \"\n",
    "                        f\"{regulation_filter.get('regulation_number', 'N/A')} \"\n",
    "                        f\"(confidence: {confidence:.0%})\"\n",
    "                    ), \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting regulation filter: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            regulation_filter = None\n",
    "        \n",
    "        # Execute RAG pipeline (now with advanced query analysis)\n",
    "        yield add_progress(\"üîç Conducting intelligent search...\"), \"\"\n",
    "        \n",
    "        rag_result = None\n",
    "        all_phase_metadata = {}\n",
    "        \n",
    "        try:\n",
    "            # Use hybrid search strategy (now with query analysis)\n",
    "            initial_candidates = search_engine.hybrid_search_strategy(\n",
    "                message, \n",
    "                search_engine._detect_query_type(message),\n",
    "                config_dict, \n",
    "                lambda x: current_progress.append(x)\n",
    "            )\n",
    "            yield add_progress(f\"‚úÖ Initial search completed: {len(initial_candidates)} candidates\"), \"\"\n",
    "            \n",
    "            # Collect metadata\n",
    "            initial_metadata = dict(getattr(search_engine, 'all_phase_results', {}))\n",
    "            \n",
    "            # Continue with reranking if we have results\n",
    "            if initial_candidates:\n",
    "                yield add_progress(f\"‚öñÔ∏è Reranking candidates...\"), \"\"\n",
    "                \n",
    "                reranked_candidates = reranker.rerank_with_kg(\n",
    "                    message, initial_candidates, \n",
    "                    search_engine._detect_query_type(message), \n",
    "                    config_dict, \n",
    "                    top_k=min(30, len(initial_candidates)),\n",
    "                    progress_callback=lambda x: current_progress.append(x)\n",
    "                )\n",
    "                \n",
    "                yield add_progress(f\"‚úÖ Reranking completed\"), \"\"\n",
    "                \n",
    "                # Final selection\n",
    "                final_results = final_selection_with_kg(reranked_candidates, \n",
    "                                                        search_engine._detect_query_type(message), \n",
    "                                                        config_dict)\n",
    "                \n",
    "                yield add_progress(f\"üéØ Final selection: {len(final_results)} results\"), \"\"\n",
    "                \n",
    "                # Build RAG result\n",
    "                rag_result = {\n",
    "                    'query': message,\n",
    "                    'query_type': search_engine._detect_query_type(message),\n",
    "                    'query_entities': [e for e, _ in query_entities_raw],\n",
    "                    'results': final_results,\n",
    "                    'all_retrieved_metadata': initial_metadata,\n",
    "                    'research_rounds': 1,\n",
    "                    'kg_enhanced': True,\n",
    "                    'query_analysis': query_analysis  # NEW: Include analysis in result\n",
    "                }\n",
    "            else:\n",
    "                yield add_progress(\"‚ö†Ô∏è No suitable candidates found\"), \"\"\n",
    "                \n",
    "                rag_result = {\n",
    "                    'query': message,\n",
    "                    'query_type': search_engine._detect_query_type(message),\n",
    "                    'query_entities': [e for e, _ in query_entities_raw],\n",
    "                    'results': [],\n",
    "                    'all_retrieved_metadata': initial_metadata,\n",
    "                    'research_rounds': 0,\n",
    "                    'kg_enhanced': True,\n",
    "                    'query_analysis': query_analysis\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            yield add_progress(f\"‚ùå Error in search: {str(e)}\"), \"\"\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            rag_result = {\n",
    "                'query': message,\n",
    "                'query_type': 'general',\n",
    "                'query_entities': [],\n",
    "                'results': [],\n",
    "                'all_retrieved_metadata': {},\n",
    "                'research_rounds': 0,\n",
    "                'kg_enhanced': False\n",
    "            }\n",
    "        \n",
    "        # ... (rest of the chat function remains unchanged - LLM generation, streaming, etc.)\n",
    "        \n",
    "        # Step 3: Generate LLM Response\n",
    "        yield add_progress(\"ü§ñ Generating KG-enhanced response...\"), \"\"\n",
    "        \n",
    "        final_progress = \"\\n\".join([msg for msg in current_progress])\n",
    "        \n",
    "        if rag_result and rag_result['results']:\n",
    "            try:\n",
    "                conversation_context = conversation_manager.get_conversation_context()\n",
    "                \n",
    "                input_ids, metadata = llm_generator.generate_with_kg_and_context(\n",
    "                    message, \n",
    "                    rag_result['results'], \n",
    "                    rag_result['query_type'], \n",
    "                    config_dict, \n",
    "                    conversation_context,\n",
    "                    all_phase_metadata\n",
    "                )\n",
    "                \n",
    "                streamer = TextIteratorStreamer(\n",
    "                    llm_tokenizer, \n",
    "                    skip_prompt=True, \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                \n",
    "                generate_kwargs = {\n",
    "                    'input_ids': input_ids,\n",
    "                    'streamer': streamer,\n",
    "                    'max_new_tokens': config_dict['max_new_tokens'],\n",
    "                    'do_sample': config_dict['temperature'] > 0,\n",
    "                    'temperature': config_dict['temperature'] if config_dict['temperature'] > 0 else 1.0,\n",
    "                    'min_p': config_dict['min_p'],\n",
    "                    'top_p': config_dict['top_p'],\n",
    "                    'top_k': config_dict['top_k']\n",
    "                }\n",
    "                \n",
    "                thread = Thread(target=llm_model.generate, kwargs=generate_kwargs)\n",
    "                thread.start()\n",
    "                \n",
    "                thinking_content = []\n",
    "                final_answer = []\n",
    "                live_output = []\n",
    "                in_thinking_block = False\n",
    "                saw_think_tag = False\n",
    "                thinking_header_shown = False\n",
    "                accumulated_text = ''\n",
    "                \n",
    "                for new_text in streamer:\n",
    "                    accumulated_text += new_text\n",
    "                    \n",
    "                    if '<think>' in new_text:\n",
    "                        in_thinking_block = True\n",
    "                        saw_think_tag = True\n",
    "                        new_text = new_text.replace('<think>', '')\n",
    "                        if not thinking_header_shown and show_thinking:\n",
    "                            live_output = [f\"**Proses Penelitian:**\\n\\n{final_progress}\\n\\n---\\n\\nüß† **Sedang berfikir...**\\n\"]\n",
    "                            thinking_header_shown = True\n",
    "                    \n",
    "                    if '</think>' in new_text:\n",
    "                        in_thinking_block = False\n",
    "                        new_text = new_text.replace('</think>', '')\n",
    "                        if show_thinking:\n",
    "                            live_output.append('\\n\\n-----\\n‚úÖ **Jawaban:**\\n')\n",
    "                    \n",
    "                    if saw_think_tag:\n",
    "                        if in_thinking_block:\n",
    "                            thinking_content.append(new_text)\n",
    "                            if show_thinking:\n",
    "                                live_output.append(new_text)\n",
    "                        else:\n",
    "                            final_answer.append(new_text)\n",
    "                            live_output.append(new_text)\n",
    "                    else:\n",
    "                        if len(accumulated_text) > 20 and not saw_think_tag:\n",
    "                            if not thinking_header_shown:\n",
    "                                live_output = [f\"**Proses Penelitian:**\\n\\n{final_progress}\\n\\n---\\n\\n‚≠ê **Jawaban langsung:**\\n\\n\"]\n",
    "                                thinking_header_shown = True\n",
    "                            final_answer.append(new_text)\n",
    "                            live_output.append(new_text)\n",
    "                        else:\n",
    "                            if not thinking_header_shown:\n",
    "                                progress_with_generation = final_progress + f\"\\n\\nü§ñ Generating response...\"\n",
    "                                live_output = [f\"**Proses Penelitian:**\\n\\n{progress_with_generation}\\n\\n{new_text}\"]\n",
    "                            else:\n",
    "                                live_output.append(new_text)\n",
    "                    \n",
    "                    yield history + [[message, ''.join(live_output)]], \"\"\n",
    "                \n",
    "                thread.join()\n",
    "                \n",
    "                response_text = ''.join(final_answer).strip()\n",
    "        \n",
    "                # Build base output\n",
    "                final_output = f'<details><summary>üìã <b>Proses Penelitian Selesai (klik untuk melihat)</b></summary>\\n\\n{final_progress}\\n</details>\\n\\n'\n",
    "        \n",
    "                if thinking_content and show_thinking:\n",
    "                    thinking_text = ''.join(thinking_content).strip()\n",
    "                    final_output += (\n",
    "                        '<details><summary>üß† <b>Proses berfikir (klik untuk melihat)</b></summary>\\n\\n'\n",
    "                        + thinking_text +\n",
    "                        '\\n</details>\\n\\n'\n",
    "                        + '-----\\n‚úÖ **Jawaban:**\\n'\n",
    "                        + response_text\n",
    "                    )\n",
    "                else:\n",
    "                    final_output += f\"‚úÖ **Jawaban:**\\n{response_text}\"\n",
    "                \n",
    "                # ============================================================================\n",
    "                # PHASE 2 FIX: DISPLAY COMMUNITY DETECTION RESULTS\n",
    "                # ============================================================================\n",
    "                \n",
    "                # Extract community analysis from research log\n",
    "                community_analysis_data = None\n",
    "                if rag_result.get('research_log') and rag_result['research_log'].get('final_consensus_log'):\n",
    "                    final_consensus_log = rag_result['research_log']['final_consensus_log']\n",
    "                    community_analysis_data = final_consensus_log.get('community_analysis')\n",
    "                \n",
    "                # Format and display community clusters if available\n",
    "                if community_analysis_data and isinstance(community_analysis_data, list) and len(community_analysis_data) > 0:\n",
    "                    final_output += \"\\n\\n---\\n\\n### üåê Discovered Thematic Clusters\\n\\n\"\n",
    "                    final_output += \"_The research team identified these interconnected legal themes in the retrieved documents:_\\n\\n\"\n",
    "                    \n",
    "                    for cluster_idx, cluster_data in enumerate(community_analysis_data, 1):\n",
    "                        try:\n",
    "                            # Extract cluster information\n",
    "                            cluster_id = cluster_data.get('cluster_id', cluster_idx)\n",
    "                            size = cluster_data.get('size', 0)\n",
    "                            \n",
    "                            # Build cluster header\n",
    "                            final_output += f\"**Cluster {cluster_idx}** ({size} documents)\\n\"\n",
    "                            \n",
    "                            # Display keywords/summary\n",
    "                            if 'top_keywords' in cluster_data and cluster_data['top_keywords']:\n",
    "                                keywords = cluster_data['top_keywords']\n",
    "                                if isinstance(keywords, list):\n",
    "                                    keywords_str = \", \".join([f\"`{kw}`\" for kw in keywords[:8]])  # Top 8 keywords\n",
    "                                    final_output += f\"- **Key Terms:** {keywords_str}\\n\"\n",
    "                            \n",
    "                            elif 'summary' in cluster_data and cluster_data['summary']:\n",
    "                                summary = str(cluster_data['summary'])\n",
    "                                final_output += f\"- {summary}\\n\"\n",
    "                            \n",
    "                            # Display primary domain if available\n",
    "                            if 'primary_domain' in cluster_data and cluster_data['primary_domain']:\n",
    "                                final_output += f\"- **Domain:** {cluster_data['primary_domain']}\\n\"\n",
    "                            \n",
    "                            # Display regulation types if available\n",
    "                            if 'regulation_types' in cluster_data and cluster_data['regulation_types']:\n",
    "                                reg_types = cluster_data['regulation_types']\n",
    "                                if isinstance(reg_types, list):\n",
    "                                    final_output += f\"- **Types:** {', '.join(reg_types[:3])}\\n\"\n",
    "                            \n",
    "                            # Display coherence score if available\n",
    "                            if 'coherence_score' in cluster_data:\n",
    "                                coherence = float(cluster_data['coherence_score'])\n",
    "                                final_output += f\"- **Coherence:** {coherence:.2%}\\n\"\n",
    "                            \n",
    "                            final_output += \"\\n\"\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error formatting cluster {cluster_idx}: {e}\")\n",
    "                            continue\n",
    "                    \n",
    "                    final_output += \"_These clusters were automatically discovered using network analysis of cross-references and semantic relationships between legal documents._\\n\\n\"\n",
    "                \n",
    "                # ============================================================================\n",
    "                # Continue with collapsible sections (sources, metadata)\n",
    "                # ============================================================================\n",
    "                \n",
    "                collapsible_sections = []\n",
    "                \n",
    "                if show_sources and rag_result['results']:\n",
    "                    sources_info = format_sources_info(rag_result['results'], config_dict)\n",
    "                    collapsible_sections.append(\n",
    "                        f'<details><summary>üìñ <b>Sumber Hukum Utama ({len(rag_result[\"results\"])} dokumen)</b></summary>\\n\\n{sources_info}\\n</details>'\n",
    "                    )\n",
    "                \n",
    "                if show_metadata and rag_result.get('all_retrieved_metadata'):\n",
    "                    metadata_info = format_retrieved_metadata(rag_result['all_retrieved_metadata'], config_dict)\n",
    "                    if metadata_info.strip():\n",
    "                        collapsible_sections.append(\n",
    "                            f'<details><summary>üìö <b>Semua Metadata Dokumen yang Ditemukan</b></summary>\\n\\n{metadata_info}\\n</details>'\n",
    "                        )\n",
    "                \n",
    "                if collapsible_sections:\n",
    "                    final_output += f\"\\n\\n---\\n\\n\" + \"\\n\\n\".join(collapsible_sections)\n",
    "                \n",
    "                yield history + [[message, final_output]], \"\"\n",
    "                \n",
    "                rag_result['llm_response'] = {\n",
    "                    'thinking': ''.join(thinking_content).strip(),\n",
    "                    'response': response_text\n",
    "                }\n",
    "                conversation_manager.add_to_history(message, rag_result)\n",
    "                \n",
    "                if len(conversation_manager.conversation_history) > 0:\n",
    "                    print(f\"‚úÖ Conversation saved. {len(conversation_manager.conversation_history)} exchanges ready for export.\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_output = f'<details><summary>üìã <b>Proses Penelitian Selesai (klik untuk melihat)</b></summary>\\n\\n{final_progress}\\n</details>\\n\\n'\n",
    "                error_output += f\"‚ùå **Error generating response:** {str(e)}\\n\\n\"\n",
    "                error_output += \"Maaf, terjadi kesalahan saat membuat respons. Silakan coba lagi.\"\n",
    "                \n",
    "                yield history + [[message, error_output]], \"\"\n",
    "                \n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                \n",
    "        else:\n",
    "            final_output = f'<details><summary>üìã <b>Proses Penelitian Selesai (klik untuk melihat)</b></summary>\\n\\n{final_progress}\\n</details>\\n\\n'\n",
    "            final_output += \"‚ùå **Tidak ada hasil ditemukan**\\n\\n\"\n",
    "            final_output += \"Maaf, tidak ditemukan dokumen hukum yang relevan dengan pertanyaan Anda. Silakan coba:\\n\"\n",
    "            final_output += \"- Menggunakan kata kunci yang berbeda\\n\"\n",
    "            final_output += \"- Memperjelas pertanyaan Anda\\n\"\n",
    "            final_output += \"- Menggunakan istilah hukum yang lebih spesifik\"\n",
    "            \n",
    "            yield history + [[message, final_output]], \"\"\n",
    "            \n",
    "            if rag_result:\n",
    "                rag_result['llm_response'] = {\n",
    "                    'thinking': 'Tidak ada dokumen yang ditemukan',\n",
    "                    'response': 'Maaf, tidak ditemukan dokumen hukum yang relevan dengan pertanyaan Anda.'\n",
    "                }\n",
    "                conversation_manager.add_to_history(message, rag_result)\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ùå **Terjadi kesalahan sistem:**\\n\\n{str(e)}\\n\\n\"\n",
    "        error_msg += \"Silakan coba lagi atau hubungi administrator jika masalah berlanjut.\"\n",
    "        yield history + [[message, error_msg]], \"\"\n",
    "        \n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def clear_conversation():\n",
    "    \"\"\"Clear conversation history\"\"\"\n",
    "    try:\n",
    "        conversation_manager.clear_conversation()\n",
    "        \n",
    "        # ALSO clear search context\n",
    "        if search_engine and hasattr(search_engine, 'context_manager'):\n",
    "            search_engine.context_manager.clear()\n",
    "        \n",
    "        return [], \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error clearing conversation: {e}\")\n",
    "        return [], \"\"\n",
    "\n",
    "def get_system_info():\n",
    "    \"\"\"Get system information\"\"\"\n",
    "    if not initialization_complete:\n",
    "        return \"Sistem belum selesai inisialisasi.\"\n",
    "    \n",
    "    try:\n",
    "        stats = dataset_loader.get_statistics()\n",
    "        info = f\"\"\"## üìä Enhanced KG Legal RAG System Information\n",
    "\n",
    "**Enhanced Features:**\n",
    "- **Realistic Research Team**: 5 distinct researcher personas with unique expertise\n",
    "- **Query-Specific Assembly**: Optimal team selection based on query type\n",
    "- **Multi-Stage Process**: Individual ‚Üí Cross-validation ‚Üí Devil's Advocate ‚Üí Consensus\n",
    "- **Advanced Customization**: Granular control over all search phases\n",
    "\n",
    "**Research Team Personas:**\n",
    "- **üë®‚Äç‚öñÔ∏è Senior Legal Researcher**: 15 years exp, authority-focused\n",
    "- **üë©‚Äç‚öñÔ∏è Junior Legal Researcher**: 3 years exp, comprehensive coverage\n",
    "- **üìö Knowledge Graph Specialist**: 8 years exp, relationship-focused  \n",
    "- **‚öñÔ∏è Procedural Law Expert**: 12 years exp, methodical analysis\n",
    "- **üîç Devil's Advocate**: 10 years exp, critical challenges\n",
    "\n",
    "**Models:**\n",
    "- **Embedding:** {EMBEDDING_MODEL}\n",
    "- **Reranker:** {RERANKER_MODEL}\n",
    "- **LLM:** {LLM_MODEL}\n",
    "\n",
    "**Dataset Statistics:**\n",
    "- **Total Documents:** {stats.get('total_records', 0):,}\n",
    "- **KG-Enhanced:** {stats.get('kg_enhanced', 0):,} ({stats.get('kg_enhancement_rate', 0):.1%})\n",
    "- **Avg Entities/Doc:** {stats.get('avg_entities_per_doc', 0):.1f}\n",
    "- **Avg Authority Score:** {stats.get('avg_authority_score', 0):.3f}\n",
    "- **Avg KG Connectivity:** {stats.get('avg_kg_connectivity', 0):.3f}\n",
    "\n",
    "**Performance Metrics:**\n",
    "- **Authority Tiers:** {stats.get('authority_tiers', 0)}\n",
    "- **Temporal Tiers:** {stats.get('temporal_tiers', 0)}  \n",
    "- **KG Connectivity Tiers:** {stats.get('kg_connectivity_tiers', 0)}\n",
    "- **Memory Optimized:** {stats.get('memory_optimized', False)}\n",
    "\n",
    "**Research Process Features:**\n",
    "- **Cross-Validation:** Team member validation of findings\n",
    "- **Devil's Advocate:** Critical challenge of assumptions  \n",
    "- **Consensus Building:** Weighted agreement based on expertise\n",
    "- **Conflict Resolution:** Automatic handling of disagreements\n",
    "\"\"\"\n",
    "        return info\n",
    "    except Exception as e:\n",
    "        return f\"Error getting system info: {e}\"\n",
    "\n",
    "# =============================================================================\n",
    "# ENHANCED GRADIO INTERFACE\n",
    "# =============================================================================\n",
    "\n",
    "def create_gradio_interface():\n",
    "    \"\"\"Create enhanced Gradio interface with full customization\"\"\"\n",
    "    \n",
    "    custom_css = \"\"\"\n",
    "    /* Base container - responsive to zoom */\n",
    "    .gradio-container {\n",
    "        max-width: 100%;\n",
    "        width: 100%;\n",
    "        margin: 0 auto;\n",
    "        padding: 0;\n",
    "        overflow-x: hidden;\n",
    "    }\n",
    "    \n",
    "    /* Main chat area - scalable dimensions */\n",
    "    .main-chat-area {\n",
    "        width: 100%;\n",
    "        max-width: 75em; /* Changed from rem to em for zoom scaling */\n",
    "        margin: 0 auto;\n",
    "        padding: 1.25em; /* Changed from rem to em */\n",
    "        box-sizing: border-box;\n",
    "    }\n",
    "    \n",
    "    /* Chatbot container - responsive sizing */\n",
    "    .chat-container {\n",
    "        height: 75vh; /* Viewport height scales with zoom */\n",
    "        min-height: 25em; /* Reduced and changed to em */\n",
    "        max-height: none; /* Remove max-height to allow scaling */\n",
    "        width: 100%;\n",
    "        overflow-y: auto;\n",
    "        border: 0.0625em solid #e0e0e0; /* Changed to em */\n",
    "        border-radius: 0.75em; /* Changed to em */\n",
    "        background: white;\n",
    "        box-sizing: border-box;\n",
    "        resize: vertical; /* Allow manual resizing if needed */\n",
    "    }\n",
    "    \n",
    "    /* Prevent width changes from content expansion */\n",
    "    .chatbot {\n",
    "        width: 100%;\n",
    "        max-width: none;\n",
    "        min-width: 0;\n",
    "    }\n",
    "    \n",
    "    /* Chat messages - scalable overflow handling */\n",
    "    .message-wrap {\n",
    "        max-width: 100%;\n",
    "        word-wrap: break-word;\n",
    "        overflow-wrap: break-word;\n",
    "    }\n",
    "    \n",
    "    /* Center the chatbot placeholder */\n",
    "    .chatbot .wrap {\n",
    "        display: flex;\n",
    "        align-items: center;\n",
    "        justify-content: center;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    .chatbot .placeholder {\n",
    "        text-align: center;\n",
    "        display: flex;\n",
    "        align-items: center;\n",
    "        justify-content: center;\n",
    "        height: 100%;\n",
    "        width: 100%;\n",
    "    }\n",
    "\n",
    "    .chatbot .empty {\n",
    "        display: flex;\n",
    "        align-items: center;\n",
    "        justify-content: center;\n",
    "        height: 100%;\n",
    "        width: 100%;\n",
    "        text-align: center;\n",
    "        color: #666;\n",
    "        font-size: 1em; /* Changed to em for scaling */\n",
    "    }\n",
    "    \n",
    "    /* Input area styling */\n",
    "    .input-row {\n",
    "        margin-top: 0.9375em; /* Changed to em */\n",
    "        width: 100%;\n",
    "    }\n",
    "    \n",
    "    .input-row .form {\n",
    "        width: 100%;\n",
    "    }\n",
    "    \n",
    "    /* Settings panels - scalable */\n",
    "    .settings-panel {\n",
    "        background-color: #f8f9fa;\n",
    "        padding: 1.25em; /* Changed to em */\n",
    "        border-radius: 0.75em; /* Changed to em */\n",
    "        margin-bottom: 0.9375em; /* Changed to em */\n",
    "        box-shadow: 0 0.125em 0.25em rgba(0,0,0,0.1); /* Changed to em */\n",
    "        width: 100%;\n",
    "        box-sizing: border-box;\n",
    "    }\n",
    "    \n",
    "    .status-panel {\n",
    "        background-color: #e8f4fd;\n",
    "        padding: 0.9375em; /* Changed to em */\n",
    "        border-radius: 0.5em; /* Changed to em */\n",
    "        border-left: 0.25em solid #2196F3; /* Changed to em */\n",
    "        margin-bottom: 0.625em; /* Changed to em */\n",
    "    }\n",
    "    \n",
    "    /* Responsive breakpoints - using em for zoom-friendly breakpoints */\n",
    "    @media (max-width: 87.5em) {\n",
    "        .main-chat-area {\n",
    "            max-width: 95%;\n",
    "            padding: 0.9375em;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    @media (max-width: 64em) {\n",
    "        .chat-container {\n",
    "            height: 70vh;\n",
    "            min-height: 20em; /* Reduced for better mobile experience */\n",
    "        }\n",
    "        \n",
    "        .main-chat-area {\n",
    "            padding: 0.9375em;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    @media (max-width: 48em) {\n",
    "        .chat-container {\n",
    "            height: 65vh;\n",
    "            min-height: 18em; /* Further reduced */\n",
    "        }\n",
    "        \n",
    "        .main-chat-area {\n",
    "            padding: 0.625em;\n",
    "        }\n",
    "        \n",
    "        .settings-panel {\n",
    "            padding: 0.9375em;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    @media (max-width: 30em) {\n",
    "        .chat-container {\n",
    "            height: 60vh;\n",
    "            min-height: 15em; /* Minimum for usability */\n",
    "        }\n",
    "        \n",
    "        .main-chat-area {\n",
    "            padding: 0.5em;\n",
    "        }\n",
    "        \n",
    "        .settings-panel {\n",
    "            padding: 0.75em;\n",
    "            margin-bottom: 0.625em;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    /* Prevent layout shifts from dynamic content */\n",
    "    .block {\n",
    "        min-width: 0;\n",
    "    }\n",
    "    \n",
    "    /* Tab content - centered tabs */\n",
    "    .tab-nav {\n",
    "        margin-bottom: 1.25em; /* Changed to em */\n",
    "        display: flex;\n",
    "        justify-content: center;\n",
    "        align-items: center;\n",
    "        width: 100%;\n",
    "    }\n",
    "    \n",
    "    /* Center the tab navigation */\n",
    "    .tabs {\n",
    "        display: flex;\n",
    "        flex-direction: column;\n",
    "        align-items: center;\n",
    "        width: 100%;\n",
    "    }\n",
    "    \n",
    "    /* Style the tab buttons - scalable */\n",
    "    .tab-nav button {\n",
    "        margin: 0 0.5em; /* Changed to em */\n",
    "        padding: 0.75em 1.5em; /* Changed to em */\n",
    "        border-radius: 0.5em; /* Changed to em */\n",
    "        font-weight: 500;\n",
    "        transition: all 0.2s ease;\n",
    "    }\n",
    "    \n",
    "    /* Center tab container */\n",
    "    .tabitem {\n",
    "        width: 100%;\n",
    "        max-width: 75em; /* Changed to em */\n",
    "        margin: 0 auto;\n",
    "    }\n",
    "    \n",
    "    /* Examples styling */\n",
    "    .examples {\n",
    "        margin-top: 0.9375em; /* Changed to em */\n",
    "    }\n",
    "    \n",
    "    /* Button styling */\n",
    "    .clear-btn {\n",
    "        margin-left: auto;\n",
    "    }\n",
    "    \n",
    "    /* Ensure consistent column widths in settings */\n",
    "    .settings-columns {\n",
    "        display: grid;\n",
    "        grid-template-columns: 1fr 1fr;\n",
    "        gap: 1.25em; /* Changed to em */\n",
    "        width: 100%;\n",
    "    }\n",
    "    \n",
    "    @media (max-width: 48em) {\n",
    "        .settings-columns {\n",
    "            grid-template-columns: 1fr;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    /* Fix for expandable content not affecting layout */\n",
    "    .prose {\n",
    "        max-width: 100%;\n",
    "    }\n",
    "    \n",
    "    /* Prevent horizontal scroll */\n",
    "    * {\n",
    "        box-sizing: border-box;\n",
    "    }\n",
    "    \n",
    "    /* Enhanced zoom support */\n",
    "    html {\n",
    "        -webkit-text-size-adjust: 100%;\n",
    "        -ms-text-size-adjust: 100%;\n",
    "    }\n",
    "    \n",
    "    /* Ensure text scales properly with browser zoom */\n",
    "    body, .gradio-container, .chatbot {\n",
    "        font-size: 1em; /* Base font size that scales with zoom */\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    with gr.Blocks(\n",
    "        title=\"Enhanced Indonesian Legal Assistant\",\n",
    "        theme=gr.themes.Default(),\n",
    "        css=custom_css\n",
    "    ) as interface:\n",
    "        \n",
    "        with gr.Tabs():\n",
    "            # Main Chat Tab\n",
    "            with gr.TabItem(\"üí¨ Konsultasi Hukum\", id=\"chat\"):\n",
    "                with gr.Column(elem_classes=\"main-chat-area\"):\n",
    "                    chatbot = gr.Chatbot(\n",
    "                        height=\"75vh\",\n",
    "                        show_label=False,\n",
    "                        container=True,\n",
    "                        bubble_full_width=True,\n",
    "                        elem_classes=\"chat-container\",\n",
    "                        show_copy_button=True,\n",
    "                        sanitize_html=True,\n",
    "                        render_markdown=True,\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row(elem_classes=\"input-row\"):\n",
    "                        msg_input = gr.Textbox(\n",
    "                            placeholder=\"Tanyakan tentang hukum Indonesia...\",\n",
    "                            show_label=False,\n",
    "                            container=False,\n",
    "                            scale=10,\n",
    "                            submit_btn=True,\n",
    "                            lines=1,\n",
    "                            max_lines=3,\n",
    "                            interactive=True\n",
    "                        )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        with gr.Column():\n",
    "                            gr.Examples(\n",
    "                                examples=[\n",
    "                                    \"Apakah ada pengaturan yang menjamin kesetaraan hak antara guru dan dosen dalam memperoleh tunjangan profesi?\",\n",
    "                                    \"Apakah terdapat mekanisme pengawasan terhadap penyimpanan uang negara agar terhindar dari penyalahgunaan atau kebocoran keuangan?\", \n",
    "                                    \"Bagaimana mekanisme hukum untuk memperoleh izin resmi bagi pihak yang menjalankan usaha sebagai pengusaha pabrik, penyimpanan, importir, penyalur, maupun penjual eceran barang kena cukai?\",\n",
    "                                    \"Apakah terdapat kewajiban pemerintah untuk menyediakan dana khusus bagi penyuluhan, atau dapat melibatkan sumber pendanaan alternatif seperti swasta dan masyarakat?\",\n",
    "                                    \"Bagaimana prosedur hukum yang harus ditempuh sebelum sanksi denda administrasi di bidang cukai dapat dikenakan kepada pelaku usaha?\",\n",
    "                                    \"Bagaimana sistem perencanaan kas disusun agar mampu mengantisipasi kebutuhan mendesak negara/daerah tanpa mengganggu stabilitas fiskal?\",\n",
    "                                    \"syarat dan prosedur perceraian menurut hukum Indonesia\",\n",
    "                                    \"hak dan kewajiban pekerja dalam UU Ketenagakerjaan\"\n",
    "                                ],\n",
    "                                inputs=msg_input,\n",
    "                                examples_per_page=2,\n",
    "                                label=\"\"\n",
    "                            )\n",
    "            \n",
    "            # Enhanced Settings Tab\n",
    "            with gr.TabItem(\"‚öôÔ∏è Pengaturan Sistem\", id=\"settings\"):\n",
    "                #gr.Markdown(\"### üîß Enhanced RAG Configuration with Research Team\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        # Basic Settings\n",
    "                        with gr.Group(elem_classes=\"settings-panel\"):\n",
    "                            gr.Markdown(\"#### üéØ Basic Settings\")\n",
    "                            final_top_k = gr.Slider(1, 10, value=3, step=1, label=\"Final Top K Results\")\n",
    "                            temperature = gr.Slider(0.0, 2.0, value=0.7, step=0.1, label=\"LLM Temperature\")\n",
    "                            max_new_tokens = gr.Slider(512, 4096, value=2048, step=256, label=\"Max New Tokens\")\n",
    "                        \n",
    "                        # Research Team Settings\n",
    "                        with gr.Group(elem_classes=\"settings-panel researcher-settings\"):\n",
    "                            gr.Markdown(\"#### üë• Research Team Configuration\")\n",
    "                            research_team_size = gr.Slider(1, 5, value=4, step=1, label=\"Team Size\")\n",
    "                            enable_cross_validation = gr.Checkbox(label=\"Enable Cross-Validation\", value=True)\n",
    "                            enable_devil_advocate = gr.Checkbox(label=\"Enable Devil's Advocate\", value=True)\n",
    "                            consensus_threshold = gr.Slider(0.3, 0.9, value=0.6, step=0.05, label=\"Consensus Threshold\")\n",
    "                        \n",
    "                        # Display Settings\n",
    "                        with gr.Group(elem_classes=\"settings-panel\"):\n",
    "                            gr.Markdown(\"#### üí¨ Display Settings\")\n",
    "                            show_thinking = gr.Checkbox(label=\"Show Thinking Process\", value=True)\n",
    "                            show_sources = gr.Checkbox(label=\"Show Legal Sources\", value=True)\n",
    "                            show_metadata = gr.Checkbox(label=\"Show All Retrieved Metadata\", value=True)\n",
    "\n",
    "                        with gr.Group(elem_classes=\"settings-panel\"):\n",
    "                            gr.Markdown(\"#### üß† LLM Generation Settings\")\n",
    "                            top_p = gr.Slider(0.1, 1.0, value=1.0, step=0.1, label=\"Top P\")\n",
    "                            top_k = gr.Slider(1, 100, value=20, step=1, label=\"Top K\")\n",
    "                            min_p = gr.Slider(0.01, 0.3, value=0.1, step=0.01, label=\"Min P\")\n",
    "\n",
    "                        with gr.Group(elem_classes=\"settings-panel\"):\n",
    "                            gr.Markdown(\"#### üìä System Information\")\n",
    "                            system_info_btn = gr.Button(\"üìà View System Stats\", variant=\"primary\")\n",
    "                            reset_defaults_btn = gr.Button(\"üîÑ Reset to Defaults\", variant=\"secondary\")\n",
    "                            system_info_output = gr.Markdown(\"\")\n",
    "\n",
    "                        # In create_gradio_interface(), add to Settings tab:\n",
    "                        with gr.Group(elem_classes=\"settings-panel\"):\n",
    "                            gr.Markdown(\"#### üè• System Health\")\n",
    "                            health_check_btn = gr.Button(\"üîç Run Health Check\", variant=\"secondary\")\n",
    "                            health_report_output = gr.Markdown(\"\")\n",
    "                        \n",
    "                        # Connect health check button\n",
    "                        health_check_btn.click(\n",
    "                            lambda: format_health_report(system_health_check()),\n",
    "                            outputs=health_report_output\n",
    "                        )\n",
    "                    \n",
    "                    with gr.Column():\n",
    "                        # Enhanced Search Phase Configuration\n",
    "                        with gr.Group(elem_classes=\"settings-panel phase-settings\"):\n",
    "                            gr.Markdown(\"#### üîç Search Phase Configuration\")\n",
    "                            \n",
    "                            gr.Markdown(\"**Initial Scan Phase**\")\n",
    "                            initial_scan_enabled = gr.Checkbox(label=\"Enable Initial Scan\", value=True)\n",
    "                            initial_scan_candidates = gr.Slider(100, 800, value=400, step=50, label=\"Candidates\")\n",
    "                            initial_scan_semantic = gr.Slider(0.1, 0.5, value=0.20, step=0.05, label=\"Semantic Threshold\")\n",
    "                            initial_scan_keyword = gr.Slider(0.02, 0.15, value=0.06, step=0.01, label=\"Keyword Threshold\")\n",
    "                            \n",
    "                            gr.Markdown(\"**Focused Review Phase**\")\n",
    "                            focused_review_enabled = gr.Checkbox(label=\"Enable Focused Review\", value=True)\n",
    "                            focused_review_candidates = gr.Slider(50, 300, value=150, step=25, label=\"Candidates\")\n",
    "                            focused_review_semantic = gr.Slider(0.2, 0.6, value=0.35, step=0.05, label=\"Semantic Threshold\")\n",
    "                            focused_review_keyword = gr.Slider(0.05, 0.2, value=0.12, step=0.01, label=\"Keyword Threshold\")\n",
    "                            \n",
    "                            gr.Markdown(\"**Deep Analysis Phase**\")\n",
    "                            deep_analysis_enabled = gr.Checkbox(label=\"Enable Deep Analysis\", value=True)\n",
    "                            deep_analysis_candidates = gr.Slider(20, 120, value=60, step=10, label=\"Candidates\")\n",
    "                            deep_analysis_semantic = gr.Slider(0.3, 0.7, value=0.45, step=0.05, label=\"Semantic Threshold\")\n",
    "                            deep_analysis_keyword = gr.Slider(0.1, 0.3, value=0.18, step=0.01, label=\"Keyword Threshold\")\n",
    "                            \n",
    "                            gr.Markdown(\"**Verification Phase**\")\n",
    "                            verification_enabled = gr.Checkbox(label=\"Enable Verification\", value=True)\n",
    "                            verification_candidates = gr.Slider(10, 60, value=30, step=5, label=\"Candidates\")\n",
    "                            verification_semantic = gr.Slider(0.4, 0.8, value=0.55, step=0.05, label=\"Semantic Threshold\")\n",
    "                            verification_keyword = gr.Slider(0.15, 0.35, value=0.22, step=0.01, label=\"Keyword Threshold\")\n",
    "                            \n",
    "                            gr.Markdown(\"**Expert Review Phase (Optional)**\")\n",
    "                            expert_review_enabled = gr.Checkbox(label=\"Enable Expert Review\", value=True)\n",
    "                            expert_review_candidates = gr.Slider(15, 80, value=45, step=5, label=\"Candidates\")\n",
    "                            expert_review_semantic = gr.Slider(0.35, 0.75, value=0.50, step=0.05, label=\"Semantic Threshold\")\n",
    "                            expert_review_keyword = gr.Slider(0.12, 0.3, value=0.20, step=0.01, label=\"Keyword Threshold\")\n",
    "\n",
    "            \n",
    "            with gr.TabItem(\"üì• Export Conversation\", id=\"export\"):\n",
    "                with gr.Column(elem_classes=\"main-chat-area\"):\n",
    "                    gr.Markdown(\"\"\"\n",
    "                    ## Export Your Conversation\n",
    "                    \n",
    "                    Download your complete consultation history including:\n",
    "                    - All questions and answers\n",
    "                    - Research team process details\n",
    "                    - Legal sources consulted\n",
    "                    - Metadata and analysis\n",
    "                    \"\"\")\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        export_format = gr.Radio(\n",
    "                            choices=[\"Markdown\", \"JSON\", \"HTML\"],\n",
    "                            value=\"Markdown\",\n",
    "                            label=\"Export Format\"\n",
    "                        )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        include_metadata_export = gr.Checkbox(\n",
    "                            label=\"Include Technical Metadata\",\n",
    "                            value=True\n",
    "                        )\n",
    "                        include_research_process_export = gr.Checkbox(\n",
    "                            label=\"Include Research Team Process\",\n",
    "                            value=True\n",
    "                        )\n",
    "                        include_full_content_export = gr.Checkbox(\n",
    "                            label=\"Include Full Document Content (JSON only)\",\n",
    "                            value=False\n",
    "                        )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        export_button = gr.Button(\"üì• Generate Export\", variant=\"primary\", size=\"lg\")\n",
    "                    \n",
    "                    export_output = gr.Textbox(\n",
    "                        label=\"Export Output\",\n",
    "                        lines=20,\n",
    "                        max_lines=30,\n",
    "                        show_copy_button=True\n",
    "                    )\n",
    "                    \n",
    "                    download_file = gr.File(\n",
    "                        label=\"Download Export File\",\n",
    "                        visible=True\n",
    "                    )\n",
    "                    \n",
    "                    gr.Markdown(\"\"\"\n",
    "                    ### Export Format Guide\n",
    "                    \n",
    "                    - **Markdown**: Human-readable format, great for reading and sharing\n",
    "                    - **JSON**: Structured data, perfect for processing or archiving\n",
    "                    - **HTML**: Styled webpage, best for printing or presentation\n",
    "                    \"\"\")\n",
    "            \n",
    "            # Export function\n",
    "            def export_conversation_handler(export_format, include_metadata, include_research, include_full):\n",
    "                \"\"\"Handle export button click\"\"\"\n",
    "                try:\n",
    "                    if not conversation_manager.conversation_history:\n",
    "                        return \"No conversation to export.\", None\n",
    "                    \n",
    "                    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                    \n",
    "                    if export_format == \"Markdown\":\n",
    "                        content = conversation_manager.export_to_markdown(include_metadata, include_research)\n",
    "                        filename = f\"legal_consultation_{timestamp}.md\"\n",
    "                        extension = \"md\"\n",
    "                    elif export_format == \"JSON\":\n",
    "                        content = conversation_manager.export_to_json(include_full)\n",
    "                        filename = f\"legal_consultation_{timestamp}.json\"\n",
    "                        extension = \"json\"\n",
    "                    else:  # HTML\n",
    "                        content = conversation_manager.export_to_html(include_metadata)\n",
    "                        filename = f\"legal_consultation_{timestamp}.html\"\n",
    "                        extension = \"html\"\n",
    "                    \n",
    "                    # Save to temporary file\n",
    "                    import tempfile\n",
    "                    with tempfile.NamedTemporaryFile(mode='w', suffix=f'.{extension}', delete=False, encoding='utf-8') as f:\n",
    "                        f.write(content)\n",
    "                        temp_path = f.name\n",
    "                    \n",
    "                    return content, temp_path\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    return f\"Export failed: {str(e)}\", None\n",
    "            \n",
    "            # Connect export button\n",
    "            export_button.click(\n",
    "                export_conversation_handler,\n",
    "                inputs=[export_format, include_metadata_export, include_research_process_export, include_full_content_export],\n",
    "                outputs=[export_output, download_file]\n",
    "            )\n",
    "            # About Tab\n",
    "            with gr.TabItem(\"‚ÑπÔ∏è About Enhanced System\", id=\"about\"):\n",
    "                with gr.Column(elem_classes=\"main-chat-area\"):\n",
    "                    gr.Markdown(\"\"\"\n",
    "                    # üèõÔ∏è Enhanced KG-Indonesian Legal RAG System\n",
    "                    \n",
    "                    ## üÜï Enhanced Features\n",
    "                    \n",
    "                    ### üë• Realistic Research Team Simulation\n",
    "                    The system now features **5 distinct researcher personas** with unique characteristics:\n",
    "                    \n",
    "                    - **üë®‚Äç‚öñÔ∏è Senior Legal Researcher**: 15 years experience, authority-focused, systematic approach\n",
    "                    - **üë©‚Äç‚öñÔ∏è Junior Legal Researcher**: 3 years experience, broad comprehensive coverage\n",
    "                    - **üìö Knowledge Graph Specialist**: 8 years experience, relationship and semantic focus\n",
    "                    - **‚öñÔ∏è Procedural Law Expert**: 12 years experience, methodical step-by-step analysis\n",
    "                    - **üîç Devil's Advocate**: 10 years experience, critical analysis and challenges\n",
    "                    \n",
    "                    ### üéØ Query-Specific Team Assembly\n",
    "                    Teams are automatically assembled based on query type:\n",
    "                    - **Specific Article**: Senior + Specialist + Devil's Advocate\n",
    "                    - **Procedural**: Procedural Expert + Junior + Senior\n",
    "                    - **Definitional**: Senior + Specialist + Junior\n",
    "                    - **Sanctions**: Senior + Procedural Expert + Devil's Advocate\n",
    "                    - **General**: All researchers (customizable team size)\n",
    "                    \n",
    "                    ### üîÑ Multi-Stage Research Process\n",
    "                    1. **Individual Research**: Each researcher conducts research based on their expertise\n",
    "                    2. **Cross-Validation**: Team members validate each other's findings\n",
    "                    3. **Devil's Advocate Review**: Critical challenges to prevent groupthink\n",
    "                    4. **Consensus Building**: Weighted consensus based on experience and accuracy\n",
    "                    \n",
    "                    ### ‚öôÔ∏è Advanced Customization\n",
    "                    - **Granular Phase Control**: Enable/disable and adjust each search phase individually\n",
    "                    - **Team Configuration**: Control team size, cross-validation, devil's advocate\n",
    "                    - **Consensus Thresholds**: Adjust agreement requirements for final results\n",
    "                    - **Real-time Updates**: All settings apply immediately to the research process\n",
    "                    \n",
    "                    ## üîß Configuration Guide\n",
    "                    \n",
    "                    ### Recommended Settings\n",
    "                    - **Team Size**: 3-4 for optimal balance between coverage and efficiency\n",
    "                    - **Consensus Threshold**: 0.6 for balanced precision/recall\n",
    "                    - **Cross-Validation**: Enable for complex queries requiring validation\n",
    "                    - **Devil's Advocate**: Enable for critical decisions and sanctions queries\n",
    "                    \n",
    "                    ### Search Phase Optimization\n",
    "                    - **Initial Scan**: High candidate count, low thresholds for broad coverage\n",
    "                    - **Focused Review**: Moderate filtering for promising candidates\n",
    "                    - **Deep Analysis**: Strict thresholds for quality documents\n",
    "                    - **Verification**: Highest standards for final validation\n",
    "                    - **Expert Review**: Optional phase for complex specialized queries\n",
    "                    \n",
    "                    ### Performance Tuning\n",
    "                    - **Lower thresholds**: Increase recall but may reduce precision\n",
    "                    - **Higher candidate counts**: More comprehensive but slower processing\n",
    "                    - **Team size optimization**: Larger teams for complex queries, smaller for simple ones\n",
    "                    \n",
    "                    ## üìä Research Analytics\n",
    "                    \n",
    "                    The enhanced system provides detailed insights into the research process:\n",
    "                    - **Per-Researcher Metrics**: Success rates and specialization effectiveness\n",
    "                    - **Phase Analysis**: Which phases contribute most to final results\n",
    "                    - **Consensus Tracking**: Team agreement patterns and conflict resolution\n",
    "                    - **Query Success Patterns**: Learning from successful query-answer pairs\n",
    "                    \n",
    "                    ## üöÄ Technical Improvements\n",
    "                    \n",
    "                    - **Memory Optimization**: Efficient handling of large legal document collections\n",
    "                    - **Parallel Processing**: Multiple researchers work simultaneously\n",
    "                    - **Smart Caching**: Researchers build on each other's work\n",
    "                    - **Error Handling**: Robust fallback mechanisms for edge cases\n",
    "                    - **Streaming Responses**: Real-time progress updates during research\n",
    "                    \n",
    "                    **Note**: This enhanced system combines human-like legal research methodology with AI efficiency, providing transparency into the research process while maintaining high accuracy and comprehensive coverage.\n",
    "                    \"\"\")\n",
    "        \n",
    "        # Hidden state for enhanced configuration\n",
    "        config_state = gr.State(DEFAULT_CONFIG)\n",
    "        \n",
    "        def update_enhanced_config(*args):\n",
    "            \"\"\"Update configuration with all enhanced settings\"\"\"\n",
    "            try:\n",
    "                search_phases = {\n",
    "                    'initial_scan': {\n",
    "                        'candidates': int(args[5]),\n",
    "                        'semantic_threshold': float(args[6]),\n",
    "                        'keyword_threshold': float(args[7]),\n",
    "                        'description': 'Quick broad scan like human initial reading',\n",
    "                        'time_limit': 30,\n",
    "                        'focus_areas': ['regulation_type', 'enacting_body'],\n",
    "                        'enabled': bool(args[4])\n",
    "                    },\n",
    "                    'focused_review': {\n",
    "                        'candidates': int(args[9]),\n",
    "                        'semantic_threshold': float(args[10]),\n",
    "                        'keyword_threshold': float(args[11]),\n",
    "                        'description': 'Focused review of promising candidates',\n",
    "                        'time_limit': 45,\n",
    "                        'focus_areas': ['content', 'chapter', 'article'],\n",
    "                        'enabled': bool(args[8])\n",
    "                    },\n",
    "                    'deep_analysis': {\n",
    "                        'candidates': int(args[13]),\n",
    "                        'semantic_threshold': float(args[14]),\n",
    "                        'keyword_threshold': float(args[15]),\n",
    "                        'description': 'Deep contextual analysis like careful reading',\n",
    "                        'time_limit': 60,\n",
    "                        'focus_areas': ['kg_entities', 'cross_references'],\n",
    "                        'enabled': bool(args[12])\n",
    "                    },\n",
    "                    'verification': {\n",
    "                        'candidates': int(args[17]),\n",
    "                        'semantic_threshold': float(args[18]),\n",
    "                        'keyword_threshold': float(args[19]),\n",
    "                        'description': 'Final verification and cross-checking',\n",
    "                        'time_limit': 30,\n",
    "                        'focus_areas': ['authority_score', 'temporal_score'],\n",
    "                        'enabled': bool(args[16])\n",
    "                    },\n",
    "                    'expert_review': {\n",
    "                        'candidates': int(args[21]),\n",
    "                        'semantic_threshold': float(args[22]),\n",
    "                        'keyword_threshold': float(args[23]),\n",
    "                        'description': 'Expert specialist review for complex cases',\n",
    "                        'time_limit': 40,\n",
    "                        'focus_areas': ['legal_richness', 'completeness_score'],\n",
    "                        'enabled': bool(args[20])\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                new_config = {\n",
    "                    'final_top_k': int(args[0]),\n",
    "                    'temperature': float(args[1]),\n",
    "                    'max_new_tokens': int(args[2]),\n",
    "                    'research_team_size': int(args[3]),\n",
    "                    'enable_cross_validation': bool(args[24]),\n",
    "                    'enable_devil_advocate': bool(args[25]),\n",
    "                    'consensus_threshold': float(args[26]),\n",
    "                    'top_p': float(args[27]),\n",
    "                    'top_k': int(args[28]),\n",
    "                    'min_p': float(args[29]),\n",
    "                    'search_phases': search_phases,\n",
    "                    'max_rounds': 5,\n",
    "                    'initial_quality': 0.8,\n",
    "                    'quality_degradation': 0.15,\n",
    "                    'min_quality': 0.3,\n",
    "                    'parallel_research': True\n",
    "                }\n",
    "                \n",
    "                # Update the global config reference if search_engine exists\n",
    "                if 'search_engine' in globals() and search_engine is not None:\n",
    "                    # Update search engine's internal config reference\n",
    "                    search_engine.current_config = new_config\n",
    "                \n",
    "                return new_config\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error updating enhanced config: {e}\")\n",
    "                return DEFAULT_CONFIG\n",
    "        \n",
    "        def reset_to_enhanced_defaults():\n",
    "            \"\"\"Reset to enhanced default values\"\"\"\n",
    "            try:\n",
    "                return (\n",
    "                    DEFAULT_CONFIG['final_top_k'],  # 0\n",
    "                    DEFAULT_CONFIG['temperature'],  # 1\n",
    "                    DEFAULT_CONFIG['max_new_tokens'],  # 2\n",
    "                    DEFAULT_CONFIG['research_team_size'],  # 3\n",
    "                    DEFAULT_SEARCH_PHASES['initial_scan']['enabled'],  # 4\n",
    "                    DEFAULT_SEARCH_PHASES['initial_scan']['candidates'],  # 5\n",
    "                    DEFAULT_SEARCH_PHASES['initial_scan']['semantic_threshold'],  # 6\n",
    "                    DEFAULT_SEARCH_PHASES['initial_scan']['keyword_threshold'],  # 7\n",
    "                    DEFAULT_SEARCH_PHASES['focused_review']['enabled'],  # 8\n",
    "                    DEFAULT_SEARCH_PHASES['focused_review']['candidates'],  # 9\n",
    "                    DEFAULT_SEARCH_PHASES['focused_review']['semantic_threshold'],  # 10\n",
    "                    DEFAULT_SEARCH_PHASES['focused_review']['keyword_threshold'],  # 11\n",
    "                    DEFAULT_SEARCH_PHASES['deep_analysis']['enabled'],  # 12\n",
    "                    DEFAULT_SEARCH_PHASES['deep_analysis']['candidates'],  # 13\n",
    "                    DEFAULT_SEARCH_PHASES['deep_analysis']['semantic_threshold'],  # 14\n",
    "                    DEFAULT_SEARCH_PHASES['deep_analysis']['keyword_threshold'],  # 15\n",
    "                    DEFAULT_SEARCH_PHASES['verification']['enabled'],  # 16\n",
    "                    DEFAULT_SEARCH_PHASES['verification']['candidates'],  # 17\n",
    "                    DEFAULT_SEARCH_PHASES['verification']['semantic_threshold'],  # 18\n",
    "                    DEFAULT_SEARCH_PHASES['verification']['keyword_threshold'],  # 19\n",
    "                    DEFAULT_SEARCH_PHASES['expert_review']['enabled'],  # 20\n",
    "                    DEFAULT_SEARCH_PHASES['expert_review']['candidates'],  # 21\n",
    "                    DEFAULT_SEARCH_PHASES['expert_review']['semantic_threshold'],  # 22\n",
    "                    DEFAULT_SEARCH_PHASES['expert_review']['keyword_threshold'],  # 23\n",
    "                    DEFAULT_CONFIG['enable_cross_validation'],  # 24\n",
    "                    DEFAULT_CONFIG['enable_devil_advocate'],  # 25\n",
    "                    DEFAULT_CONFIG['consensus_threshold'],  # 26\n",
    "                    DEFAULT_CONFIG['top_p'],  # 27\n",
    "                    DEFAULT_CONFIG['top_k'],  # 28\n",
    "                    DEFAULT_CONFIG['min_p']   # 29\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error resetting to defaults: {e}\")\n",
    "                return tuple([0.5] * 30)  # Fallback\n",
    "        \n",
    "        # All configuration inputs\n",
    "        config_inputs = [\n",
    "            final_top_k, temperature, max_new_tokens, research_team_size,  # 0-3\n",
    "            initial_scan_enabled, initial_scan_candidates, initial_scan_semantic, initial_scan_keyword,  # 4-7\n",
    "            focused_review_enabled, focused_review_candidates, focused_review_semantic, focused_review_keyword,  # 8-11\n",
    "            deep_analysis_enabled, deep_analysis_candidates, deep_analysis_semantic, deep_analysis_keyword,  # 12-15\n",
    "            verification_enabled, verification_candidates, verification_semantic, verification_keyword,  # 16-19\n",
    "            expert_review_enabled, expert_review_candidates, expert_review_semantic, expert_review_keyword,  # 20-23\n",
    "            enable_cross_validation, enable_devil_advocate, consensus_threshold,  # 24-26\n",
    "            top_p, top_k, min_p  # 27-29\n",
    "        ]\n",
    "        \n",
    "        # Connect all inputs to config update\n",
    "        for input_component in config_inputs:\n",
    "            try:\n",
    "                input_component.change(\n",
    "                    update_enhanced_config,\n",
    "                    inputs=config_inputs,\n",
    "                    outputs=config_state\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error connecting config input: {e}\")\n",
    "        \n",
    "        # Reset button\n",
    "        try:\n",
    "            reset_defaults_btn.click(\n",
    "                reset_to_enhanced_defaults,\n",
    "                outputs=config_inputs\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up reset button: {e}\")\n",
    "        \n",
    "        # Chat functionality\n",
    "        try:\n",
    "            msg_input.submit(\n",
    "                chat_with_legal_rag,\n",
    "                inputs=[msg_input, chatbot, config_state, show_thinking, show_sources, show_metadata],\n",
    "                outputs=[chatbot, msg_input]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up chat: {e}\")\n",
    "        \n",
    "        # System info\n",
    "        try:\n",
    "            system_info_btn.click(\n",
    "                get_system_info,\n",
    "                outputs=system_info_output\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up system info: {e}\")\n",
    "    \n",
    "    return interface\n",
    "\n",
    "# =============================================================================\n",
    "# DEBUGGING AND DIAGNOSTIC FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def debug_research_process():\n",
    "    \"\"\"Debug function to identify potential issues\"\"\"\n",
    "    debug_results = {\n",
    "        'potential_issues': [],\n",
    "        'suggestions': [],\n",
    "        'performance_metrics': {}\n",
    "    }\n",
    "    \n",
    "    # Memory check\n",
    "    try:\n",
    "        import psutil\n",
    "        memory_percent = psutil.virtual_memory().percent\n",
    "        if memory_percent > 85:\n",
    "            debug_results['potential_issues'].append(\"High memory usage detected\")\n",
    "            debug_results['suggestions'].append(\"Consider reducing batch sizes or team size\")\n",
    "    except ImportError:\n",
    "        debug_results['potential_issues'].append(\"Cannot monitor memory usage - install psutil\")\n",
    "    \n",
    "    # Initialization check\n",
    "    if not initialization_complete:\n",
    "        debug_results['potential_issues'].append(\"System not fully initialized\")\n",
    "        debug_results['suggestions'].append(\"Wait for complete initialization\")\n",
    "    \n",
    "    # Dataset quality check\n",
    "    if dataset_loader:\n",
    "        stats = dataset_loader.get_statistics()\n",
    "        if stats.get('kg_enhancement_rate', 0) < 0.3:\n",
    "            debug_results['potential_issues'].append(\"Low KG enhancement rate\")\n",
    "            debug_results['suggestions'].append(\"Verify KG preprocessing quality\")\n",
    "    \n",
    "    # Research team balance check\n",
    "    total_researchers = len(RESEARCH_TEAM_PERSONAS)\n",
    "    if total_researchers < 3:\n",
    "        debug_results['potential_issues'].append(\"Insufficient researcher diversity\")\n",
    "        debug_results['suggestions'].append(\"Add more researcher personas for better coverage\")\n",
    "    \n",
    "    # Configuration validation\n",
    "    total_candidates = sum(phase['candidates'] for phase in DEFAULT_SEARCH_PHASES.values())\n",
    "    if total_candidates > 1000:\n",
    "        debug_results['potential_issues'].append(\"High candidate processing load\")\n",
    "        debug_results['suggestions'].append(\"Consider reducing candidate counts in early phases\")\n",
    "    \n",
    "    return debug_results\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION (UNCHANGED AS REQUESTED)\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üèõÔ∏è Enhanced KG Indonesian Legal RAG System\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Configuration\n",
    "    ENABLE_SHARING = False  # Set to True for public sharing\n",
    "    AUTO_INITIALIZE = True  # Set to True to initialize on startup\n",
    "    \n",
    "    # Create interface first\n",
    "    print(\"\\nüì± Creating Gradio interface...\")\n",
    "    interface = create_gradio_interface()\n",
    "    print(\"‚úÖ Interface created\")\n",
    "    \n",
    "    # Initialize system if needed\n",
    "    if AUTO_INITIALIZE or ENABLE_SHARING:\n",
    "        print(\"\\nüîß Initializing system...\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        def init_progress(msg):\n",
    "            print(f\"   {msg}\")\n",
    "        \n",
    "        try:\n",
    "            success = initialize_models_and_data(init_progress)\n",
    "            \n",
    "            if not success:\n",
    "                print(\"\\n‚ùå System initialization failed!\")\n",
    "                print(\"Please check the error messages above and try again.\")\n",
    "                exit(1)\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"‚úÖ System initialized successfully!\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            # Print system info\n",
    "            stats = dataset_loader.get_statistics()\n",
    "            print(f\"\\nüìä Dataset Statistics:\")\n",
    "            print(f\"   - Total Records: {stats.get('total_records', 0):,}\")\n",
    "            print(f\"   - KG Enhanced: {stats.get('kg_enhanced', 0):,}\")\n",
    "            print(f\"   - Enhancement Rate: {stats.get('kg_enhancement_rate', 0):.1%}\")\n",
    "            \n",
    "            # Run health check\n",
    "            print(f\"\\nüè• Running health check...\")\n",
    "            health = system_health_check()\n",
    "            print(f\"   Status: {health['status'].upper()}\")\n",
    "            if health['warnings']:\n",
    "                print(f\"   Warnings: {len(health['warnings'])}\")\n",
    "            if health['errors']:\n",
    "                print(f\"   Errors: {len(health['errors'])}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Initialization error: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            exit(1)\n",
    "    \n",
    "    # Launch interface\n",
    "    print(\"\\nüöÄ Launching interface...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    try:\n",
    "        launch_kwargs = {\n",
    "            'server_name': \"0.0.0.0\" if ENABLE_SHARING else \"127.0.0.1\",\n",
    "            'server_port': 7860,\n",
    "            'share': ENABLE_SHARING,\n",
    "            'debug': False,\n",
    "            'show_error': True,\n",
    "            'quiet': False,\n",
    "            'max_threads': 10,\n",
    "            'auth': None,\n",
    "            'inbrowser': not ENABLE_SHARING,\n",
    "            'show_api': False\n",
    "        }\n",
    "        \n",
    "        print(f\"   Server: {launch_kwargs['server_name']}:{launch_kwargs['server_port']}\")\n",
    "        print(f\"   Share: {launch_kwargs['share']}\")\n",
    "        print(f\"   Auto-open browser: {launch_kwargs['inbrowser']}\")\n",
    "        \n",
    "        interface.launch(**launch_kwargs)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n‚ö†Ô∏è Shutting down gracefully...\")\n",
    "        clear_cache()\n",
    "        print(\"‚úÖ Shutdown complete\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Failed to launch interface: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        exit(1)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
